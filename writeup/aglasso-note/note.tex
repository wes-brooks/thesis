\documentclass[authoryear, review, 11pt]{elsarticle}

\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{rotating}
\usepackage[nolists,nomarkers]{endfloat}
\DeclareDelayedFloatFlavour{sidewaystable}{table}

\usepackage{relsize}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{booktabs}


\usepackage{setspace}
\setstretch{2}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}



\title{Local Variable Selection and Parameter Estimation of Spatially Varying Coefficient Regression Models}
\author{Wesley Brooks}
\date{}                                           % Activate to display a given date or no date


\begin{document}

%\begin{abstract}
%Researchers who analyze spatial data often wish to discern how a certain response variable is related to a set of covariates. When it is believed that the effect of a given covariate may be different at different locations, a spatially varying coefficient regression model, in which the effects of the covariates are allowed to vary across the spatial domain, may be appropriate. In this case, it may be the case that the covariate has a meaningful association with the response in some parts of the spatial domain but not in others. Identifying the covariates that are associated with the response at a given location is called local model selection. Geographically weighted regression, a kernel-based method for estimating the local regression coefficients in a spatially varying coefficient regression model, is considered here. A new method is introduced for local model selection and coefficient estimation in spatially varying coefficient regression models. The idea is to apply a penalty of the elastic net type to a local likelihood function, with a local elastic net tuning parameter and a global bandwidth parameter selected via information criteria. Simulations are used to evaluate the performance of the new method in model selection and coefficient estimation, and the method is applied to a real data example in spatial demography.
%\end{abstract}

\maketitle
%\section{}
%\subsection{}

%Pastebin:


\section{Spatially varying coefficients regression \label{section:SVCR}}
	\subsection{Model}	
	Consider $n$ data points, observed at sampling locations $\bm{s}_1, \dots, \bm{s}_n$, which are distributed in a spatial domain $D \subset \mathbb{R}^2$ according to a density $f(\bm{s})$. For $i = 1, \dots, n$, let $y(\bm{s}_i)$ and $\bm{x}(\bm{s}_i)$ denote the univariate response variable, and a $(p+1)$-variate vector of covariates measured at location $\bm{s}_i$, respectively. At each location $\bm{s}_i$, assume that the outcome is related to the covariates by a linear model where the coefficients $\bm{\beta}(\bm{s}_i)$ may be spatially-varying and $\varepsilon(\bm{s}_i)$ is random error at location $\bm{s}_i$. That is,
	\begin{align}\label{eq:lm(s)}
		y(\bm{s}_i) = \bm{x}(\bm{s}_i)' \bm{\beta}(\bm{s}_i) + \varepsilon(\bm{s}_i).
	\end{align}
	
	Further assume that the error term $\varepsilon(\bm{s}_i)$ is normally distributed with zero mean and variance $\sigma^2$, and that $\varepsilon(\bm{s}_i)$, $i=1, \dots, n$ are independent. That is,
	\begin{align} \label{eq:err}
		\varepsilon(\bm{s}_i) \overset{iid}{\sim} \mathcal{N} \left( 0,\sigma^2 \right).
	\end{align}

  In the context of nonparametric regression, the boundary-effect bias can be reduced by local polynomial modeling, usually in the form of a locally linear model \citep{Fan-1996}. Here, locally linear coefficients are estimated by augmenting the local design matrix with covariate-by-location interactions in two dimensions as proposed by \cite{Wang:2008b}. The augmented local design matrix at location $\bm{s}_i$ is
  \begin{align}
		\bm{Z}(\bm{s}_i) = \left( \bm{X}  \:\: L_i \bm{X} \:\: M_i \bm{X} \right)
	\end{align} 
  
	where $\bm{X}$ is the unaugmented matrix of covariates, $L_i = \text{diag}\{s_{i',x} - s_{i,x}\}$ and $M_i = \text{diag}\{s_{i',y} - s_{i,y}\}$ for $i' = 1, \dots, n$.
  
  \subsection{Estimation}		
  The total log-likelihood of the observed data is the sum of the log-likelihood of each individual observation:
  \begin{align} \label{eq:coefficients}
  	\ell \left\{ \bm{\beta} \right\} = -(1/2) \sum_{i'=1}^n \left[ \log{ \sigma^2}  + \sigma^{-2}  \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \bm{\beta}(\bm{s}_i) \right\}^2 \right].
	\end{align}
	
	Since there are a total of $n \times 3(p+1)$ free parameters for $n$ observations, the model is not identifiable and it is not possible to directly maximize the total likelihood.
  
    The values of the local coefficients $\bm{\beta}(\bm{s})$ are estimated at location $\bm{s}$ by the weighted likelihood
    \begin{align}\label{eq:local-likelihood}
		\mathcal{L} \left\{ \bm{\beta}(\bm{s}) \right\} &= \prod_{i'=1}^n \left\{ \left(2 \pi \sigma^2  \right)^{-1/2}  \exp \left[ -(1/2) \sigma^{-2}  \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \bm{\beta}(\bm{s}) \right\}^2 \right] \right\} ^ {K_h( \| \bm{s} - \bm{s}_{i'} \| )},
	\end{align}
  
    where the weights are calculated by a kernel function $K_h(\cdot)$ such as the Epanechnikov kernel:
    \begin{align}\label{eq:epanechnikov}
        K_h(\delta_{ii'}) &= h^{-2} K\left( h^{-1} \delta_{ii'} \right) \notag \\
        K(x) &= \begin{cases} (3/4) (1-x^2) &\mbox{ if } \delta_{ii'} < h, \\ 0 &\mbox{ if } \delta_{ii'} \geq h. \end{cases}
	\end{align}
  
  Thus, the local log-likelihood function is, up to an additive constant: 
  \begin{align}\label{eq:local-log-likelihood}
		\ell \left\{ \bm{\beta}(\bm{s}) \right\} &= -(1/2) \sum_{i'=1}^n K_h( \| \bm{s} - \bm{s}_{i'} \| ) \left[ \log{\sigma^2}  + \sigma^{-2}  \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \bm{\beta}(\bm{s}) \right\}^2 \right].
  \end{align}
  
  This local likelihood can be maximized by weighted least squares
  \begin{align}\label{eq:beta-hat}
    \hat{\bm{\beta}}(\bm{s}) = \left\{ \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\}^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Y}.
  \end{align}
	
  From (\ref{eq:local-log-likelihood}), the maximum local likelihood estimate $\hat{\sigma}_i^2$ is:	 
  \begin{align}
    \hat{\sigma}^2(\bm{s}) = \left\{ \sum \limits_{i'=1}^{n} K_h( \| \bm{s} - \bm{s}_{i'} \| ) \right\}^{-1} \sum \limits_{i'=1}^n K_h( \| \bm{s} - \bm{s}_{i'} \| ) \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \hat{\bm{\beta}}(\bm{s}) \right\}^2
  \end{align}
	 

\section{Local variable selection and parameter estimation \label{section:model-selection}}
	\subsection{Local variable selection}
	Local adaptive grouped regularization (LAGR) is explored as a penalty function for local variable selection in SVCR models. The proposed local variable selection with LAGR penalty is an $\ell_1$ regularization method for variable selection in regression models \citep{Wang-Leng-2008,Zou:2006}. The adaptive group lasso selects groups of covariates for inclusion or exclusion in the model. For an SVCR model, each variable group is a covariate and its interactions on location.
	
	\subsubsection{Local variable selection and coefficient estimation with the adaptive group lasso}
	The objective function for the LAGR at location $\bm{s}$ consists of the local log-likelihood and an additive penalty:% that is the weighted $\ell_1$-norm of the coefficients, defined to be
	\begin{align}\label{eq:adaptive-lasso-WLS}
		\mathcal{S} \{ \bm{\beta}(\bm{s}) \} &= -2\ell_i\left\{ \bm{\beta}( \bm{s} ) \right\} + \mathcal{J}_1\{ \bm{\beta}(\bm{s}) \} \notag \\
		&= \sum_{i'=1}^n K_h( \| \bm{s} - \bm{s}_{i'} \| )  \left[ \log{\sigma^2}  + \sigma^{-2}  \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \bm{\beta}(\bm{s}_i) \right\}^2 \right] +  \lambda_n (\bm{s}) \sum_{j=1}^p \| \beta_j(\bm{s}) \| \| \tilde{\beta}_j(\bm{s}) \|^{-\gamma}
	\end{align}
	
	where $\sum_{i'=1}^n K_h( \| \bm{s} - \bm{s}_i \| ) \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \bm{\beta}(\bm{s}) \right\}^2$ is the weighted sum of squares minimized by traditional GWR, and $\mathcal{J}_1\{ \bm{\beta}(\bm{s}) \} = \lambda_n (\bm{s}) \sum_{j=1}^p \| \beta_j(\bm{s}) \| \| \tilde{\beta}_j(\bm{s}) \|^{-\gamma}$ is the LAGR penalty. With the vector of unpenalized local coefficients $\tilde{\bm{\beta}}(\bm{s})$, the LAGR penalty for the $j$th group of coefficients $\beta_j(\bm{s})$ at location $\bm{s}$ is $\lambda_n (\bm{s}) \| \tilde{\beta}_j(\bm{s}) \|^{-\gamma}$, where $\lambda_n (\bm{s}) > 0$ is a the local tuning parameter applied to all coefficients at location $\bm{s}$ and $\tilde{\bm{\beta}}(\bm{s}) = \left\{ \tilde{\beta}_1 (\bm{s}), \dots, \tilde{\beta}_p (\bm{s}) \right\}'$ is the vector of adaptive weights at location $\bm{s}$.


    \section{Asymptotic properties}
        Consider a local model at location $\bm{s}$ where there are $p_0 < p$ covariates with nonzero local regression coefficients. Without loss of generality, assume these are covariates $1, \dots, p_0$.
        
        Let $a_n = \max \{ \lambda_n(\bm{s}) \| \tilde{\beta}_j(\bm{s}) \|^{-\gamma}, j \le p_0 \}$ be the largest penalty applied to a covariate group whose true coefficient norm is nonzero, and $b_n = \min \{ \lambda_n(\bm{s}) \| \tilde{\beta}_j(\bm{s}) \|^{-\gamma}, j > p_0 \}$ be the smallest penalty applied to a covariate group whose true coefficient norm is zero.


    \subsection{Asymptotic normality}
    
        \begin{theorem}\label{theorem:consistency} \label{theorem:normality}
            If $h^{-1} n^{-1/2} a_n \xrightarrow{p} 0$ and $h n^{-1/2} b_n \xrightarrow{p} \infty$ then $\hat{\bm{\beta}}(\bm{s}) - \bm{\beta}(\bm{s}) - \frac{\kappa_2 h^2}{2 \kappa_0} \{ \bm{\beta}_{uu}(\bm{s}) + \bm{\beta}_{vv}(\bm{s}) \} = O_p(n^{-1/2} h^{-1} )$
        \end{theorem}
  
        \begin{proof}
            The idea of the proof is to show that the objective being minimized achieves a unique minimum, which must be $\hat{\bm{\beta}}(\bm{s})$.
    
            The order of convergence is $h n^{1/2}$ where $h = O(n^{-1/6})$.
    
            We find the limiting distribution of the estimator:
            \begin{align}\label{eq:consistency}
                \mkern-36mu V_4^{(n)}(\bm{u}) &= Q \left\{ \bm{\beta} (\bm{s}) + h^{-1} n^{-1/2} \bm{u} \right\} - Q \left\{ \bm{\beta}(\bm{s}) \right\} \notag \\
                &\mkern-36mu= (1/2) \left[ \bm{Y} - \bm{Z}(\bm{s}) \left\{ \bm{\beta}(\bm{s}) + h^{-1} n^{-1/2} \bm{u} \right\} \right]^T \bm{W}(\bm{s}) \left[ \bm{Y} - \bm{Z}(\bm{s}) \left\{ \bm{\beta}(\bm{s}) + h^{-1} n^{-1/2} \bm{u} \right\} \right] \notag \\
                &+ \sum_{j=1}^p \lambda_j \| \bm{\beta}(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| \notag \\
                &- (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) \right\} - \sum_{j=1}^p \lambda_j \| \bm{\beta}(\bm{s}) \| \notag \\ 
                &\mkern-36mu= (1/2) \bm{u}^T \left\{ h^{-2} n^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\} \bm{u} - \bm{u}^T \left[ h^{-1} n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) \right\} \right] \notag \\
                &+ \sum_{j=1}^p n^{-1/2} \lambda_j n^{1/2} \left\{ \|\bm{\beta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| - \| \bm{\beta}_j(\bm{s}) \| \right\}
            \end{align}
        
            Note the different limiting behavior of the third term between the cases $j \le p_0$ and $j > p_0$:
        
            \paragraph{Case $j \le p_0$}
            If $j \le p_0$ then $n^{-1/2} \lambda_j \to n^{-1/2} \lambda \| \bm{\beta}_j(\bm{s}) \|^{-\gamma}$ and $| \sqrt{n} \left\{ \|\bm{\beta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| - \| \bm{\beta}_j(\bm{s}) \| \right\} | \le h^{-1} \| \bm{u}_j \|$ so $\lim \limits_{n \to \infty} \lambda_j \left( \|\bm{\beta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| - \| \bm{\beta}_j(\bm{s}) \| \right) \le h^{-1} n^{-1/2} \lambda_j  \| \bm{u}_j \| \le h^{-1} n^{-1/2} a_n \| \bm{u}_j \| \to 0$

            \paragraph{Case $j > p_0$}
            If $j > p_0$ then $\lambda_j \left( \|\bm{\beta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| - \| \bm{\beta}_j(\bm{s}) \| \right) = \lambda_j h^{-1} n^{-1/2} \| \bm{u}_j \| $. 
        
            And note that $h = O(n^{-1/6})$ so that if $h n^{-1/2} b_n \xrightarrow{p} \infty$ then $h^{-1} n^{-1/2} b_n \xrightarrow{p} \infty$.
        
            Now, if $\| \bm{u}_j \| \ne 0$ then $h^{-1} n^{-1/2} \lambda_j \| \bm{u}_j \| \ge h^{-1} n^{-1/2} b_n \| \bm{u}_j \| \to \infty$. On the other hand, if $\| \bm{u}_j \| = 0$ then $h^{-1} n^{-1/2} \lambda_j \| \bm{u}_j \| = 0$.

            Thus, the limit of $V_4^{(n)} (\bm{u})$ is the same as the limit of $V_4^{*(n)} (\bm{u})$ where

            \begin{equation*}
                \mkern-72muV_4^{*(n)} (\bm{u}) = \begin{cases}(1/2) \bm{u}^T \left\{ h^{-2} n^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\} \bm{u} - \bm{u}^T \left[ h^{-1} n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) \right\} \right] &\mbox{ if } \| \bm{u}_j \| = 0 \; \forall j > p_0 \\ \infty &\mbox{ otherwise } \end{cases}
            \end{equation*}


            And $V_4^{*(n)}(\bm{u})$ is convex and is minimized at $\hat{\bm{u}}^{(n)}$:

            \begin{align} \label{eq:limit}
                0 &=  \left\{ h^{-2} n^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\} \hat{\bm{u}}^{(n)} - \left[ h^{-1} n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) \right\} \right] \notag \\
                \therefore \hat{\bm{u}}^{(n)} &= \left\{ n^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\}^{-1} \left[ h n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) \right\} \right] \notag \\
            \end{align}

            By the epiconvergence results of \cite{Geyer-1994} and \cite{Knight-Fu-2000}, the minimizer of the limiting function is the limit of the minimizers $\hat{\bm{u}}^{(n)}$. And since, by Lemma 2 of \cite{Sun-Yan-Zhang-Lu-2014}, 

            \begin{equation}
                \hat{\bm{u}}^{(n)} \xrightarrow{d} N \left(0, f(\bm{s}) \kappa_0^{-2} \nu_0 \sigma^2 \Psi^{-1} \right)
            \end{equation}
            the result is proven.
        \end{proof}


    \subsection{Selection}
    
        \begin{theorem} \label{theorem:selection}   
            If $h n^{-1/2} b_n \xrightarrow{p} \infty$ then $P \left\{ \hat{\bm{\beta}}_{(b)} (\bm{s}) = 0 \right\} \to 1$.
        \end{theorem}

        \begin{proof}
            We showed in Theorem \ref{theorem:normality} that the 
            
            The proof is by contradiction.
      
            Recall that the objective to be minimized by $\hat{\bm{\beta}}_{(p)} (\bm{s})$ is
            \begin{align}\label{eq:objective}
                Q \left\{ \hat{\bm{\beta}} (\bm{s}) \right\} = (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \hat{\bm{\beta}} (\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \hat{\bm{\beta}} (\bm{s}) \right\} + \sum_{j=1}^p \lambda_j \| \hat{\bm{\beta}}_p (\bm{s}) \|
            \end{align}

            Let $\hat{\bm{\beta}}_p(\bm{s}) \ne 0$. Then (\ref{eq:objective}) is differentiable w.r.t. $\bm{\beta}_p(\bm{s})$ and $Q$ is maximized at
            \begin{align}
                0 &= \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}_{(-p)}(\bm{s}) \hat{\bm{\beta}}_{(-p)}(\bm{s}) - \bm{Z}_{(p)}(\bm{s}) \hat{\bm{\beta}}_{(p)}(\bm{s}) \right\} - \lambda_p \frac{ \hat{\bm{\beta}}_{(p)}(\bm{s}) }{\| \hat{\bm{\beta}}_{(p)}(\bm{s}) \|} \notag \\
                &= \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \left[ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \bm{\beta}_{uu}(\bm{s}) + \bm{\beta}_{vv}(\bm{s}) \right\} \right] \notag \\
                &\mkern+72mu+ \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{(-p)}(\bm{s}) \left[ \bm{\beta}_{(-p)}(\bm{s}) + \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \bm{\beta}_{(-p),uu}(\bm{s}) + \bm{\beta}_{(-p),vv}(\bm{s}) \right\} - \hat{\bm{\beta}}_{(-p)}(\bm{s}) \right] \notag \\
                &\mkern+72mu+ \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{(p)}(\bm{s}) \left[ \bm{\beta}_{(-p)}(\bm{s}) + \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \bm{\beta}_{(p),uu}(\bm{s}) + \bm{\beta}_{(p),vv}(\bm{s}) \right\} - \hat{\bm{\beta}}_{(p)}(\bm{s}) \right]  \notag \\
                &\mkern+72mu- \lambda_p \frac{ \hat{\bm{\beta}}_{(p)}(\bm{s}) }{\| \hat{\bm{\beta}}_{(p)}(\bm{s}) \|} \notag \\
            \end{align}
        
            So
            \begin{align}\label{eq:selection}
                \frac{h}{\sqrt{n}} \lambda_p \frac{ \hat{\bm{\beta}}_{(p)}(\bm{s}) }{\| \hat{\bm{\beta}}_{(p)}(\bm{s}) \|} &= \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \frac{h}{\sqrt{n}} \left[ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \bm{\beta}_{uu}(\bm{s}) + \bm{\beta}_{vv}(\bm{s}) \right\} \right] \notag \\ 
                &+ \left\{ n^{-1}  \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{(-p)}(\bm{s}) \right\} h \sqrt{n} \left[ \bm{\beta}_{(-p)}(\bm{s}) + \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \bm{\beta}_{(-p),uu}(\bm{s}) + \bm{\beta}_{(-p),vv}(\bm{s}) \right\} - \hat{\bm{\beta}}_{(-p)}(\bm{s}) \right] \notag \\
                &+ \left\{ n^{-1} \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{(-p)}(\bm{s}) \right\} h \sqrt{n} \left[ \bm{\beta}_{(-p)}(\bm{s}) + \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \bm{\beta}_{(p),uu}(\bm{s}) + \bm{\beta}_{(p),vv}(\bm{s}) \right\} - \hat{\bm{\beta}}_{(p)}(\bm{s}) \right]
            \end{align}

            From Lemma 2 of \cite{Sun-Yan-Zhang-Lu-2014}, $\left\{ n^{-1} \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{(-p)}(\bm{s}) \right\} = O_p(1)$ and $\left\{ n^{-1} \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{(p)}(\bm{s}) \right\} = O_p(1)$.
        
            From Theorem 3 of \cite{Sun-Yan-Zhang-Lu-2014}, we have that $h \sqrt{n} \left[ \hat{\bm{\beta}}_{(-p)} (\bm{s}) - \bm{\beta}_{(-p)}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \beta_{(p),uu}(\bm{s}) + \beta_{(p),vv}(\bm{s}) \right\}\right] = O_p(1)$ and $h \sqrt{n} \left[ \hat{\bm{\beta}}_{(p)}(\bm{s}) - \bm{\beta}_{(p)}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \beta_{(p),uu}(\bm{s}) + \beta_{(p),vv}(\bm{s}) \right\} \right] = O_p(1)$.
        
            So the second and third terms of the sum in (\ref{eq:selection}) are $O_p(1)$.
        
            We showed in the proof of \ref{theorem:consistency} that $h  \sqrt{n} \bm{Z}_{(p)}^T(\bm{s}) \bm{W}(\bm{s}) \left[ \bm{Y} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \bm{\beta}_{uu}(\bm{s}) + \bm{\beta}_{vv}(\bm{s}) \right\} \right]= O_p(1)$.

            The three terms of the sum to the right of the equals sign in (\ref{eq:selection}) are $O_p(1)$, so for $\hat{\bm{\beta}}_{(p)} (\bm{s})$ to be a solution, we must have that $h n^{-1/2} \lambda_p \frac{ \hat{\bm{\beta}}_{(p)} (\bm{s}) }{\| \hat{\bm{\beta}}_{(p)} (\bm{s}) \|} = O_p(1)$.

            But since by assumption $\hat{\bm{\beta}}_{(p)} (\bm{s}_i) \ne 0$, there must be some $k \in \{ 1, \dots, d_p \}$ such that $ | \hat{\beta}_{(p),k} (\bm{s}) | = \max \{ | \hat{\beta}_{(p),k'} (\bm{s}) | : 1 \le k' \le d_p \} $. And for this $k$, we have that $| \hat{\beta}_{(p),k} (\bm{s}) | / \| \hat{\bm{\beta}}_{(p)} (\bm{s}) \| \ge 1 / \sqrt{d_p} > 0$.

            Now since $h n^{-1/2} b_n \to \infty$, we have that $h n^{-1/2} \lambda_p \frac{ \hat{\bm{\beta}}_{(p)} (\bm{s}) }{\| \hat{\bm{\beta}}_{(p)} (\bm{s}) \|} \ge h n^{-1/2} b_n d_p^{-1/2} \to \infty$ and therefore the term to the left of the equals sign dominates the sum to the right of the equals sign in (\ref{eq:selection}). So for large enough $n$, $\hat{\bm{\beta}}_{(p)} (\bm{s}) \ne 0$ cannot maximize $Q$.
        
            So $P \left\{ \hat{\bm{\beta}}_{(b)} (\bm{s}) = 0 \right\} \to 1$.
        \end{proof}


    \subsection{A note on rates}
        To prove the oracle properties of LAGR, we assumed that $h^{-1} n^{-1/2} a_n \xrightarrow{p} 0$ and $h n^{-1/2} b_n \xrightarrow{p} \infty$. Therefore, $h^{-1} n^{-1/2} \lambda_n  \to 0$ for $j \le p_0$ and $h n^{-1/2} \lambda_n \| \bm{\beta}_j(\bm{s}) \|^{-\gamma} \to \infty$ for $j > p_0$.
        
        We require a $\lambda_n$ that can satisfy both assumptions. Suppose $\lambda_n = n^{\alpha}$, and recall that $h = O(n^{-1/6})$. Then $h^{-1} n^{-1/2} \lambda_n = O(n^{-1/3 + \alpha})$ and $h n^{-1/2} \lambda_n (h \sqrt{n})^{\gamma} = O(n^{-2/3 + \alpha + \gamma/3})$.
        
        So $ (2 - \gamma)/3 < \alpha < 1/3 $, which can only be satisfied for $\gamma > 1$.

\section{References}
\bibliographystyle{chicago}
\bibliography{../../references/gwr}

\end{document}  