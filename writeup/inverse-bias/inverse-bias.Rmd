---
title: "inverse-bias"
author: "Wesley Brooks"
date: "February 11, 2015"
output: html_document
---


#Introduction


#Local polynomial regression


#Local adaptive grouped regularization


#The Bayesian bootstrap
The bootstrap is a nonparametric method for estimating the distribution of a random variable. The Bayesian bootstrap (BB) is a related method. 

#Bias correction
The local coefficients in a varying coefficient regression model are estimated with bias. The bias arises because the coefficient surfaces are estimated by taking a kernel-weighted mean, approximating the coefficient sufaces as locally linear. Since in fact the coefficient surfaces may be curving, the kernel-weighted mean estimates a secant of the coefficient surface, rather than a tangent. Thus, the estimate is biased in the direction of the local curvature. Computing valid confidence intervals for the coefficients requires correcting the bias.

Knowing how the bias arises, we attempt to remove it by inverting the process. The estimate of the coefficient $\beta_1(s)$ is given by

$$\hat{\beta}_1(s) \approx \int_{\mathbb{R}} (1 \;\; t-s) (\beta(s) \;\; \nabla\beta(s))^T K_h(\|t-s\|) f(t) {\rm d}t$$

Or

\begin{align}
	\hat{\beta}_1(s) &= (X^T(s) \;\; W(s) \;\; X(s))^{-1} X^T(s) W(s) Y\\
	&\to \sum_{i=1}^n K_h(\|s-s_i\|) ((1 \;\; s-s_i) (1 \;\; s-s_i)^T)^{-1} (1 \;\; s-s_i) (\beta(s) \;\; \nabla\beta(s))
\end{align}


 
A concrete example may help illustrate how this is done. Consider a regression model where the true coefficients are the functions plotted in Figure 1.

```{r setup, echo=FALSE, message=FALSE, cache=TRUE, include=FALSE}
source("sim.r")
```

```{r truth, echo=FALSE, fig.width=10}
source("plot.r")
```

The solid lines are the true coefficients and the dashed lines are the smoothed versions that are estimable by kernel smoothing. Since the 

#Simulation


#Example

