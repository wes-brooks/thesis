---
title: "Linked bootstrap"
author: "Wesley Brooks"
output:
    pdf_document:
        fig_caption: true   
bibliography: ../../references/refs.bib
---

# Introduction


# Local polynomial regression


# Inference

## Parametric bootstrap

### Naive parametric bootstrap
We require a parametric model from which to make parametric bootstrap draws. For local polynomial regression, the model used to make the draws is the one that uses all available covariates. An independent model is generated at each observation location.
\begin{gather*}
\left\{ f(\bm{s})h^{2}n\right\} ^{1/2}\left[\hat{\bm{\beta}}_{(a)}(\bm{s})-\bm{\beta}_{(a)}(\bm{s})-(2\kappa_{0})^{-1}\kappa_{2}h^{2}\left\{ \nabla_{uu}^{2}\bm{\beta}_{(a)}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{(a)}(\bm{s})\right\} \right]\\
\xrightarrow{d}N\left(0,\kappa_{0}^{-2}\nu_{0}\sigma^{2}\Psi_{(a)}(\bm{s})^{-1}\right),
\end{gather*}
where $\left\{ \nabla_{uu}^{2}\bm{\beta}_{(a)}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{(a)}(\bm{s})\right\} =\left(\nabla_{uu}^{2}\bm{\beta}_{1}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{1}(\bm{s}),\dots,\nabla_{uu}^{2}\bm{\beta}_{p_{0}}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{p_{0}}(\bm{s})\right)^{T}$.

Denote bootstrap draws from the distribution for $\hat{\bm{\beta}}(\bm{s})$ by $\bm{\beta}^*_k(\bm{s})$ for $k = 1, \dots, B$. Then the corresponding response values are
\begin{align*}
y^*_k(\bm{s}) &= \bm{x}(\bm{s}) \bm{\beta}^*_k(\bm{s}) + \varepsilon(\bm{s}) \\
&\text{for  } i=1, \dots, B
\end{align*}

Now we have the resampled responses at $\bm{s}$: $y^*_1(\bm{s}), \dots, y^*_B(\bm{s})$ from the correct marginal distribution, but the draws at $\bm{s}$ are unconditional on the bootstrap draws at location $\bm{t}$.

In truth, the 

### Linked parametric bootstrap
The linked parametric bootstrap is used to generate draws of the coefficient functions with dependence between locations. This is accomplished by generating a single random vector, and multiplying it by the coefficient covariance matrix at each location.

\begin{align*}
    \hat{\bm{\beta}}(\bm{s}) &\sim N(\hat{\bm{\mu}}(\bm{s}), \Sigma(\bm{s})) \\
    \bm{b}_k &\sim N(0, I_p) \\
    \bm{\beta}^{**}_k(\bm{s}) &= \hat{\bm{\mu}}(\bm{s}) + \Sigma(\bm{s})^{1/2} \bm{b}_i \\
    &\text{for } k=1, \dots, B
\end{align*}

So that $\bm{\beta}^{**}(\bm{s})$ has the same marginal distribution as $\bm{\beta}^*_k(\bm{s})$ but the resampled functions are smoother than under the naive parametric bootstrap.

# Example
The method is illustrated by an example where the data arises from a varying coefficient model with a one-dimensional effect-modifying parameter, $t \in [0,1]$. The data is generated by:
\begin{align*}
y(t) &= \beta_0(t) + x(t)\beta_1(t) + \varepsilon(t)\\
\beta_0(t) &= 1 - 4(t-2) + 0.7(t-1)^2 + t^3 - 0.2t^4 \\
\beta_1(t) &= \cos{t} \\
X &\sim^{iid} N(3,2) \\
\varepsilon &\sim^{iid} N(0,1)
\end{align*}

```{r truth, warning=FALSE, error=FALSE, echo=FALSE, cache=FALSE}
set.seed(11181982)

#Set constants:
B = 1
n = 100
h = 1.5

#Simulate data:
f1 = function(x) {-0.2*x^4 + x^3 + 0.7*(x-1)^2 - 4*(x-2) + 1}
f2 = function(x) {cos(x)}
tt = seq(0, 5, len=n)
X = rnorm(n, mean=3, sd=2)
y = f1(tt) + X*f2(tt) + rnorm(n)
```

The response and the coefficients are plotted below:

```{r plot-truth, fig.width=10, fig.height=5, warning=FALSE, error=FALSE, echo=FALSE, fig.cap="From left to right, the simulated response, the intercept function and the coefficient function used to simulate the data."}
layout(matrix(1:3, 1, 3))
plot(x=tt, y=y, xlab='t', ylab='y')
plot(x=tt, y=f1(tt), xlab='t', ylab=expression(beta[0]), type='l')
plot(x=tt, y=f2(tt), xlab='t', ylab=expression(beta[1]), type='l')
```

```{r imports, results='hide', warning=FALSE, error=FALSE, echo=FALSE, include=FALSE} 
#Import packages:
library(lagr)
library(MASS)
library(expm)
library(mgcv)
```

```{r parametric-bootstrap, results='hide', warning=FALSE, error=FALSE, echo=FALSE, cache=FALSE}
#Fit the local poynomial models:
m = list()
fitted = vector()
resid = vector()
estimate = vector()

for (i in 1:n) {
    w = epanechnikov(abs(tt-tt[i]), h)
    X.loc = cbind(X, tt-tt[i], (tt-tt[i])*X)
    m[[i]] = lm(y~X.loc, weights=w)
    fitted = c(fitted, m[[i]]$fitted[i])
    resid = c(resid, m[[i]]$resid[i])
    estimate = c(estimate, m[[i]]$coef[3])
}

#Draw coefficients two ways:
#1) independent bootstrap
#2) Linked bootstrap
#Lists to hold the results:
beta.hat1 = list()
beta.hat2 = list()
y.hat1 = list()
y.hat2 = list()

#Bootstrap draws:
for (j in 1:B) {
    cat(paste("j is ", j, "\n", sep=""))
    beta.hat1[[j]] = matrix(NA,2,0)
    y.hat.seed = as.matrix(rnorm(4))
    beta.hat2[[j]] = matrix(NA,2,0)
    y.hat1[[j]] = vector()
    y.hat.seed = as.matrix(rnorm(4))
    y.hat2[[j]] = vector()
    
    for (i in 1:n) {
        Sigma = with(summary(m[[i]]), cov.unscaled * sigma^2)
        y.hat1[[j]] = c(y.hat1[[j]], mvrnorm(1, mu=m[[i]]$coef, Sigma=Sigma)[1])
        y.hat2[[j]] = c(y.hat2[[j]], (m[[i]]$coef + (sqrtm(Sigma) %*% y.hat.seed))[1])
        
        beta.hat1[[j]] = cbind(beta.hat1[[j]], mvrnorm(1, mu=m[[i]]$coef, Sigma=Sigma)[1:2])
        beta.hat2[[j]] = cbind(beta.hat2[[j]], (m[[i]]$coef + (sqrtm(Sigma) %*% y.hat.seed))[1:2])
    }
}
```

The parametric bootstrap is used to simulate the distribution of the estimated coefficient functions. The simulated response $y$ is plotted in Figure \ref{fig:fig1} along with a bootstrap resample $y^*$ from the NPB and a resample $y^{**}$ from the LPB. There is no apparent consistent difference between the two kinds of bootstrap resamples $y^*$ and $y*{**}$ - note that the resamples fall near each other with neither one consistently greater than the other in the left-hand lot, and generally along the $1-1$ line in the right-hand plot. There is an apparent tendency for the resampled data to "trim the peaks" and "fill the valleys" of the response, which we will return to.

```{r fig1, fig.width=10, fig.height=5, warning=FALSE, error=FALSE, echo=FALSE, fig.cap="The simulated response, and the response as resampled by the naive parametric bootstrap and the linked parametric bootstrap."}
layout(matrix(1:2,1,2))

#plot of the response (y):
fit1 = diag(t(beta.hat1[[1]]) %*% rbind(1, X))
fit2 = diag(t(beta.hat2[[1]]) %*% rbind(1, X))
yy = range(c(y, fit1, fit2))

plot(x=tt, y=y, xlab='t', ylab='response', ylim=yy, cex=0.6, bty='n')
par(new=TRUE)
plot(x=tt, y=fit1, ann=FALSE, xaxt='n', yaxt='n', col='red', pch=2, ylim=yy, cex=0.6, bty='n')
par(new=TRUE)
plot(x=tt, y=fit2, ann=FALSE, xaxt='n', yaxt='n', col='blue', pch=3, ylim=yy, cex=0.6, bty='n')
legend(x='top', legend=c("Truth", "Naive", "Linked"), pch=c(1,2,3), col=c("black", "red", "blue"), bty='n', cex=0.8)

#Plot the two responses against each other:
plot(fit1, fit2, xlab="Naive parametric bootstrap", ylab="Linked parametric bootstrap")
abline(0,1, lty=2)
```

In Figure \ref{fig:paraboot-estimates}, the true $\beta_0(t)$ and $\beta_1(t)$ coefficient surfaces that were used to simulate the data are plotted along with example draws of $\beta_0^*(t)$, $\beta_0^{**}(t)$, $\beta_1^*(t)$, and $\beta_1^{**}(t)$. It is immediately obvious that the linked bootstrap draws are smoother than the naive bootstrap draws, and the linked bootstrap draws appear to better resemble the truth.

```{r paraboot-estimates, fig.width=10, fig.height=5, warning=FALSE, error=FALSE, echo=FALSE, fig.cap="At left, the true coefficient function $\\beta_0(t)$ (black solid line) and the resampled $\\beta_0^*(t)$ from the naive parametric bootstrap (red) and $\\beta_0^{**}(t)$ from the linked parametric bootstrap (blue). At right, the true coefficient function $\\beta_1(t)$ (black solid line) and the resampled $\\beta_1^*(t)$ from the naive parametric bootstrap (red) and $\\beta_1^{**}(t)$ from the linked parametric bootstrap (blue)."}
layout(matrix(1:2,1,2))

#plot of the Intercept (\beta_0):
bb1 = range(f1(tt), beta.hat1[[1]][1,], beta.hat2[[1]][1,])
plot(x=tt, y=f1(tt), type='l', xlab='t', ylab='response', ylim=bb1, bty='n')
par(new=TRUE)
plot(x=tt, y=beta.hat1[[1]][1,], type='l', ann=FALSE, xaxt='n', yaxt='n', col='red', lty=2, ylim=bb1, bty='n')
par(new=TRUE)
plot(x=tt, y=beta.hat2[[1]][1,], type='l', ann=FALSE, xaxt='n', yaxt='n', col='blue', lty=2, ylim=bb1, lwd=2, bty='n')
legend(x='bottom', legend=c("Truth", "Naive", "Linked"), lty=c(1,2,2), lwd=c(1,1,2), col=c("black", "red", "blue"), bty='n', cex=0.8)

#plot of the local coefficient (\beta_1):
bb2 = range(f2(tt), beta.hat1[[1]][2,], beta.hat2[[1]][2,])
plot(x=tt, y=f2(tt), type='l', xlab='t', ylab='response', ylim=bb2, bty='n')
par(new=TRUE)
plot(x=tt, y=beta.hat1[[1]][2,], type='l', ann=FALSE, xaxt='n', yaxt='n', col='red', lty=2, ylim=bb2, lwd=1, bty='n')
par(new=TRUE)
plot(x=tt, y=beta.hat2[[1]][2,], type='l', ann=FALSE, xaxt='n', yaxt='n', col='blue', lty=2, ylim=bb2, lwd=2, bty='n')
legend(x='top', legend=c("Truth", "Naive", "Linked"), lty=c(1,2,2), lwd=c(1,1,2), col=c("black", "red", "blue"), bty='n', cex=0.8)
```


```{r smooth-residuals, fig.width=5, fig.height=5, warning=FALSE, error=FALSE, echo=FALSE, fig.cap="Smoothing spline used to smooth the residuals."}
g1 = gam(resid~s(tt, k=50))
s2 = g1$sig2

plot(x=tt, y=resid, bty='n', xlab='t', ylab='residual')
par(new=TRUE)
plot(x=tt, y=fitted(g1), type='l', ylim=range(resid), ann=FALSE, xaxt='n', yaxt='n', bty='n')
```

Because the local polynmial model tends to oversmooth the response, we use a spline model to estimate the local bias of the bootstrap response draws, in order to reduce the bias in the resampled data. The residuals from the original model fit are plotted in Figure \ref{fig:smooth-residuals} along with the spline used to smooth the residuals.

