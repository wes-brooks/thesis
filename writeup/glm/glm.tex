\documentclass[authoryear, review, 11pt]{elsarticle}

\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{rotating}
\usepackage[nolists,nomarkers]{endfloat}
\DeclareDelayedFloatFlavour{sidewaystable}{table}

\usepackage{relsize}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{booktabs}


\usepackage{setspace}
\setstretch{2}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}



\title{Oracle properties of local adaptive grouped regularization}
\author{Wesley Brooks}
\date{}                                           % Activate to display a given date or no date


\begin{document}

%\begin{abstract}
%Researchers who analyze spatial data often wish to discern how a certain response variable is related to a set of covariates. When it is believed that the effect of a given covariate may be different at different locations, a spatially varying coefficient regression model, in which the effects of the covariates are allowed to vary across the spatial domain, may be appropriate. In this case, it may be the case that the covariate has a meaningful association with the response in some parts of the spatial domain but not in others. Identifying the covariates that are associated with the response at a given location is called local model selection. Geographically weighted regression, a kernel-based method for estimating the local regression coefficients in a spatially varying coefficient regression model, is considered here. A new method is introduced for local model selection and coefficient estimation in spatially varying coefficient regression models. The idea is to apply a penalty of the elastic net type to a local likelihood function, with a local elastic net tuning parameter and a global bandwidth parameter selected via information criteria. Simulations are used to evaluate the performance of the new method in model selection and coefficient estimation, and the method is applied to a real data example in spatial demography.
%\end{abstract}

\maketitle
%\section{}
%\subsection{}

%Pastebin:


\section{Spatially varying coefficients regression \label{section:SVCR}}
	\subsection{Model}	
	Consider $n$ data points, observed at sampling locations $\bm{s}_i = (s_{i,1} \;\; s_{i,2})^T$ for $i = 1, \dots, \bm{s}_n$, which are distributed in a spatial domain $D \subset \mathbb{R}^2$ according to a density $f(\bm{s})$. For $i = 1, \dots, n$, let $y(\bm{s}_i)$ and $\bm{x}(\bm{s}_i)$ denote, respectively, the univariate response and the $(p+1)$-variate vector of covariates measured at location $\bm{s}_i$. At each observation location $\bm{s}_i$, assume that the outcome is related to the covariates by a generalized linear model (GLM) where the coefficients $\bm{\beta}(\bm{s}_i)$ may be spatially varying. That is, the distribution of $Y$ conditional on $X$ is:
    \begin{align*}
        f_{Y|X} (y|x) = \exp \left\{ y \right\}
    \end{align*}

	\begin{align}\label{eq:glm(s)}
        \mu(\bm{s}_i) &= g^{-1} \left\{ \eta(\bm{s}_i) \right\} \\
		\eta(\bm{s}_i) &= \bm{x}(\bm{s}_i)' \bm{\beta}(\bm{s}_i) 
	\end{align}

    In the context of nonparametric regression, the boundary-effect bias can be reduced by local polynomial modeling, usually in the form of a locally linear model \citep{Fan-1996}. Here, locally linear coefficients are estimated by augmenting the local design matrix with covariate-by-location interactions in two dimensions as proposed by \cite{Wang:2008b}. The augmented local design matrix at location $\bm{s}_i$ is
    \begin{align}
		\bm{Z}(\bm{s}_i) = \left( \bm{X}  \:\: L_i \bm{X} \:\: M_i \bm{X} \right)
	\end{align} 
  
	where $\bm{X}$ is the unaugmented matrix of covariates, $L_i = \text{diag}\{s_{i'_1} - s_{i_1}\}$ and $M_i = \text{diag}\{s_{i'_2} - s_{i_2}\}$ for $i' = 1, \dots, n$.

    Now we have that $Y(\bm{s}_i) = \left\{ \bm{Z}(\bm{s}_i) \right\}^T_i \bm{\zeta}(\bm{s}_i) + \varepsilon(\bm{s}_i)$, where $\left\{ \bm{Z}(\bm{s}_i) \right\}^T_i$ is the $i$th row of the matrix $\bm{Z}(\bm{s}_i)$ as a row vector, and $\bm{\zeta}(\bm{s}_i)$ is the vector of local coefficients at location $\bm{s}_i$, augmented with the local gradients of the coefficient surfaces in the two spatial dimensions, indicated by $\nabla_u$ and $\nabla_v$:

    \begin{equation*}
        \bm{\zeta}(\bm{s}_i) = \left( \bm{\beta}(\bm{s}_i)^T \;\; \nabla_u \bm{\beta}(\bm{s}_i)^T \;\; \nabla_v \bm{\beta}(\bm{s}_i)^T \right)^T
    \end{equation*}
  
    \subsection{Estimation}		
    The total log-likelihood of the observed data is the sum of the log-likelihood of each individual observation:
    \begin{align} \label{eq:coefficients}
        \ell \left\{ \bm{\zeta} \right\} = -(1/2) \sum_{i=1}^n \left[ \log{ \sigma^2}  + \sigma^{-2}  \left\{ y(\bm{s}_i) - \bm{z}'(\bm{s}_i) \bm{\zeta}(\bm{s}_i) \right\}^2 \right].
	\end{align}
	
	Since there are a total of $n \times 3(p+1) + 1$ parameters for $n$ observations, the model is not identifiable and it is not possible to directly maximize the total likelihood. But since the coefficient functions are smooth, the coefficients at location $\bm{s}$ can approximate the coefficients within some neighborhood of $\bm{s}$, with the quality of the approximation declining as the distance from $\bm{s}$ increases.

    This intuition is formalized by the local likelihood, which is maximized at location $\bm{s}$ to estimate the local coefficients $\bm{\zeta}(\bm{s})$:
    \begin{align}\label{eq:local-likelihood}
		\mathcal{L} \left\{ \bm{\zeta}(\bm{s}) \right\} &= \prod_{i=1}^n \left\{ \left(2 \pi \sigma^2  \right)^{-1/2}  \exp \left[ -(1/2) \sigma^{-2}  \left\{ y(\bm{s}_i) - \bm{z}'(\bm{s}_i) \bm{\zeta}(\bm{s}) \right\}^2 \right] \right\} ^ {K_h( \| \bm{s} - \bm{s}_i \| )},
	\end{align}
 
    The weights are computed from a kernel function $K_h(\cdot)$ such as the Epanechnikov kernel:
    \begin{align}\label{eq:epanechnikov}
        K_h(\| \bm{s}_i - \bm{s}_{i'}\|) &= h^{-2} K\left( h^{-1} \| \bm{s}_i - \bm{s}_{i'}\| \right) \notag \\
        K(x) &= \begin{cases} (3/4) (1-x^2) &\mbox{ if } x < 1, \\ 0 &\mbox{ if } x \geq 1. \end{cases}
	\end{align}
  
    Thus, the local log-likelihood function is, up to an additive constant: 
    \begin{align}\label{eq:local-log-likelihood}
		\ell \left\{ \bm{\zeta}(\bm{s}) \right\} &= -(1/2) \sum_{i=1}^n K_h( \| \bm{s} - \bm{s}_i \| ) \left[ \log{\sigma^2}  + \sigma^{-2}  \left\{ y(\bm{s}_i) - \bm{z}'(\bm{s}_i) \bm{\zeta}(\bm{s}) \right\}^2 \right].
    \end{align}
  
    Letting $\bm{W}(\bm{s})$ be a diagonal weight matrix where $W_{ii}(\bm{s}) = K_h( \| \bm{s} - \bm{s}_i \| )$, the local likelihood is maximized by weighted least squares:
    \begin{align}\label{eq:zeta-hat}
        \mathcal{S} \left\{ \bm{\zeta} (\bm{s}) \right\} &= (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T \notag \\
        \therefore \tilde{\bm{\zeta}}(\bm{s}) &= \left\{ \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\}^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Y}
    \end{align}

%    Now define the penalized weighted squared error loss $Q$:

%    \begin{equation*}
%        Q \left\{ \bm{\zeta} (\bm{s}) \right\} = (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T + \sum_{j=1}^p \phi_j(\bm{s}) \| \bm{\zeta}(\bm{s}) \|.
%    \end{equation*}
    
%    From (\ref{eq:local-log-likelihood}), the maximum local likelihood estimate $\hat{\sigma}_i^2$ is:	 
%    \begin{align}
%        \hat{\sigma}^2(\bm{s}) = \left\{ \sum \limits_{i=1}^{n} K_h( \| \bm{s} - \bm{s}_i \| ) \right\}^{-1} \sum \limits_{i=1}^n K_h( \| \bm{s} - \bm{s}_i \| ) \left\{ y(\bm{s}_i) - \bm{z}'(\bm{s}_i) \hat{\bm{\zeta}}(\bm{s}) \right\}^2
%    \end{align}
	 

\section{Local variable selection and parameter estimation \label{section:model-selection}}
	\subsection{Local variable selection}
	Local adaptive grouped regularization (LAGR) is explored as a method of local variable selection and coefficient estimation in SVCR models. The proposed LAGR penalty is an adaptive $\ell_1$ penalty akin to the adaptive group lasso \citep{Wang-Leng-2008,Zou:2006}.

    Grouped variables are selected together for inclusion in the model. Each group in a LAGR model consists of one covariate and its gradients on the two dimensions of spatial location. That is, $\bm{\zeta}_j(\bm{s}) = \left( \beta_j(\bm{s}) \;\;\; \nabla_u \beta_j(\bm{s}) \;\;\; \nabla_v \beta_j(\bm{s}) \right)^T$ for $j=1, \dots, p$.
	
	The objective function for the LAGR at location $\bm{s}$ is the penalized local sum of squares:
	\begin{align}\label{eq:adaptive-lasso-WLS}
		Q \{ \bm{\zeta}(\bm{s}) \} &= \mathcal{S} \left\{ \bm{\zeta}( \bm{s} ) \right\} + \mathcal{J}\{ \bm{\zeta}(\bm{s}) \} \notag \\
		&= (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T + \sum_{j=1}^p \phi_j(\bm{s}) \| \bm{\zeta}_j(\bm{s}) \| 
	\end{align}
	
	which is the sum of the weighted sum of squares $\mathcal{S} \left\{ \bm{\zeta}( \bm{s} ) \right\}$ and the LAGR penalty $\mathcal{J}\{ \bm{\zeta}(\bm{s}) \}$.

    The LAGR penalty for the $j$th group of coefficients $\bm{\zeta}_j(\bm{s})$ at location $\bm{s}$ is $\phi_j(\bm{s}) = \lambda_n (\bm{s}) \| \tilde{\bm{\zeta}}_j(\bm{s}) \|^{-\gamma}$, where $\lambda_n (\bm{s}) > 0$ is a the local tuning parameter applied to all coefficients at location $\bm{s}$ and $\tilde{\bm{\zeta}}_j (\bm{s})$ is the vector of unpenalized local coefficients from (\ref{eq:zeta-hat}).


    \subsection{Computation}
        \subsubsection{Tuning parameter selection}
        Implementing LAGR requires the selection of local tuning parameters. The criteria commonly used for selecting tuning parameters in lasso-type models are appropriate here, including GCV \citep{Wahba:1990}, Cp \citep{Mallows-1973}, AIC \citep{Akaike-1973}, and BIC \citep{Schwarz-1978}. All of the examples and simulations presented herein used the corrected AIC (AICc) \citep{Hurvich-1998} for tuning parameter selection:

        \begin{equation}
            \text{AIC}_{\text{c}}(\bm{s}) = \hat{\sigma}^{-2}(\bm{s}) \|\bm{Y} - \bm{Z}(\bm{s}) \hat{\bm{\zeta}}(\bm{s}) \|^2 + 2df + 2\frac{df (df+1)}{\sum_{i=1}^n K_h(\| \bm{s} - \bm{s}_i \|)-df-1}
        \end{equation}

        where $df$ is the degrees of freedom as defined in \cite{Yuan-Lin-2006}:

        \begin{equation}
            df = \sum_{j=1}^p I \left\{ \| \hat{\bm{\zeta}}_j(\bm{s}) \| \right\} + 2 \sum_{j=1}^p \frac{\| \hat{\bm{\zeta}}_j(\bm{s}) \|}{\| \tilde{\bm{\zeta}}_j(\bm{s}) \|}
        \end{equation}


\section{References}
\bibliographystyle{chicago}
\bibliography{../../references/gwr}

\end{document}  