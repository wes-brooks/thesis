---
title: "Bias correction for local polynomial regression estimates of varying coefficient models"
author: "Wesley Brooks"
date: "February 11, 2015"
output: html_document
---


#Introduction


#Local polynomial regression


#Local adaptive grouped regularization


#The Bayesian bootstrap
The bootstrap is a nonparametric method for estimating the distribution of a random variable. The Bayesian bootstrap (BB) is a related method. 

#Bias correction
The local coefficients in a varying coefficient regression model are estimated with bias. The bias arises because the coefficient surfaces are estimated by taking a kernel-weighted mean, approximating the coefficient sufaces as locally linear. Since in fact the coefficient surfaces may be curving, the kernel-weighted mean estimates a secant of the coefficient surface, rather than a tangent. Thus, the estimate is biased in the direction of the local curvature. Computing valid confidence intervals for the coefficients requires correcting the bias.

Knowing how the bias arises, we attempt to remove it by inverting the process. The estimate of the coefficient $\beta_1(s)$ is given by

$$\hat{\beta}_1(s) \approx \int_{\mathbb{R}} (1 \;\; t-s) (\beta_1(s) \;\; \nabla\beta_1(s))^T K_h(\|t-s\|) f(t) {\rm d}t$$

Or

\begin{align}
	\hat{\beta}_1(s) &= (X^T(s) \;\; W(s) \;\; X(s))^{-1} X^T(s) W(s) \beta_1\\
	&\to \sum_{i=1}^n K_h(\|s-s_i\|) ((1 \;\; s-s_i) (1 \;\; s-s_i)^T)^{-1} (1 \;\; s-s_i) (\beta(s) \;\; \nabla\beta(s))
\end{align}

And the slope of the coefficient is estimated by

$$\hat{\nabla\beta}_1(s) \approx \int_{\mathbb{R}} \nabla\beta_1(s) K_h(\|t-s\|) f(t) {\rm d}t$$

or

\begin{align}
	\hat{\nabla\beta}_1(s) &= (X^T(s) \;\; W(s) \;\; X(s))^{-1} X^T(s) W(s) \beta_1\\
	&\to \sum_{i=1}^n K_h(\|s-s_i\|) ((1 \;\; s-s_i) (1 \;\; s-s_i)^T)^{-1} (1 \;\; s-s_i) (\beta(s) \;\; \nabla\beta(s))
\end{align}

Estimating $\beta_1$ from $\hat{\beta}_1(s) \approx \int_{\mathbb{R}} (1 \;\; t-s) (\beta_1(s) \;\; \nabla\beta_1(s))^T K_h(\|t-s\|) f(t) {\rm d}t$ means solving a Fredholm integral equation of the first kind.

Since the method of local polynomial regression produces estimates of $\beta(s)$ and $\nabla\beta(s)$, we use them to solve the following integral equation simultaneously.

$${\rm min}_{c,d} (\hat{\beta}(t) - Q_1c - T_1d)^T(\hat{\beta}(t) - Q_1c - T_1d) + (\hat{\nabla\beta}(t) - Q_2c - T_2d)^T(\hat{\nabla\beta}(t) - Q_2c - T_2d) + \lambda c^T Q_0 c$$

In this objective function, $Q_0$ is an $n \times n$ matrix with $ij$th entry $R_{m,d}(t_i, t_j) = \|t_i - t_j\|^{2m-d} \log(\|t_i - t_j\|)$ for $d$ even, or $\|t_i - t_j\|^{2m-d}$ for $d$ odd, represents the thin plate spline basis [@Wahba-Wendelberger-1980]. $T_0$ is the $n \times d$ matrix representing the unpenalized "smooth" componenets of the thin plate splines. $Q_1 = W_1 Q_0$, $T_1 = W_1 T_0$, $Q_2 = W_2 Q_0$, and $T_2 = W_2 T_0$ are smoothed thin-plate spline representations of the coefficient surface and its derivative, where $W_1$ and $W_2$ are linear smoothing operators that emulate the process producing the kernel smoothing estimates.



A concrete example may help illustrate how this is done. Consider a regression model where the true coefficients are the functions plotted in Figure 1.

```{r setup, echo=FALSE, message=FALSE, cache=TRUE, include=FALSE}
source("sim.r")
```

```{r truth, echo=FALSE, fig.width=10}
source("plot.r")
```

The solid lines are the true coefficients and the dashed lines are the smoothed versions that are estimable by kernel smoothing. Since the 

#Simulation


#Example

