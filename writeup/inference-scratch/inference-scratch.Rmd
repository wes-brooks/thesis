---
title: "inference"
author: "Wesley Brooks"
output:
    pdf_document:
    fig_caption: true   
bibliography: ../../references/gwr.bib
---
    
# Introduction


###What are the goals of inference?
- Select the model
- Estimate the model parameters
- Estimate confidence intervals for the parameters

###What challenges are unique to inference in LAGR models?
- Selecting the bandwidth
- LAGR is not a linear smoother, so estimating degrees of freedom is difficult
- LAGR uses lasso, so estimating distributions of coefficient estimates is difficult

###Why use AIC?
- Likelihood is the basis for most statistical inference
- The likelihood can be used for model selection and estimation
- Using the same data for selection and estimation produces a downward-biased estimate of the likelihood
- The AIC is bias-corrected likelihood
- Computing AIC requires an expression for the degrees of freedom used in estimating a model
 
###Why use the parametric bootstrap?
- LAGR uses an $L_1$-norm penalty, which spoils the linearity of the estimates
- Thus, the normal distribution of the estimates for fixed model are only approximate
- Further, the model selection component of LAGR affects the coefficient estimates in unknown ways
- The bootstrap is used in the context of estimating parameter distributions post-model-selection in @Efron-2014
- Considered nonparametric bootstrap first, resampling $(X_i, y_i, \bm{s}_i)$ together
- Question arose then of prediction vs. fitting, since the only observation we have that arose from model $f(\bm{s}_j)$ is $(X_j, y_j)$
- And if we want to estimate 

Local adaptive grouped regularization (LAGR) is a method for local variable selection and local coefficient estimation in a varying coefficient regression (VCR) model [@Brooks-Zhu-Lu-2014]. Estimating a model via LAGR is straightforward. This paper addresses inference for a VCR model estimated by LAGR, focusing on the estimation of confidence intervals for the local coefficient estimates and estimation of which local coefficients should be shrunk to exactly zero.

The method of LAGR possesses the oracle property of asymptotically selecting exactly the correct variables and estimating them as accurately as if their identities were known in advance. For local selection and estimation, LAGR relies on a version of the adaptive group Lasso [@Yuan-Lin-2006, @Wang-Leng-2008]. Thus, local coefficients estimated by LAGR asymptotically acheive the distributions given in @Sun-Yan-Zhang-Lu-2014 and @Cai-Fan-Li-2000.

However, the asymptotic case is never realized in actual data analysis. Given a finite quantity of data, the set of covariates selected by LAGR is subject to uncertainty. Since coefficient estimates in a model estimated by LAGR are conditional on the selected covariates, the asymptotic expression for the distribution of the local coefficients is not useful for inference.

Further, while the local coefficient estimation is conditional on the local covariate selection, the local covariate selection is itself conditional on the bandwidth parameter $h$. In order to acheive the oracle properties, the optimal bandwidth for a LAGR model was shown to be $h_n=O(n^{-1/6})$ [@Brooks-Zhu-Lu-2014]. However, the optimal rate is not enough information to determine the bandwidth, and is anyhow irrelevant when $n$ is fixed, as is the case in most practical data analysis.

Statistical properties of estimators are typically established assuming \emph{a priori} model selection. The properties of estimates that are computed from the same data as was used for model selection are 

The properties of estimators after model selection is an area of active research. A typical approach in applications is to select a model that minimizes a selection criterion such as the AIC or BIC, and then proceed with estimation as though the model had been selected in advance. However, this leads to unreliable inference from the selected model [@Leeb-Potscher-2006]. It also ignores the discontinuous nature of estimation after model selection, which is well illustrated by Figures 8 and 9 of @Efron-2014.

Model averaging is a technique that attempts to estimate the model parameters without conditioning on the selected model. When prior distributions can be established for the candidate models and their parameters, Bayesian model averaging (BMA) is a principled approach to multimodel inference [@Hoeting-Madigan-Raftery-Volinsky-1999]. However, the establishing the prior distributions is not a trivial step, especially in a setting with many parameters, as VCR has. There is an analagous procedure called frequentist model averaging (FMA) that avoids the need to specify prior distributions. The asyptotic distribution of estimators derived from FMA has been worked out in the framework of "dwindling confidence", where coefficients decrease at a $\sqrt{n}$ rate as the sample size increases [@Hjort-Claeskens-2003]. The framework of dwindling confidence is similar to the framework of "moving parameter" asymptotics, under which the adaptive Lasso estimator fails to be consistent [@Potscher-Schneider-2009].

It is impossible to use maximum likelihood to estimate the bandwidth parameter, as reveled by a simple example: given a data set $(\bm{X}, Y, \bm{S})$, let $h=0$. Then $\hat{Y}=Y$ trivially, which results in the maximum possible likelihood. This is clearly an example of overfitting, because the model can tell us nothing about any future observations.

# Methods

## AIC step
We work within an information-theoretic framework where the optimal model is the one closest to truth $f$ in the  sense of Kullback-Leibler (KL) distance [@Kullback-Leibler-1951]. Since the truth $f$ is unknown, we are left to estimate the KL distance, which we do by means of the Akaike Information Criterion (AIC) [@Burnham-Anderson-2002, @Akaike-1973]. A model's AIC is an estimate of its expected KL distance from the truth. In fact, the AIC is an estimate of the log likelihood of an independent realization of the response, conditional on the observed covariates. The key to AIC is that a model's log likelihood is penalized by a factor equal to the degrees of freedom used in estimating the model.

Which model minimizes the AIC is not the only consideration. For model selection via LAGR, the AIC is seems to be quite discontiuous. What's more, small differences in the AIC are indicative of ambiguity in model selection. In this work we consider model averaging with model weights based on their AIC values. The smoothed AIC estimate is [@Burnham-Anderson-2002]
\begin{align}
\breve{\bm{\beta}}(\bm{s}) &= \sum_{j=1}^M w_j \hat{\bm{\beta}}_j(\bm{s}) / \sum_{k=1}^M w_k \\
w_j &= \exp ( - \Delta_j / 2 ) \\
\Delta_j &= \text{AIC}_j - \text{min}_k \text{AIC}_k
\end{align}

The smoothed AIC can be applied to any function of the estimated coefficients. In particular, the smoothed AIC can be used for prediction and to estimate whether a given coefficient is exactly zero.

## Bootstrap step
Even if the model were selected in advance, we lack an expression for the finite-sample distribution of the coefficients of a VCR model estimated by LAGR. We therefore turn to the bootstrap to estimate the finite-sample distribution of the coefficients. As it happens, we can capitalize on the bootstrap draws to smooth the discontinutiy in model selection as well.

We use the parametric bootstrap to draw from the full model at each location, then apply the method of LAGR to the bootstrap draw to get a new estimate $\breve{\bm{\beta}}^*(\bm{s})$ of the coefficients.

## Sorting property of adaptive Lasso
Given the form of a model and the $p$ covariates for consideration, there are $2^p$ possible regression models to consider. It is computationally impractical to test all of these models. The adaptive lasso provides a principled way of reducing the number of models to consider from $O(2^p)$ to $O(p)$ (proof?). Despite the sorting being "principled" it remains to show that the models are good, and that they are wisely sorted.

It has been shown that LAGR can asymptotically select exactly the "correct" covariates. In order to prove that property, we assumed that the tuning parameter $\lambda_n = n^{\alpha}$ for $\alpha \in \left( (2-\gamma)/3, 1/3 \right)$. This range of rates, though, isn't enough to decide exactly how to set the tuning parameter. In our case, we've used the AIC to tune the local variable selection.

At location $\bm{s}$, the sequence $\mathcal{M}_n$ is the sequence of models as $\lambda_n$ sweeps from $\lambda_{n,0}$ to $0$. The first element of $\mathcal{M}_n$ is always $m_0 = \bm{0}$, the null (or intercept-only) model. The last element of $\mathcal{M}_n$ is always the full model. Proposition \ref{proposition:sorting} suggests that with probability going to one, the KL-best model appears in the sequence.

\begin{proposition}
\label{proposition:sorting}
\emph{(Sorting Property)}
As $n \to \infty$, the models $\mathcal{M}_n(\bm{s})$ on the LAGR solution path at $\bm{s}$ approach a well-sorted state, meaning that the models are are nested such that the $m_0, m_1, \dots, m_{p_0}$ add one variable of the KL-best model at each step, and models $m_{p_0+1}, \dots, m_p$ each add one variable that is not in the KL-best model.
\end{proposition}


##Model averaging
We give weights to the models in the sequence based on their AIC values. If Proposition \ref{proposition:AIC-weighted-mean} holds, it indicates that the AIC weight vector converges to the weight vector that minimizes the KL distance between truth and the weighted model average. However, it is unclear whether Proposition \ref{proposition:AIC-weighted-mean} holds, since the largest difference we expect between a model that incorporates all the relevant covariates and one that is the same with one irrelevant covariate additionally is $2 \times (k+1)$, which is the complexity penalty for adding the new covariate and its interations with location, without any countervailing improvement in the model fit. That is, the model's AIC value incurs a fixed penalty for every irrelevant covariate, no matter how confident we are that that variable does not belong in the model.

In this way, AIC seems like a "greedy" model selection method, where there may be a large penalty for not including a relevant covariate (because the model fit would be so much worse) but a fixed (and relatively small) penalty for adding an irrelevant covariate.

\begin{proposition}
\label{proposition:AIC-weighted-mean}
\emph{(Consistency)}
Let $w(\bm{s})$ be a vector of model weights at $\bm{s}$ and $\hat{w}(\bm{s})$ be the vector of weights estimated by LAGR and the AIC. Then 
\begin{equation}
\frac{L (\hat{w}(\bm{s}))}{\inf_{w(\bm{s}) \in \mathcal{H}} L (w(\bm{s}))} \xrightarrow{P} 1
\end{equation}
where $L{w}$ is the K-L distance between truth and the weighted model average indicated by $w$.
\end{proposition}


#References
