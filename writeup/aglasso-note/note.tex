\documentclass[authoryear, review, 11pt]{elsarticle}

\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{rotating}
\usepackage[nolists,nomarkers]{endfloat}
\DeclareDelayedFloatFlavour{sidewaystable}{table}

\usepackage{relsize}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{booktabs}


\usepackage{setspace}
\setstretch{2}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}





\title{Local Variable Selection and Parameter Estimation of Spatially Varying Coefficient Regression Models}
\author{Wesley Brooks}
\date{}                                           % Activate to display a given date or no date


\begin{document}

%\begin{abstract}
%Researchers who analyze spatial data often wish to discern how a certain response variable is related to a set of covariates. When it is believed that the effect of a given covariate may be different at different locations, a spatially varying coefficient regression model, in which the effects of the covariates are allowed to vary across the spatial domain, may be appropriate. In this case, it may be the case that the covariate has a meaningful association with the response in some parts of the spatial domain but not in others. Identifying the covariates that are associated with the response at a given location is called local model selection. Geographically weighted regression, a kernel-based method for estimating the local regression coefficients in a spatially varying coefficient regression model, is considered here. A new method is introduced for local model selection and coefficient estimation in spatially varying coefficient regression models. The idea is to apply a penalty of the elastic net type to a local likelihood function, with a local elastic net tuning parameter and a global bandwidth parameter selected via information criteria. Simulations are used to evaluate the performance of the new method in model selection and coefficient estimation, and the method is applied to a real data example in spatial demography.
%\end{abstract}

\maketitle

	\subsection{Model}	
	Consider $n$ data points, observed at sampling locations $\bm{s}_1, \dots, \bm{s}_n$, which are distributed in a spatial domain $D \subset \mathbb{R}^2$ according to a density $f(\bm{s})$. For $i = 1, \dots, n$, let $y(\bm{s}_i)$ and $\bm{x}(\bm{s}_i)$ denote the univariate response variable, and a $(p+1)$-variate vector of covariates measured at location $\bm{s}_i$, respectively. At each location $\bm{s}_i$, assume that the outcome is related to the covariates by a linear model where the coefficients $\bm{\beta}(\bm{s}_i)$ may be spatially-varying and $\varepsilon(\bm{s}_i)$ is random error at location $\bm{s}_i$. That is,
	\begin{align}\label{eq:lm(s)}
		y(\bm{s}_i) = \bm{x}(\bm{s}_i)' \bm{\beta}(\bm{s}_i) + \varepsilon(\bm{s}_i).
	\end{align}
	
	Further assume that the error term $\varepsilon(\bm{s}_i)$ is normally distributed with zero mean and variance $\sigma^2$, and that $\varepsilon(\bm{s}_i)$, $i=1, \dots, n$ are independent. That is,
	\begin{align} \label{eq:err}
		\varepsilon(\bm{s}_i) \overset{iid}{\sim} \mathcal{N} \left( 0,\sigma^2 \right).
	\end{align}
	
  Thus, conditional on the design matrix $\bm{X}$, observations of the response variable at different locations are independent of each other.
  
  An SVCR model that estimates the regression coefficients as locally constant, as in the class of Nadaraya-Watson kernel smoothers \citep{Hardle-1990}, suffers the problem of biased estimation that is common to that class of models - particularly where there is a gradient to the coefficient surface at the boundary of the domain \citep{Hastie:1993b}.

  In the context of nonparametric regression, the boundary-effect bias can be reduced by local polynomial modeling, usually in the form of a locally linear model \citep{Fan-1996}. Here, locally linear coefficients are estimated by augmenting the local design matrix with covariate-by-location interactions in two dimensions as proposed by \cite{Wang:2008b}. The augmented local design matrix at location $\bm{s}_i$ is
  \begin{align}
    \bm{Z}(\bm{s}_i) = \left( \bm{X}  \;\;\; L(\bm{s}_i) \, \bm{X} \;\;\; M(\bm{s}_i) \, \bm{X} \right)
  \end{align} 
  
  where $\bm{X}$ is the unaugmented matrix of covariates, $L(\bm{s}_i) = \text{diag}\{ ( \bm{s}_{i'} - \bm{s}_{i} )_1 \}$ and $M(\bm{s}_i) = \text{diag}\{ ( \bm{s}_{i'} - \bm{s}_{i} )_2 \}$ for $i' = 1, \dots, n$.
  
  \subsection{Estimation}		
  The total log-likelihood of the observed data is the sum of the log-likelihood of each individual observation:
  \begin{align} \label{eq:coefficients}
  	\ell \left\{ \bm{\beta}(\bm{s}_i) \right\} = -(1/2) \sum_{i'=1}^n \left[ \log{\sigma^2(\bm{s}_i)}  + \sigma^{-2}(\bm{s}_i)  \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \bm{\beta}(\bm{s}_i) \right\}^2 \right].
  \end{align}
	
  Since there are a total of $n \times 3(p+1)$ parameters for $n$ observations, the model is not identifiable and it is not possible to directly maximize the total likelihood.
  
  The values of the local coefficients $\bm{\beta}(\bm{s}_i)$ are estimated by the weighted likelihood
  \begin{align}\label{eq:local-likelihood}
    \mathcal{L} \left\{ \bm{\beta}(\bm{s}_i) \right\} &= \prod_{i'=1}^n \left( \left\{ 2 \pi \sigma^2(\bm{s}_i)  \right\}^{-1/2}  \exp \left[- (1/2) \sigma^{-2}(\bm{s}_i)  \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \bm{\beta}(\bm{s}_i) \right\}^2 \right] \right) ^ {w_{ii'}},
  \end{align}
  
  where the weights are calculated by a kernel function $K_h(\cdot)$ such as the Epanechnikov kernel:
  \begin{align}\label{eq:epanechnikov}
    w_{ii'} &= K_h(\delta_{ii'}) = h^{-2} K\left( h^{-1} \delta_{ii'} \right) \notag \\
    K(x) &= \begin{cases} (3/4) (1-x^2) &\mbox{ if } \delta_{ii'} < h, \\ 0 &\mbox{ if } \delta_{ii'} \geq h. \end{cases}
  \end{align}
  
  Thus, the local log-likelihood function is, up to an additive constant: 
  \begin{align}\label{eq:local-log-likelihood}
    \ell\left(\bm{\beta}(\bm{s}_i)\right) &= -(1/2) \sum_{i'=1}^n w_{ii'} \left[ \log{\sigma^2(\bm{s}_i)}  + \sigma^{-2}(\bm{s}_i)  \left\{y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \bm{\beta}(\bm{s}_i) \right\}^2 \right].
  \end{align}
  
  This local likelihood can be maximized by weighted least squares
  \begin{align}\label{eq:beta-hat}
    \hat{\bm{\beta}}(\bm{s}_i) = \left\{ \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \bm{Z}(\bm{s}_i) \right\}^{-1} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \bm{Y}.
  \end{align}
	
  From (\ref{eq:local-log-likelihood}), the maximum local likelihood estimate $\hat{\sigma}^2(\bm{s}_i)$ is:	 
  \begin{align}
    \hat{\sigma}^2(\bm{s}_i) = \left(\sum \limits_{i'=1}^{n} w_{ii'} \right)^{-1} \sum \limits_{i'=1}^n w_{ii'} \left\{ y(\bm{s}_{i'}) - \bm{z}'(\bm{s}_{i'}) \hat{\bm{\beta}}(\bm{s}_i) \right\}^2
  \end{align}
  
  
\section{Asymptotics}
  \subsection{Consistency}
  \begin{theorem}\label{theorem:consistency}     
    If $h \sqrt{n} a_n \xrightarrow{p} 0$ then $\hat{\bm{\beta}}(\bm{s}) - \bm{\beta}(\bm{s}) - \frac{\kappa_2 h^2}{2 \kappa_0} \{ \bm{\beta}_{uu}(\bm{s}) + \bm{\beta}_{vv}(\bm{s}) \} = O_p(n^{-1/2} h^{-1} )$
  \end{theorem}
  
  \begin{proof}
    The idea of the proof is to show that the objective being minimized achieves a unique minimum, which must be $\hat{\bm{\beta}}(\bm{s})$.
    
    The order of convergence is $n^{1/2} h$ where $h = O(n^{-1/6})$.
    
    To show: that for any $\epsilon$, there is a sufficiently large constant $C$ such that
    \begin{align*}
      \liminf \limits_n P \left[ \inf_{u \in \mathcal{R}: \|u\| = C} Q \left\{ \bm{\beta}(\bm{s}) + h n^{-1/2} \bm{u} \right\} > Q \left\{ \bm{\beta}(\bm{s}) \right\} \right] > 1 - \epsilon
    \end{align*}
    
    We show the result:
    \begin{align}\label{eq:consistency}
      \mkern-18mu Q \left( \bm{\beta} (\bm{s}_i) + n^{-1/2} \bm{u} \right) - Q \left( \bm{\beta}(\bm{s}_i) \right) &= (1/2) \left[ \bm{Y} - \bm{Z}(\bm{s}_i) \left\{ \bm{\beta}(\bm{s}_i) + n^{-1/2} \bm{u} \right\} \right]^T \bm{W}(\bm{s}_i) \left[ \bm{Y} - \bm{Z}(\bm{s}_i) \left\{ \bm{\beta}(\bm{s}_i) + n^{-1/2} \bm{u} \right\} \right] \notag \\
      &+ n \sum_{j=1}^p \lambda_j \| \bm{\beta}(\bm{s}_i) + n^{-1/2} \bm{u} \| \notag \\
      &- (1/2) \left\{ \bm{Y} - \bm{Z} \bm{\beta}(\bm{s}_i) \right\}^T \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{Z} \bm{\beta}(\bm{s}_i) \right\} - n \sum_{j=1}^p \lambda_j \| \bm{\beta}(\bm{s}_i) \| \notag \\ 
      &\mkern-72mu= (1/2) \bm{u}^T \left\{ \frac{1}{n} \bm{Z}^T \bm{W}(\bm{s}_i) \bm{Z}(\bm{s}_i) \right\} \bm{u} - \bm{u}^T \left[ n^{-1/2} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{Z}(\bm{s}_i) \bm{\beta}(\bm{s}_i) \right\} \right] \notag \\
      &+ n \sum_{j=1}^p \lambda_j \|\bm{\beta}_j(\bm{s}_i) + n^{-1/2} \bm{u} \| - n \sum_{j=1}^p \lambda_j \| \bm{\beta}_j(\bm{s}_i) \| \notag \\
      &\mkern-72mu= (1/2) \bm{u}^T \left\{ \frac{1}{n} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \bm{Z}(\bm{s}_i) \right\} \bm{u} - \bm{u}^T \left[ n^{-1/2} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{Z}(\bm{s}_i) \bm{\beta}(\bm{s}_i) \right\} \right] \notag \\
      &+ n \sum_{j=1}^p \lambda_j \|\bm{\beta}_j(\bm{s}_i) + n^{-1/2} \bm{u} \| - n \sum_{j=1}^{p_0} \lambda_j \| \bm{\beta}_j(\bm{s}_i) \| \notag \\
      &\mkern-72mu\ge (1/2) \bm{u}^T \left\{ \frac{1}{n} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \bm{Z}(\bm{s}_i) \right\} \bm{u} - \bm{u}^T \left[ n^{-1/2} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{Z}(\bm{s}_i) \bm{\beta}(\bm{s}_i) \right\} \right] \notag \\
      &+ n \sum_{j=1}^{p_0} \lambda_j ( \|\bm{\beta}_j(\bm{s}_i) + n^{-1/2} \bm{u} \| - \| \bm{\beta}_j(\bm{s}_i) \| ) \notag \\
      &\mkern-72mu\ge (1/2) \bm{u}^T \left\{ \frac{1}{n} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \bm{Z}(\bm{s}_i) \right\} \bm{u} - \bm{u}^T \left[ n^{-1/2} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{Z}(\bm{s}_i) \bm{\beta}(\bm{s}_i) \right\} \right] \notag \\
      &- p_0 \sqrt{n} a_n \| \bm{u} \|
    \end{align}
  \end{proof}

  We'll consider the terms of the sum in (\ref{eq:consistency}) separately.
  
  \paragraph{First term.} By Lemma 2 of \cite{Sun-Yan-Zhang-Lu-2014}, $\frac{1}{n} \bm{Z}^T(\bm{s}_i) \bm{W}(\bm{s}_i) \bm{Z}(\bm{s}_i) \xrightarrow{p} \Omega$, so the first term in (\ref{eq:consistency}) converges to $\bm{u}^T \Omega \bm{u}$, a quadratic form in $\bm{u}$.
  
  \paragraph{Second term.} By a first-order Taylor expansion, we have that $\bm{\beta}(\bm{s}_i) = \bm{\beta}(\bm{s}) + \nabla \bm{\beta}(\bm{\xi}_{i}) (\bm{s}_i - \bm{s})$ where $\xi_i = \bm{s} + \theta (\bm{s}_i - \bm{s})$ and $\theta \in [0, 1]$ for $i = 1, \dots, n$. So
  \begin{align*}
    \bm{Y} - \bm{Z}(\bm{s}_i) \bm{\beta}(\bm{s}_i) &= \bm{m} + \bm{\varepsilon} - \bm{Z}(\bm{s}_i) \bm{\beta}(\bm{s}_i) \\
  \end{align*}
  
  and so the second term of (\ref{eq:consistency}) is
  \begin{align}\label{eq:linear-part}
    \bm{u}^T \left[ n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{m} + \bm{\varepsilon} - \bm{Z}(\bm{s}) \bm{\beta}(\bm{s})  \right\} \right].
  \end{align}
  
  We wish to show that (\ref{eq:linear-part}) is $O_p(1)$. Let $\{ \bm{A} \}_j$ be the $j$th column of the matrix $\bm{A}$ as a column vector, and $\{ \bm{A} \}_k^T$ be the $k$th row of the matrix $\bm{A}$ as a row vector. Now, taking the three terms of the sum separately:
  
  \paragraph{First term}
    Find the expectation and variance of the $i$th term in the sum $n^{-1/2} \bm{H}^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{m}$:

    \begin{align}\label{eq:m-term}
      \bm{H}^{-1} \; &\{ \bm{Z}^T(\bm{s}) \}_i \; \{ \bm{W}(\bm{s}) \}_{ii} \; m(\bm{s}_i) \notag \\ 
      &= K_h(\| \bm{s} - \bm{s}_i \|) \; \bm{H}^{-1} \; \{ \bm{Z}^T(\bm{s}) \}_i \; \{ \bm{Z}(\bm{s}_i) \}_i^T \; \bm{\gamma}(\bm{s}_i) \notag \\
      &= K_h(\| \bm{s} - \bm{s}_i \|) \left( \begin{array}{cccc} X_1^2(\bm{s}_i) & \dots & X_1(\bm{s}_i) X_p(\bm{s}_i) & \bm{0}_{1 \times 2p} \\ \vdots & \ddots & \vdots & \vdots \\ X_1(\bm{s}_i) X_p(\bm{s}_i) & \dots & X_p^2(\bm{s}_i) & \bm{0}_{1 \times 2p} \\ \bm{0}_{2p \times 1} & \dots & \bm{0}_{2p \times 1} & \bm{0}_{2p \times 2p} \end{array} \right) \bm{\gamma}(\bm{s}_i) \notag \\
      &= K_h(\| \bm{s} - \bm{s}_i \|) \left( \begin{array}{cc} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i)  & \bm{0}_{p \times 2p} \\ \bm{0}_{2p \times p} & \bm{0}_{2p \times 2p} \end{array} \right) \bm{\gamma}(\bm{s}_i)
    \end{align}

    So the expectation of (\ref{eq:m-term}) is:
    \begin{align}
      \text{E} [ \bm{H}^{-1} \; &\{ \bm{Z}^T(\bm{s}) \}_i \; \{ \bm{W}(\bm{s}) \}_{ii} \; \{ \bm{Z}(\bm{s}_i) \}_i^T \; \bm{\gamma}(\bm{s}_i) ] \notag \\
      &= \text{E} \; \left( \begin{array}{cc} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i)  & \bm{0}_{p \times 2p} \\ \bm{0}_{2p \times p} & \bm{0}_{2p \times 2p} \end{array} \right) \int K_h(\| \bm{s} - \bm{t} \|) \bm{\gamma}(\bm{t}) f(\bm{t}) \partial \bm{t} \notag \\
      &= \left( \begin{array}{cc} \bm{\Psi}  & \bm{0}_{p \times 2p} \\ \bm{0}_{2p \times p} & \bm{0}_{2p \times 2p} \end{array} \right) \bm{\gamma}(\bm{s}) f(\bm{s})
    \end{align}
    
    And the variance of (\ref{eq:m-term}) is:
    \begin{align}
      \text{E} [ \bm{H}^{-1} \; &\{ \bm{Z}^T(\bm{s}) \}_i \; \{ \bm{W}(\bm{s}) \}_{ii} \; \{ \bm{Z}(\bm{s}_i) \}_i^T \; \bm{\gamma}(\bm{s}_i) \bm{\gamma}^T(\bm{s}_i) \; \{ \bm{Z}^T(\bm{s}_i) \}_i \; \{ \bm{W}(\bm{s}) \}_{ii} \; \{ \bm{Z}(\bm{s}) \}^T_i \; \bm{H}^{-1} ] \notag \\
      &= \text{E} \; \left( \begin{array}{cc} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i)  & \bm{0}_{p \times 2p} \\ \bm{0}_{2p \times p} & \bm{0}_{2p \times 2p} \end{array} \right) \int K_h(\| \bm{s} - \bm{t} \|) \bm{\gamma}(\bm{t}) f(\bm{t}) \partial \bm{t} \notag \\
      &= \left( \begin{array}{cc} \bm{\Psi}  & \bm{0}_{p \times 2p} \\ \bm{0}_{2p \times p} & \bm{0}_{2p \times 2p} \end{array} \right) \bm{\gamma}(\bm{s}) f(\bm{s})
    \end{align}

  \paragraph{Second term}
    Find the expectation and variance of the $i$th term in the sum $n^{-1/2} \bm{H}^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \bm{\gamma}(\bm{s})$:

  \paragraph{Third term}
    Find the expectation and variance of the $i$th term in the sum $n^{-1/2} \bm{H}^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{\varepsilon}$:

    \begin{align}\label{eq:epsilon-term}
      \bm{H}^{-1} \; &\{ \bm{Z}^T(\bm{s}) \}_i \; \{ \bm{W}(\bm{s}) \}_{ii} \; \varepsilon(\bm{s}_i) \notag \\
      &= K_h(\| \bm{s} - \bm{s}_i \|) \; \bm{H}^{-1} \; \{ \bm{Z}^T(\bm{s}) \}_i \; \varepsilon(\bm{s}_i) \notag \\
      &= K_h(\| \bm{s} - \bm{s}_i \|) \left( \begin{array}{c} X_1(\bm{s}_i) \\ \vdots \\ X_p(\bm{s}_i) \\ X_1(\bm{s}_i) (\bm{s}_i - \bm{s})_1 \\ \vdots \\ X_p(\bm{s}_i) (\bm{s}_i - \bm{s})_1 \\ X_1(\bm{s}_i) (\bm{s}_i - \bm{s})_2 \\ \vdots \\X_p(\bm{s}_i) (\bm{s}_i - \bm{s})_2 \end{array} \right) \varepsilon(\bm{s}_i) \notag \\
      &= K_h(\| \bm{s} - \bm{s}_i \|) \left( \begin{array}{c} \bm{X}(\bm{s}_i) \\ \bm{X}(\bm{s}_i) (\bm{s}_i - \bm{s})_1 \\ \bm{X}(\bm{s}_i) (\bm{s}_i - \bm{s})_2 \end{array} \right) \varepsilon(\bm{s}_i)
    \end{align}

    So the expectation of (\ref{eq:epsilon-term}) is:
    \begin{align}
      \text{E} [ \bm{H}^{-1} \; &\{ \bm{Z}^T(\bm{s}) \}_i \; \{ \bm{W}(\bm{s}) \}_{ii} \; \varepsilon(\bm{s}_i) ] \notag \\
      &= \text{E} \; \left( \begin{array}{c} \bm{X}(\bm{s}_i) \\ \bm{X}(\bm{s}_i) (\bm{s}_i - \bm{s})_1 \\ \bm{X}(\bm{s}_i) (\bm{s}_i - \bm{s})_2 \end{array} \right) \text{E} \; \varepsilon(\bm{s}_i) \; \int K_h(\| \bm{s} - \bm{t} \|) f(\bm{t}) \partial \bm{t} \notag \\
      &= \left( \begin{array}{c} \bm{\mu}(\bm{s}_i) \\ \bm{\mu}(\bm{s}_i) (\bm{s}_i - \bm{s})_1 \\ \bm{\mu}(\bm{s}_i) (\bm{s}_i - \bm{s})_2 \end{array} \right) \times 0 \times f(\bm{s}) \notag \\
      &= 0
    \end{align}
    
    
    And the variance of (\ref{eq:epsilon-term}) is:
    \begin{align}
      \text{E} [ \bm{H}^{-1} \; &\{ \bm{Z}^T(\bm{s}) \}_i \; \{ \bm{W}(\bm{s}) \}_{ii} \; \varepsilon^2(\bm{s}_i) \; \{ \bm{W}(\bm{s}) \}_{ii} \; \{ \bm{Z}(\bm{s}) \}^T_i \;   \bm{H}^{-1} ] \notag \\
      &= \text{E} \; \left( \begin{array}{ccc} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) & h^{-1} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) (\bm{s}_i - \bm{s})_1 & h^{-1} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) (\bm{s}_i - \bm{s})_2 \\ h^{-1} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) (\bm{s}_i - \bm{s})_1 & h^{-2} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) (\bm{s}_i - \bm{s})^2_1 & h^{-2} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) (\bm{s}_i - \bm{s})_1 (\bm{s}_i - \bm{s})_2 \\ h^{-1} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) (\bm{s}_i - \bm{s})_2 & h^{-2} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) (\bm{s}_i - \bm{s})_1 (\bm{s}_i - \bm{s})_2 & h^{-2} \bm{X}(\bm{s}_i) \bm{X}^T(\bm{s}_i) (\bm{s}_i - \bm{s})^2_2 \end{array} \right)  \notag \\
      & \:\:\:\: \times E \; \varepsilon^2(\bm{s}_i) \; \int K^2_h(\| \bm{s} - \bm{t} \|) f(\bm{t}) \partial \bm{t} \notag \\
      &= \left( \begin{array}{ccc} \bm{\Psi} & h^{-1} \bm{\Psi} (\bm{s}_i - \bm{s})_1 & h^{-1} \bm{\Psi} (\bm{s}_i - \bm{s})_2 \\ h^{-1} \bm{\Psi} (\bm{s}_i - \bm{s})_1 & h^{-2}\bm{\Psi} (\bm{s}_i - \bm{s})^2_1 & h^{-2}\bm{\Psi} (\bm{s}_i - \bm{s})_1 (\bm{s}_i - \bm{s})_2 \\ h^{-1} \bm{\Psi} (\bm{s}_i - \bm{s})_2 & h^{-2} \bm{\Psi} (\bm{s}_i - \bm{s})_1 (\bm{s}_i - \bm{s})_2 & h^{-2} \bm{\Psi} (\bm{s}_i - \bm{s})^2_2 \end{array} \right) \times \sigma^2 \times h^{-2} f(\bm{s}) \nu_0 \notag \\
      &= 0
    \end{align}
  
  \paragraph{Third term.} By assumption, $p_0 \sqrt{n} a_n = O(\sqrt{n} a_n) = o_p(1)$.
  
  So the quadratic term dominates the sum, implying that the difference $Q \left\{ \bm{\beta}(\bm{s}_i) + n^{-1/2} \bm{u} \right\} > Q \left\{ \bm{\beta}(\bm{s}_i) \right\}$ is positive, which proves the result.
  
  \subsection{Selection}
    
    \begin{theorem}\label{theorem:selection}   
      If $\sqrt{n} a_n \xrightarrow{p} 0$ and $\sqrt{n} b_n \xrightarrow{p} \infty$ then $P \left\{ \hat{\bm{\beta}}_{(b)} (\bm{s}_i) = 0 \right\} \to 1$.
    \end{theorem}

    \begin{proof}
      The proof is by contradiction. Specifically, we show that if the statement of the theorem does not hold, then the MLE $\hat{\bm{\beta}}(\bm{s}_i)$ cannot be a maximum of the likelihood.
      
      Recall that the objective to be minimized by $\hat{\bm{\beta}}_{(p)} (\bm{s}_i)$ is
      \begin{align}\label{eq:objective}
        Q \left\{ \bm{\beta}(\bm{s}_i) \right\} = (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}_i) \bm{\beta}(\bm{s}_i) \right\}^T \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{Z}(\bm{s}_i) \bm{\beta}(\bm{s}_i) \right\} + n \sum_{j=1}^p \lambda_j \| \bm{\beta}_p(\bm{s}_i) \|
      \end{align}

      Let $\hat{\bm{\beta}}_p(\bm{s}_i) \ne 0$. Then (\ref{eq:objective}) is differentiable w.r.t. $\bm{\beta}_p(\bm{s}_i)$ and $Q$ is maximized at
      \begin{align}\label{eq:selection}
        0 &= \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{X}_{(-p)} \hat{\bm{\beta}}_{-p}(\bm{s}_i) - \bm{X}_{(p)} \hat{\bm{\beta}}_p(\bm{s}_i) \right\} + n \lambda_p \frac{ \hat{\bm{\beta}}_p(\bm{s}_i) }{\| \bm{\beta}_p(\bm{s}_i) \|} \notag \\
        &= \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{X} \bm{\beta}(\bm{s}_i) + \bm{X} \bm{\beta}(\bm{s}_i) -  \bm{X}_{(-p)} \hat{\bm{\beta}}_{-p}(\bm{s}_i) - \bm{X}_{(p)} \hat{\bm{\beta}}_p(\bm{s}_i) \right\} + n \lambda_p \frac{ \hat{\bm{\beta}}_p(\bm{s}_i) }{\| \bm{\beta}_p(\bm{s}_i) \|} \notag \\
        &= \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{X} \bm{\beta}(\bm{s}_i) \right\} + \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \left\{ \bm{X}_{(-p)} \bm{\beta}_{-p}(\bm{s}_i) - \bm{X}_{(-p)} \hat{\bm{\beta}}_{-p}(\bm{s}_i) \right\} \notag \\
        &\mkern+72mu+ \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \left\{ \bm{X}_{(-p)} \bm{\beta}_{-p}(\bm{s}_i) - \bm{X}_{(p)} \hat{\bm{\beta}}_p(\bm{s}_i) \right\} + n \lambda_p \frac{ \hat{\bm{\beta}}_p(\bm{s}_i) }{\| \bm{\beta}_p(\bm{s}_i) \|} \notag \\
        &= \sqrt{f(\bm{s}_i) h^2 n^{-1}} \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{X} \bm{\beta}(\bm{s}_i) \right\} + \sqrt{f(\bm{s}_i) h^2 n^{-1}} \bm{X}_{(p)}^T  \bm{W}(\bm{s}_i) \bm{X}_{(-p)} \left\{ \bm{\beta}_{-p}(\bm{s}_i) - \hat{\bm{\beta}}_{-p}(\bm{s}_i) \right\} \notag \\
        &\mkern+72mu+ \sqrt{f(\bm{s}_i) h^2 n^{-1}} \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \bm{X}_{(-p)} \left\{ \bm{\beta}_{-p}(\bm{s}_i) - \hat{\bm{\beta}}_p(\bm{s}_i) \right\} + \sqrt{f(\bm{s}_i) h^2 n} \lambda_p \frac{ \hat{\bm{\beta}}_p(\bm{s}_i) }{\| \bm{\beta}_p(\bm{s}_i) \|} \notag \\
        &= \sqrt{f(\bm{s}_i) h^2 n^{-1}} \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{X} \bm{\beta}(\bm{s}_i) \right\} +  n^{-1} \left\{ \bm{X}_{(p)}^T  \bm{W}(\bm{s}_i) \bm{X}_{(-p)} \right\} \sqrt{f(\bm{s}_i) h^2 n} \left\{ \bm{\beta}_{-p}(\bm{s}_i) - \hat{\bm{\beta}}_{-p}(\bm{s}_i) \right\} \notag \\
        &\mkern+72mu+ n^{-1} \left\{ \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \bm{X}_{(-p)} \right\} \sqrt{f(\bm{s}_i) h^2 n} \left\{ \bm{\beta}_{-p}(\bm{s}_i) - \hat{\bm{\beta}}_p(\bm{s}_i) \right\} + \sqrt{f(\bm{s}_i) h^2 n} \lambda_p \frac{ \hat{\bm{\beta}}_p(\bm{s}_i) }{\| \bm{\beta}_p(\bm{s}_i) \|}
      \end{align}
      
      From Lemma 2 of \cite{Sun-Yan-Zhang-Lu-2014}, $n^{-1} \left\{ \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \bm{X}_{(-p)} \right\} = O_p(1)$ and $n^{-1} \left\{ \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \bm{X}_{(p)} \right\} = O_p(1)$. From Theorem 3 of \cite{Sun-Yan-Zhang-Lu-2014}, we have that $\sqrt{f(\bm{s}_i) h^2 n} \left\{ \bm{\beta}_{(-p)} (\bm{s}_i) - \hat{\bm{\beta}}_{(-p)} (\bm{s}_i) \right\} = O_p(1)$ and $\sqrt{f(\bm{s}_i) h^2 n} \left\{ \bm{\beta}_{(p)}(\bm{s}_i) - \hat{\bm{\beta}}_{(p)}(\bm{s}_i) \right\} = O_p(1)$. So the second and third terms of the sum in (\ref{eq:selection}) are $O_p(1)$. We showed in the proof of \ref{theorem:consistency} that $\sqrt{f(\bm{s}_i) h^2 n^{-1}} \bm{X}_{(p)}^T \bm{W}(\bm{s}_i) \left\{ \bm{Y} - \bm{X} \bm{\beta}(\bm{s}_i) \right\} = O_p(1)$.

      Because the first three terms of the sum in \ref{eq:selection} are $O_p(1)$, for $\hat{\bm{\beta}}_{(p)} (\bm{s}_i)$ to be a solution, we must have that $\sqrt{f(\bm{s}_i) h^2 n} \lambda_p \frac{ \hat{\bm{\beta}}_{(p)} (\bm{s}_i) }{\| \bm{\beta}_{(p)} (\bm{s}_i) \|} = O_p(1)$.

      But since by assumption $\hat{\bm{\beta}}_{(p)} (\bm{s}_i) \ne 0$, there must be some $k \in \{ 1, \dots, d_p \}$ such that $ | \hat{\beta}_{(p),k} (\bm{s}_i) | = \max \{ | \hat{\beta}_{(p),k'} (\bm{s}_i) | : 1 \le k' \le d_p \} $. And for this $k$, we have that $| \hat{\beta}_{(p),k} (\bm{s}_i) | / \| \hat{\bm{\beta}}_{(p)} (\bm{s}_i) \| \ge 1 / \sqrt{d_p} > 0$.

      Now since $\sqrt{n} b_n \to \infty$, we have that $\sqrt{f(\bm{s}_i) h^2 n} \lambda_p \frac{ \hat{\bm{\beta}}_{(p)} (\bm{s}_i) }{\| \hat{\bm{\beta}}_{(p)} (\bm{s}_i) \|}$ is unbounded and therefore dominates the $O_p(1)$ terms of the sum in (\ref{eq:selection}). So for large enough $n$, $\hat{\bm{\beta}}_{(p)} (\bm{s}_i) \ne 0$ cannot maximize $Q$.
    \end{proof}

  \subsection{Oracle property}
    Here we show that the estimation accuracy is just as good as if the relevant predictor groups were specified in advance.

    \begin{theorem}
      If $\sqrt{n} a_n \to 0$ and $\sqrt{n} b_n \to \infty$, then $\sqrt{n h^2 f(\bm{s})} \left( \hat{\bm{\beta}}_{(a)} (\bm{s}_i) - \bm{\beta}_{(a)} (\bm{s}_i) - \frac{\kappa_2 h^2}{2 \kappa_0} \{ \bm{\beta}_{uu} (\bm{s}_i) + \bm{\beta}_{vv} (\bm{s}_i) \} \right) \xrightarrow{d} N(0,\Sigma_{(a)} (\bm{s}_i))$.
    \end{theorem}

    \begin{proof}
      The proof proceeds by showing that if the tuning parameter $\lambda$ is chosen correctly, then the penalty term vanishes for the relevant predictor groups and becomes infinite for the irrelevant predictor groups.

    \end{proof}

    Since $ \| \tilde{\bm{\beta}} (\bm{s}_i) \|^{\gamma} = O_p\left\{ ( n h^2 )^{-\gamma / 2} \right\} $ and $h = O(n^{-1/6})$, in order for $\sqrt{n} a_n \to 0$ and $\sqrt{n} b_n \to \infty$, we require that $\lambda = O(n^{\alpha})$ where $\alpha \in \left( - \left\{ 1 + \gamma - \frac{\gamma}{6} \right\} /2, -1/2 \right) $.

\section{References}
\bibliographystyle{chicago}
\bibliography{../../references/gwr}

\end{document}  