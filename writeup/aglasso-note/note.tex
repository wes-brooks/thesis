\documentclass[authoryear, review, 11pt]{elsarticle}

\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{rotating}
\usepackage[nolists,nomarkers]{endfloat}
\DeclareDelayedFloatFlavour{sidewaystable}{table}

\usepackage{relsize}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{booktabs}


\usepackage{setspace}
\setstretch{2}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}



\title{Oracle properties of local adaptive grouped regularization}
\author{Wesley Brooks}
\date{}                                           % Activate to display a given date or no date


\begin{document}

%\begin{abstract}
%Researchers who analyze spatial data often wish to discern how a certain response variable is related to a set of covariates. When it is believed that the effect of a given covariate may be different at different locations, a spatially varying coefficient regression model, in which the effects of the covariates are allowed to vary across the spatial domain, may be appropriate. In this case, it may be the case that the covariate has a meaningful association with the response in some parts of the spatial domain but not in others. Identifying the covariates that are associated with the response at a given location is called local model selection. Geographically weighted regression, a kernel-based method for estimating the local regression coefficients in a spatially varying coefficient regression model, is considered here. A new method is introduced for local model selection and coefficient estimation in spatially varying coefficient regression models. The idea is to apply a penalty of the elastic net type to a local likelihood function, with a local elastic net tuning parameter and a global bandwidth parameter selected via information criteria. Simulations are used to evaluate the performance of the new method in model selection and coefficient estimation, and the method is applied to a real data example in spatial demography.
%\end{abstract}

\maketitle
%\section{}
%\subsection{}

%Pastebin:


\section{Spatially varying coefficients regression \label{section:SVCR}}
	\subsection{Model}	
	Consider $n$ data points, observed at sampling locations $\bm{s}_i = (s_{i,1} \;\; s_{i,2})^T$ for $i = 1, \dots, \bm{s}_n$, which are distributed in a spatial domain $D \subset \mathbb{R}^2$ according to a density $f(\bm{s})$. For $i = 1, \dots, n$, let $y(\bm{s}_i)$ and $\bm{x}(\bm{s}_i)$ denote, respectively, the univariate response and the $(p+1)$-variate vector of covariates measured at location $\bm{s}_i$. At each location $\bm{s}_i$, assume that the outcome is related to the covariates by a linear model where the coefficients $\bm{\beta}(\bm{s}_i)$ may be spatially-varying and $\varepsilon(\bm{s}_i)$ is random error at location $\bm{s}_i$. That is,
	\begin{align}\label{eq:lm(s)}
		y(\bm{s}_i) = \bm{x}(\bm{s}_i)' \bm{\beta}(\bm{s}_i) + \varepsilon(\bm{s}_i).
	\end{align}
	
	Further assume that the error term $\varepsilon(\bm{s}_i)$ is normally distributed with zero mean and variance $\sigma^2$, and that $\varepsilon(\bm{s}_i)$, $i=1, \dots, n$ are independent. That is,
	\begin{align} \label{eq:err}
		\bm{\varepsilon} \overset{iid}{\sim} \mathcal{N} \left( 0,\sigma^2 \right).
	\end{align}

    In the context of nonparametric regression, the boundary-effect bias can be reduced by local polynomial modeling, usually in the form of a locally linear model \citep{Fan-1996}. Here, locally linear coefficients are estimated by augmenting the local design matrix with covariate-by-location interactions in two dimensions as proposed by \cite{Wang:2008b}. The augmented local design matrix at location $\bm{s}_i$ is
    \begin{align}
		\bm{Z}(\bm{s}_i) = \left( \bm{X}  \:\: L_i \bm{X} \:\: M_i \bm{X} \right)
	\end{align} 
  
	where $\bm{X}$ is the unaugmented matrix of covariates, $L_i = \text{diag}\{s_{i'_1} - s_{i_1}\}$ and $M_i = \text{diag}\{s_{i'_2} - s_{i_2}\}$ for $i' = 1, \dots, n$.

    Now we have that $Y(\bm{s}_i) = \left\{ \bm{Z}(\bm{s}_i) \right\}^T_i \bm{\zeta}(\bm{s}_i) + \varepsilon(\bm{s}_i)$, where $\left\{ \bm{Z}(\bm{s}_i) \right\}^T_i$ is the $i$th row of the matrix $\bm{Z}(\bm{s}_i)$ as a row vector, and $\bm{\zeta}(\bm{s}_i)$ is the vector of local coefficients at location $\bm{s}_i$, augmented with the local gradients of the coefficient surfaces in the two spatial dimensions, indicated by $\nabla_u$ and $\nabla_v$:

    \begin{equation*}
        \bm{\zeta}(\bm{s}_i) = \left( \bm{\beta}(\bm{s}_i)^T \;\; \nabla_u \bm{\beta}(\bm{s}_i)^T \;\; \nabla_v \bm{\beta}(\bm{s}_i)^T \right)^T
    \end{equation*}
  
    \subsection{Estimation}		
    The total log-likelihood of the observed data is the sum of the log-likelihood of each individual observation:
    \begin{align} \label{eq:coefficients}
        \ell \left\{ \bm{\zeta} \right\} = -(1/2) \sum_{i=1}^n \left[ \log{ \sigma^2}  + \sigma^{-2}  \left\{ y(\bm{s}_i) - \bm{z}'(\bm{s}_i) \bm{\zeta}(\bm{s}_i) \right\}^2 \right].
	\end{align}
	
	Since there are a total of $n \times 3(p+1) + 1$ parameters for $n$ observations, the model is not identifiable and it is not possible to directly maximize the total likelihood. But since the coefficient functions are smooth, the coefficients at location $\bm{s}$ can approximate the coefficients within some neighborhood of $\bm{s}$, with the quality of the approximation declining as the distance from $\bm{s}$ increases.

    This intuition is formalized by the local likelihood, which is maximized at location $\bm{s}$ to estimate the local coefficients $\bm{\zeta}(\bm{s})$:
    \begin{align}\label{eq:local-likelihood}
		\mathcal{L} \left\{ \bm{\zeta}(\bm{s}) \right\} &= \prod_{i=1}^n \left\{ \left(2 \pi \sigma^2  \right)^{-1/2}  \exp \left[ -(1/2) \sigma^{-2}  \left\{ y(\bm{s}_i) - \bm{z}'(\bm{s}_i) \bm{\zeta}(\bm{s}) \right\}^2 \right] \right\} ^ {K_h( \| \bm{s} - \bm{s}_i \| )},
	\end{align}
 
    The weights are computed from a kernel function $K_h(\cdot)$ such as the Epanechnikov kernel:
    \begin{align}\label{eq:epanechnikov}
        K_h(\| \bm{s}_i - \bm{s}_{i'}\|) &= h^{-2} K\left( h^{-1} \| \bm{s}_i - \bm{s}_{i'}\| \right) \notag \\
        K(x) &= \begin{cases} (3/4) (1-x^2) &\mbox{ if } x < 1, \\ 0 &\mbox{ if } x \geq 1. \end{cases}
	\end{align}
  
    Thus, the local log-likelihood function is, up to an additive constant: 
    \begin{align}\label{eq:local-log-likelihood}
		\ell \left\{ \bm{\zeta}(\bm{s}) \right\} &= -(1/2) \sum_{i=1}^n K_h( \| \bm{s} - \bm{s}_i \| ) \left[ \log{\sigma^2}  + \sigma^{-2}  \left\{ y(\bm{s}_i) - \bm{z}'(\bm{s}_i) \bm{\zeta}(\bm{s}) \right\}^2 \right].
    \end{align}
  
    Letting $\bm{W}(\bm{s})$ be a diagonal weight matrix where $W_{ii}(\bm{s}) = K_h( \| \bm{s} - \bm{s}_i \| )$, the local likelihood is maximized by weighted least squares:
    \begin{align}\label{eq:zeta-hat}
        \mathcal{S} \left\{ \bm{\zeta} (\bm{s}) \right\} &= (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T \notag \\
        \therefore \tilde{\bm{\zeta}}(\bm{s}) &= \left\{ \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\}^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Y}
    \end{align}

%    Now define the penalized weighted squared error loss $Q$:

%    \begin{equation*}
%        Q \left\{ \bm{\zeta} (\bm{s}) \right\} = (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T + \sum_{j=1}^p \phi_j(\bm{s}) \| \bm{\zeta}(\bm{s}) \|.
%    \end{equation*}
    
%    From (\ref{eq:local-log-likelihood}), the maximum local likelihood estimate $\hat{\sigma}_i^2$ is:	 
%    \begin{align}
%        \hat{\sigma}^2(\bm{s}) = \left\{ \sum \limits_{i=1}^{n} K_h( \| \bm{s} - \bm{s}_i \| ) \right\}^{-1} \sum \limits_{i=1}^n K_h( \| \bm{s} - \bm{s}_i \| ) \left\{ y(\bm{s}_i) - \bm{z}'(\bm{s}_i) \hat{\bm{\zeta}}(\bm{s}) \right\}^2
%    \end{align}
	 

\section{Local variable selection and parameter estimation 
Estimating the local coefficients by (\ref{eq:zeta-hat}) relies on \emph{a priori} variable selection. The goal of local adaptive grouped regularization (LAGR) is to simultaneously select the locally relevant predictors and estimate the local coefficients.

\label{section:model-selection}}
	\subsection{Local variable selection}
	The proposed LAGR penalty is an adaptive $\ell_1$ penalty akin to the adaptive group lasso \citep{Wang-Leng-2008,Zou:2006}. Grouped variables are selected together for inclusion in the model. Each group in a LAGR model consists of one covariate and its gradients on the two dimensions of spatial location. That is, $\bm{\zeta}_j(\bm{s}) = \left( \beta_j(\bm{s}) \;\;\; \nabla_u \beta_j(\bm{s}) \;\;\; \nabla_v \beta_j(\bm{s}) \right)^T$ for $j=1, \dots, p$.
	
	The objective function for the LAGR at location $\bm{s}$ is the penalized local sum of squares:
	\begin{align}\label{eq:adaptive-lasso-WLS}
		Q \{ \bm{\zeta}(\bm{s}) \} &= \mathcal{S} \left\{ \bm{\zeta}( \bm{s} ) \right\} + \mathcal{J}\{ \bm{\zeta}(\bm{s}) \} \notag \\
		&= (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta} (\bm{s}) \right\}^T + \sum_{j=1}^p \phi_j(\bm{s}) \| \bm{\zeta}_j(\bm{s}) \| 
	\end{align}
	
	which is the sum of the weighted sum of squares $\mathcal{S} \left\{ \bm{\zeta}( \bm{s} ) \right\}$ and the LAGR penalty $\mathcal{J}\{ \bm{\zeta}(\bm{s}) \}$.

    The LAGR penalty for the $j$th group of coefficients $\bm{\zeta}_j(\bm{s})$ at location $\bm{s}$ is $\phi_j(\bm{s}) = \lambda_n (\bm{s}) \| \tilde{\bm{\zeta}}_j(\bm{s}) \|^{-\gamma}$, where $\lambda_n (\bm{s}) > 0$ is a the local tuning parameter applied to all coefficients at location $\bm{s}$ and $\tilde{\bm{\zeta}}_j (\bm{s})$ is the vector of unpenalized local coefficients from (\ref{eq:zeta-hat}).


    \subsection{Computation}
        \subsubsection{Tuning parameter selection}
        Implementing LAGR requires the selection of local tuning parameters. The criteria commonly used for selecting tuning parameters in lasso-type models are appropriate here, including GCV \citep{Wahba:1990}, Cp \citep{Mallows-1973}, AIC \citep{Akaike-1973}, and BIC \citep{Schwarz-1978}. All of the examples and simulations presented herein used the corrected AIC (AICc) \citep{Hurvich-1998} for tuning parameter selection:

        \begin{equation}
            \text{AIC}_{\text{c}}(\bm{s}) = \hat{\sigma}^{-2}(\bm{s}) \|\bm{Y} - \bm{Z}(\bm{s}) \hat{\bm{\zeta}}(\bm{s}) \|^2 + 2df + 2\frac{df (df+1)}{\sum_{i=1}^n K_h(\| \bm{s} - \bm{s}_i \|)-df-1}
        \end{equation}

        where $df$ is the degrees of freedom as defined in \cite{Yuan-Lin-2006}:

        \begin{equation}
            df = \sum_{j=1}^p I \left\{ \| \hat{\bm{\zeta}}_j(\bm{s}) \| \right\} + 2 \sum_{j=1}^p \frac{\| \hat{\bm{\zeta}}_j(\bm{s}) \|}{\| \tilde{\bm{\zeta}}_j(\bm{s}) \|}
        \end{equation}

    \section{Asymptotic properties}

        \subsection{Notation and assumptions}
        Consider the local model at location $\bm{s}$ where there are $p_0 < p$ covariates $\bm{X}_{(a)}(\bm{s})$ with nonzero local regression coefficients, indicated by $\bm{\beta}_{(a)}(\bm{s})$. The remaining covariates $\bm{X}_{(b)}(\bm{s})$ have true coefficients equal to zero, indicated by $\bm{\beta}_{(b)}(\bm{s})$. Without loss of generality, assume these are covariates $1, \dots, p_0$.

        Let $\Psi = E \left\{ \bm{X}(\bm{s}_1) \bm{X}^T(\bm{s}_1) \right\}$.

        Let $h = O(n^{-1/6})$.
        
        Let $a_n = \max \{ \phi_j(\bm{s}), j \le p_0 \}$ be the largest penalty applied to a covariate group whose true coefficient norm is nonzero, and $b_n = \min \{ \phi_j(\bm{s}), j > p_0 \}$ be the smallest penalty applied to a covariate group whose true coefficient norm is zero.
        
        Let $\bm{Z}_k(\bm{s})$ be the design matrix for covariate group $k$, and $\bm{Z}_{-k}(\bm{s})$ be the design matrix for all the data except covariate group $k$, respectively. Similarly, let $\bm{\zeta}_k(\bm{s})$ be the coefficients for covariate group $k$ and $\bm{\zeta}_{-k}(\bm{s})$ be the coefficients for all covariate groups except $k$.

        Finally, let $\kappa_0 = \int_{R^2} K(\|\bm{s}\|) ds$, $\kappa_2 = \int_{R^2} [(1,0)\bm{s}]^2 K(\|\bm{s}\|) ds = \int_{R^2} [(0,1)\bm{s}]^2 K(\|\bm{s}\|) ds$, and $\nu_0 = \int_{R^2} K^2(\|\bm{s}\|) ds$.

        \subsection{Results}
        \paragraph{Asymptotic normality}
        \begin{theorem} \label{theorem:normality} \setstretch{2}
            If $h^{-1} n^{-1/2} a_n \xrightarrow{p} 0$ and $h n^{-1/2} b_n \xrightarrow{p} \infty$ then \[h \sqrt{n} \left[ \hat{\bm{\beta}}_{(a)}(\bm{s}) - \bm{\beta}_{(a)}(\bm{s}) - \frac{\kappa_2 h^2}{2 \kappa_0} \{ \nabla_{uu}^2 \bm{\beta}_{(a)} (\bm{s}) + \nabla_{vv}^2 \bm{\beta}_{(a)} (\bm{s}) \} \right] \xrightarrow{d} N(0, f(\bm{s})^{-1} \kappa_0^{-2} \nu_0 \sigma^2 \Psi^{-1} )\]
        \end{theorem}

        \paragraph{Selection}
        \begin{theorem} \label{theorem:selection} \setstretch{2} 
            If $h^{-1} n^{-1/2} a_n \xrightarrow{p} \infty$ and $h n^{-1/2} b_n \xrightarrow{p} \infty$ then $P \left\{ \| \hat{\bm{\zeta}}_j (\bm{s}) \| = 0 \right\} \to 0$ if $j \le p_0$ and $P \left\{ \| \hat{\bm{\zeta}}_j (\bm{s}) \| = 0 \right\} \to 1$ if $j > p_0$.
        \end{theorem}

        \paragraph{Remarks}
        Together, TheoremÂ \ref{theorem:normality} and Theorem \ref{theorem:selection} indicate that the LAGR estimates have the same asymptotic distribution as a local regression model where the nonzero coefficients are known in advance \citep{Sun-Yan-Zhang-Lu-2014}, and that the LAGR estimates of true zero coefficients go to zero with probability one. Thus, selection and estimation by LAGR has the oracle property.

        \paragraph{A note on rates}
        To prove the oracle properties of LAGR, we assumed that $h^{-1} n^{-1/2} a_n \xrightarrow{p} 0$ and $h n^{-1/2} b_n \xrightarrow{p} \infty$. Therefore, $h^{-1} n^{-1/2} \lambda_n(\bm{s})  \to 0$ for $j \le p_0$ and $h n^{-1/2} \lambda_n(\bm{s}) \| \bm{\zeta}_j(\bm{s}) \|^{-\gamma} \to \infty$ for $j > p_0$.
        
        We require that $\lambda_n(\bm{s})$ can satisfy both assumptions. Suppose $\lambda_n(\bm{s}) = n^{\alpha}$, and recall that $h = O(n^{-1/6})$ and $ \| \tilde{\bm{\zeta}}_p(\bm{s}) \| = O(h^{-1} n^{-1/2})$. Then $h^{-1} n^{-1/2} \lambda_n(\bm{s}) = O(n^{-1/3 + \alpha})$ and $h n^{-1/2} \lambda_n(\bm{s})  \| \tilde{\bm{\zeta}}_p(\bm{s} \|^{-\gamma} = O(n^{-2/3 + \alpha + \gamma/3})$.
        
        So $ (2 - \gamma)/3 < \alpha < 1/3 $, which can only be satisfied for $\gamma > 1$.

    \section{Simulations}
        A simulation study was undertaken to assess the performance of LAGR for local variable selection and coefficient estimation. 


    \section{Data example}
        Here we present an application of LAGR to a real data set. The data is the Boston house price data from \cite{Harrison-Rubinfeld-1978; Gilley-Pace-1996; Pace-Gilley-1997}. This is areal data, measured on census tracts in Boston, Massachusetts for the 1978 Harrison and Rubinfeld paper. The response variable, MEDV, is the median selling price of homes in the census block (capped at USD 50,000). Predictors whose local coefficients were estimated via LAGR are CRIM, representing the per capita crime rate; RM, the average number of rooms for houses sold within the census tract; RAD, which measures the accessibility of radial roads from the tract; TAX, the full-value property tax rate per USD 10,000 (constant for all Boston tracts); and LSTAT, the percentage of the population in a tract considered ``lower status". The same data was analyzed in \cite{Sun-Yan-Zhang-Lu-2014}.

        An adaptive bandwidth was used, with the bandwidth at each location set such that the sum of the kernel weights for the local model was 17\% of the number of observations. 

        


\appendix
    \section{Proofs of theorems} \label{app:proofs}
        \begin{proof}[Proof of theorem \ref{theorem:normality}] \setstretch{2}
            Define $V_4^{(n)}(\bm{u})$ to be the 
            \begin{align}\label{eq:consistency}
                \mkern-36mu V_4^{(n)}(\bm{u}) &= Q \left\{ \bm{\zeta} (\bm{s}) + h^{-1} n^{-1/2} \bm{u} \right\} - Q \left\{ \bm{\zeta}(\bm{s}) \right\} \notag \\
                &\mkern-36mu= (1/2) \left[ \bm{Y} - \bm{Z}(\bm{s}) \left\{ \bm{\zeta}(\bm{s}) + h^{-1} n^{-1/2} \bm{u} \right\} \right]^T \bm{W}(\bm{s}) \left[ \bm{Y} - \bm{Z}(\bm{s}) \left\{ \bm{\zeta}(\bm{s}) + h^{-1} n^{-1/2} \bm{u} \right\} \right] \notag \\
                &+ \sum_{j=1}^p \phi_j(\bm{s}) \| \bm{\zeta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| \notag \\
                &- (1/2) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) \right\}^T \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) \right\} - \sum_{j=1}^p \phi_j(\bm{s}) \| \bm{\zeta}_j(\bm{s}) \| \notag \\ 
                &\mkern-36mu= (1/2) \bm{u}^T \left\{ h^{-2} n^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\} \bm{u} - \bm{u}^T \left[ h^{-1} n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) \right\} \right] \notag \\
                &+ \sum_{j=1}^p n^{-1/2} \phi_j(\bm{s}) n^{1/2} \left\{ \|\bm{\zeta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| - \| \bm{\zeta}_j(\bm{s}) \| \right\}
            \end{align}
        
            Note the different limiting behavior of the third term between the cases $j \le p_0$ and $j > p_0$:
        
            \paragraph{Case $j \le p_0$}
            If $j \le p_0$ then $n^{-1/2} \phi_j(\bm{s}) \to n^{-1/2} \lambda_n(\bm{s}) \| \bm{\zeta}_j(\bm{s}) \|^{-\gamma}$ and $| \sqrt{n} \left\{ \|\bm{\zeta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| - \| \bm{\zeta}_j(\bm{s}) \| \right\} | \le h^{-1} \| \bm{u}_j \|$ so \[\lim \limits_{n \to \infty} \phi_j(\bm{s}) \left( \|\bm{\zeta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| - \| \bm{\zeta}_j(\bm{s}) \| \right) \le h^{-1} n^{-1/2} \phi_j(\bm{s})  \| \bm{u}_j \| \le h^{-1} n^{-1/2} a_n \| \bm{u}_j \| \to 0\]

            \paragraph{Case $j > p_0$}
            If $j > p_0$ then $\phi_j(\bm{s}) \left( \|\bm{\zeta}_j(\bm{s}) + h^{-1} n^{-1/2} \bm{u}_j \| - \| \bm{\zeta}_j(\bm{s}) \| \right) = \phi_j(\bm{s}) h^{-1} n^{-1/2} \| \bm{u}_j \| $.
        
            And note that $h = O(n^{-1/6})$ so that if $h n^{-1/2} b_n \xrightarrow{p} \infty$ then $h^{-1} n^{-1/2} b_n \xrightarrow{p} \infty$.
        
            Now, if $\| \bm{u}_j \| \ne 0$ then \[h^{-1} n^{-1/2} \phi_j(\bm{s}) \| \bm{u}_j \| \ge h^{-1} n^{-1/2} b_n \| \bm{u}_j \| \to \infty\]. On the other hand, if $\| \bm{u}_j \| = 0$ then $h^{-1} n^{-1/2} \phi_j(\bm{s}) \| \bm{u}_j \| = 0$.

            Thus, the limit of $V_4^{(n)} (\bm{u})$ is the same as the limit of $V_4^{*(n)} (\bm{u})$ where

            \begin{equation*}
                \mkern-72muV_4^{*(n)} (\bm{u}) = \begin{cases}(1/2) \bm{u}^T \left\{ h^{-2} n^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\} \bm{u} - \bm{u}^T \left[ h^{-1} n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) \right\} \right] &\mbox{ if } \| \bm{u}_j \| = 0 \; \forall j > p_0 \\ \infty &\mbox{ otherwise } \end{cases}.
            \end{equation*}


            From which it is clear that  $V_4^{*(n)}(\bm{u})$ is convex and its unique minimizer is $\hat{\bm{u}}^{(n)}$:

            \begin{align} \label{eq:limit}
                0 &=  \left\{ h^{-2} n^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\} \hat{\bm{u}}^{(n)} - \left[ h^{-1} n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) \right\} \right] \notag \\
                \therefore \hat{\bm{u}}^{(n)} &= \left\{ n^{-1} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}(\bm{s}) \right\}^{-1} \left[ h n^{-1/2} \bm{Z}^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) \right\} \right] \notag \\
            \end{align}

            By the epiconvergence results of \cite{Geyer-1994} and \cite{Knight-Fu-2000}, the minimizer of the limiting function is the limit of the minimizers $\hat{\bm{u}}^{(n)}$. And since, by Lemma 2 of \cite{Sun-Yan-Zhang-Lu-2014}, 

            \begin{equation}
                \hat{\bm{u}}^{(n)} \xrightarrow{d} N \left(\frac{\kappa_2 h^2}{2 \kappa_0} \{ \nabla_{uu}^2 \bm{\zeta}_j (\bm{s}) + \nabla_{vv}^2 \bm{\zeta}_j (\bm{s}) \}, f(\bm{s}) \kappa_0^{-2} \nu_0 \sigma^2 \Psi^{-1} \right)
            \end{equation}
            the result is proven.
        \end{proof}

        \begin{proof}[Proof of theorem \ref{theorem:selection}] \setstretch{2}
            We showed in Theorem \ref{theorem:normality} that $\hat{\bm{\zeta}}_j (\bm{s}) \xrightarrow{p} \bm{\zeta}_j (\bm{s}) + \frac{\kappa_2 h^2}{2 \kappa_0} \{ \nabla_{uu}^2 \bm{\zeta}_j (\bm{s}) + \nabla_{vv}^2 \bm{\zeta}_j (\bm{s}) \}$, so to complete the proof of selection consistency, it only remains to show that $P \left\{ \hat{\bm{\zeta}}_j (\bm{s}) = 0 \right\} \to 1$ if $j > p_0$.
            
            The proof is by contradiction. Without loss of generality we consider only the case $j=p$.

            Assume $\| \hat{\bm{\zeta}}_p(\bm{s}) \| \ne 0$. Then $Q \left\{ \bm{\zeta}(\bm{s}) \right\}$ is differentiable w.r.t. $\bm{\zeta}_p(\bm{s})$ and is minimized where
            \begin{align}
                0 &= \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \left\{ \bm{Y} - \bm{Z}_{-p}(\bm{s}) \hat{\bm{\zeta}}_{-p}(\bm{s}) - \bm{Z}_p(\bm{s}) \hat{\bm{\zeta}}_p(\bm{s}) \right\} - \phi_p(\bm{s}) \frac{ \hat{\bm{\zeta}}_p(\bm{s}) }{\| \hat{\bm{\zeta}}_p(\bm{s}) \|} \notag \\
                &= \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \left[ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \bm{\zeta}(\bm{s}) + \nabla^2_{vv} \bm{\zeta}(\bm{s}) \right\} \right] \notag \\
                &\mkern+72mu+ \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{-p}(\bm{s}) \left[ \bm{\zeta}_{-p}(\bm{s}) + \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \bm{\zeta}_{-p}(\bm{s}) + \nabla^2_{vv} \bm{\zeta}_{-p}(\bm{s}) \right\} - \hat{\bm{\zeta}}_{-p}(\bm{s}) \right] \notag \\
                &\mkern+72mu+ \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_p(\bm{s}) \left[ \bm{\zeta}_p(\bm{s}) + \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \bm{\zeta}_p(\bm{s}) + \nabla^2_{vv} \bm{\zeta}_p(\bm{s}) \right\} - \hat{\bm{\zeta}}_p(\bm{s}) \right]  \notag \\
                &\mkern+72mu- \phi_p(\bm{s}) \frac{ \hat{\bm{\zeta}}_p(\bm{s}) }{\| \hat{\bm{\zeta}}_p(\bm{s}) \|} \notag \\
            \end{align}
        
            So
            \begin{align}\label{eq:selection}
                \frac{h}{\sqrt{n}} \phi_p(\bm{s}) \frac{ \hat{\bm{\zeta}}_p(\bm{s}) }{\| \hat{\bm{\zeta}}_p(\bm{s}) \|} &= \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \frac{h}{\sqrt{n}} \left[ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \bm{\zeta}(\bm{s}) + \nabla^2_{vv} \bm{\zeta}(\bm{s}) \right\} \right] \notag \\ 
                &+ \left\{ n^{-1}  \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{-p}(\bm{s}) \right\} h \sqrt{n} \left[ \bm{\zeta}_{-p}(\bm{s}) + \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \bm{\zeta}_{-p}(\bm{s}) + \nabla^2_{vv} \bm{\zeta}_{-p}(\bm{s}) \right\} - \hat{\bm{\zeta}}_{-p}(\bm{s}) \right] \notag \\
                &+ \left\{ n^{-1} \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_p(\bm{s}) \right\} h \sqrt{n} \left[ \bm{\zeta}_p(\bm{s}) + \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \bm{\zeta}_p(\bm{s}) + \nabla^2_{vv} \bm{\zeta}_p(\bm{s}) \right\} - \hat{\bm{\zeta}}_p(\bm{s}) \right]
            \end{align}

            From Lemma 2 of \cite{Sun-Yan-Zhang-Lu-2014}, $\left\{ n^{-1} \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_{-p}(\bm{s}) \right\} = O_p(1)$ and $\left\{ n^{-1} \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \bm{Z}_p(\bm{s}) \right\} = O_p(1)$.
        
            From Theorem 3 of \cite{Sun-Yan-Zhang-Lu-2014}, we have that $h \sqrt{n} \left[ \hat{\bm{\zeta}}_{-p} (\bm{s}) - \bm{\zeta}_{-p}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \zeta_{-p}(\bm{s}) + \nabla^2_{vv} \zeta_{-p}(\bm{s}) \right\}\right] = O_p(1)$ and $h \sqrt{n} \left[ \hat{\bm{\zeta}}_p(\bm{s}) - \bm{\zeta}_p(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \zeta_p(\bm{s}) + \nabla^2_{vv} \zeta_p(\bm{s}) \right\} \right] = O_p(1)$.
        
            So the second and third terms of the sum in (\ref{eq:selection}) are $O_p(1)$.
        
            We showed in the proof of \ref{theorem:normality} that $h  \sqrt{n} \bm{Z}_p^T(\bm{s}) \bm{W}(\bm{s}) \left[ \bm{Y} - \bm{Z}(\bm{s}) \bm{\zeta}(\bm{s}) - \frac{h^2 \kappa_2}{2 \kappa_0} \left\{ \nabla^2_{uu} \bm{\zeta}(\bm{s}) + \nabla^2_{vv} \bm{\zeta}(\bm{s}) \right\} \right]= O_p(1)$.

            The three terms of the sum to the right of the equals sign in (\ref{eq:selection}) are $O_p(1)$, so for $\hat{\bm{\zeta}}_p (\bm{s})$ to be a solution, we must have that $h n^{-1/2} \phi_p(\bm{s}) \hat{\bm{\zeta}}_p (\bm{s}) / \| \hat{\bm{\zeta}}_p (\bm{s}) \| = O_p(1)$.

            But since by assumption $\hat{\bm{\zeta}}_p (\bm{s}) \ne 0$, there must be some $k \in \{ 1, \dots, 3 \}$ such that $ | \hat{\zeta}_{p_k} (\bm{s}) | = \max \{ | \hat{\zeta}_{p_{k'}} (\bm{s}) | : 1 \le k' \le 3 \} $. And for this $k$, we have that $| \hat{\zeta}_{p_k} (\bm{s}) | / \| \hat{\bm{\zeta}}_p (\bm{s}) \| \ge 1 / \sqrt{3} > 0$.

            Now since $h n^{-1/2} b_n \to \infty$, we have that $h n^{-1/2} \phi_p(\bm{s}) \hat{\bm{\zeta}}_p (\bm{s}) / \| \hat{\bm{\zeta}}_p (\bm{s}) \| \ge h b_n / \sqrt{3n} \to \infty$ and therefore the term to the left of the equals sign dominates the sum to the right of the equals sign in (\ref{eq:selection}). So for large enough $n$, $\hat{\bm{\zeta}}_p (\bm{s}) \ne 0$ cannot maximize $Q$.
        
            So $P \left\{ \hat{\bm{\zeta}}_{(b)} (\bm{s}) = 0 \right\} \to 1$.
        \end{proof}


\section{References}
\bibliographystyle{chicago}
\bibliography{../../references/gwr}

\end{document}  