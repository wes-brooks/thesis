#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass elsarticle
\begin_preamble


\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\usepackage{amsthm}
\usepackage{mathabx}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{rotating}
\usepackage[nolists,nomarkers]{endfloat}
\DeclareDelayedFloatFlavour{sidewaystable}{table}

\usepackage{relsize}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{booktabs}





\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\bw}{\mbox{bw}}
\DeclareMathOperator*{\df}{\mbox{df}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\E}{\mathop{\mathbb E}}



\newtheorem{theorem}{}[section]
\newtheorem{lemma}[theorem]{}\newtheorem{proposition}[theorem]{}\newtheorem{corollary}[theorem]{}



\title{Oracle properties of local adaptive grouped regularization}
\author{Wesley Brooks}
                                           % Activate to display a given date or no date
\end_preamble
\options authoryear,review
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing other 2
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Local adaptive grouped regularization and its oracle properties
\end_layout

\begin_layout Author
Wesley Brooks
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%Varying coefficient regression
\end_layout

\end_inset

 Whereas the coefficients in traditional linear regression are scalar constants,
 the coefficients in a varying coefficient regression (VCR) model are functions
 - often 
\emph on
smooth
\emph default
 functions - of some effect modifying variable 
\begin_inset CommandInset citation
LatexCommand citep
key "Hastie:1993a"

\end_inset

.
 When the effect modifying variable represents location in a spatial domain,
 a VCR model implies a spatially varying coefficient regression (SVCR) model
 wherein that the regression coefficients vary over space.
 Statistical inference for the coefficients as functions of location in
 an SVCR model is more complicated than estimating the coefficients in a
 traditional linear regression model where the coefficients are constant
 across the spatial domain.
 My research concerns the development of new methodology for the analysis
 of spatial data using SVCR.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

%Spatial data / spatial regression
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The methodology described herein is applicable to geostatistical data and
 areal data.
 Let 
\begin_inset Formula $\mathcal{D}$
\end_inset

 be a spatial domain on which data is collected.
 For geostatistical data, let 
\begin_inset Formula $\bm{s}$
\end_inset

 denote a location in 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
 Let a univariate spatial process 
\begin_inset Formula $\left\{ Y(\bm{s}):\bm{s}\in\mathcal{D}\right\} $
\end_inset

 and a possibly multivariate spatial process 
\begin_inset Formula $\left\{ \bm{X}(\bm{s}):\bm{s}\in\mathcal{D}\right\} $
\end_inset

 denote random fields of the response and the covariates, respectively.
 For 
\begin_inset Formula $i=1,\dots,n$
\end_inset

, let 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

 denote the sampling location in 
\begin_inset Formula $\mathcal{D}$
\end_inset

 of the 
\begin_inset Formula $i$
\end_inset

th observation of the response and the covariates.
 Let the observed data be denoted 
\begin_inset Formula $\left\{ y(\bm{s}_{i}),\bm{x}(\bm{s}_{i})\right\} $
\end_inset

, 
\begin_inset Formula $i=1,\dots,n$
\end_inset

.
 Then the data are a realization of the random fields at the sampling locations
 
\begin_inset Formula $\left\{ Y(\bm{s}_{i}),\bm{X}(\bm{s}_{i})\right\} $
\end_inset

 for 
\begin_inset Formula $i=1,\dots,n$
\end_inset

.
\end_layout

\begin_layout Standard
For areal data, the spatial domain 
\begin_inset Formula $\mathcal{D}$
\end_inset

 is partitioned into 
\begin_inset Formula $n$
\end_inset

 regions 
\begin_inset Formula $\{D_{1},\dots,D_{n}\}$
\end_inset

 such that 
\begin_inset Formula $\mathcal{D}=\bigcup\limits _{i=1}^{n}D_{i}$
\end_inset

.
 In the case of areal data, the random variables 
\begin_inset Formula $\left\{ Y(D_{i}),\bm{X}(D_{i})\right\} $
\end_inset

 are defined for regions instead of for point locations; population and
 spatial mean temperature are examples of areal data.
 The analytical method described herein can be applied to areal data if
 they are recast as geostatistical data by assuming that the data are point-refe
renced to a representative location of each region, such as the centroid.
 That is, 
\begin_inset Formula $\left\{ \bm{X}(\bm{s}_{i}),Y(\bm{s}_{i})\right\} $
\end_inset

 where 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

 is the centroid of 
\begin_inset Formula $D_{i}$
\end_inset

 for 
\begin_inset Formula $i=1,\dots,n$
\end_inset

.
\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is a variance parameter, 
\begin_inset Formula $\phi$
\end_inset

 is a range parameter, and 
\begin_inset Formula $\delta(\bm{s},\bm{t})$
\end_inset

 is the Euclidean distance between locations 
\begin_inset Formula $\bm{s}$
\end_inset

 and 
\begin_inset Formula $\bm{t}$
\end_inset

.
 The general form of a covariance function in the Matérn class is 
\begin_inset Formula 
\begin{align}
\text{Cov}(W(\bm{s}),W(\bm{t}))=\left\{ \Gamma(\nu)2^{\nu-1}\right\} ^{-1}\left\{ \delta(\bm{s},\bm{t})\phi^{-1}\sqrt{2\nu}\right\} ^{\nu}K_{\nu}\left(\delta(\bm{s},\bm{t})\phi^{-1}\sqrt{2\nu}\right)\label{eq:matern-covarinace}
\end{align}

\end_inset

where 
\begin_inset Formula $\nu$
\end_inset

 denotes the degree of smoothness, 
\begin_inset Formula $K_{\nu}$
\end_inset

 denotes the modified Bessel equation of the second kind, and as before
 
\begin_inset Formula $\phi$
\end_inset

 denotes a range parameter and 
\begin_inset Formula $\delta(\bm{s},\bm{t})$
\end_inset

 the Euclidean distance between locations 
\begin_inset Formula $\bm{s}$
\end_inset

 and 
\begin_inset Formula $\bm{t}$
\end_inset

.
 The exponential covariance function corresponds to a Matérn class covariance
 function with 
\begin_inset Formula $\nu=1/2$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%SVCR - justification
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

%Stationarity in spatial linear regression
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A random field is said to be stationary if the joint distribution of a the
 response at a finite set of locations does not change when the set of locations
 are all shifted in space by a fixed spatial lag.
 That is, letting 
\begin_inset Formula $\left\{ T(\bm{s}):\bm{s}\in\mathcal{D}\right\} $
\end_inset

 be a random field on spatial domain 
\begin_inset Formula $\mathcal{D}$
\end_inset

 that takes value 
\begin_inset Formula $T(\bm{s}_{i})$
\end_inset

 at location 
\begin_inset Formula $\bm{s}_{i}\in\mathcal{D}$
\end_inset

 for 
\begin_inset Formula $i=1,\dots,n$
\end_inset

, the random field 
\begin_inset Formula $T(\bm{s})$
\end_inset

 is stationary if 
\begin_inset Formula $F_{n}\left(T(\bm{s}_{1}),\dots,T(\bm{s}_{n})\right)=F_{n}\left(T(\bm{s}_{1}+\bm{h}),\dots,T(\bm{s}_{n}+\bm{h})\right)$
\end_inset

 where 
\begin_inset Formula $F_{n}(\cdot)$
\end_inset

 is the joint distribution of a length 
\begin_inset Formula $n$
\end_inset

 sample from 
\begin_inset Formula $T(\bm{s})$
\end_inset

 and 
\begin_inset Formula $\bm{h}$
\end_inset

 is a fixed spatial lag.
 The random field 
\begin_inset Formula $\left\{ T(\bm{s}):\bm{s}\in\mathcal{D}\right\} $
\end_inset

 is second-order stationary if the following are satisfied: 
\begin_inset Formula 
\begin{align}
E\left\{ T(\bm{s})\right\}  & =\mu\text{ for all }\bm{s}\in\mathcal{D}\notag\\
\text{var}\left\{ T(\bm{s})\right\}  & =\sigma^{2}<\infty\text{ for all }\bm{s}\in\mathcal{D}\notag\\
\text{cov}\left\{ T(\bm{s}),T(\bm{s}+\bm{h})\right\}  & =C(\bm{h})
\end{align}

\end_inset

where the function 
\begin_inset Formula $C(\cdot)$
\end_inset

 depends only on the spatial lag 
\begin_inset Formula $\bm{h}$
\end_inset

 and not on the location 
\begin_inset Formula $\bm{s}$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

%the joint distribution at any two locations in the domain does not change
 when the locations are shifted by a fixed spatial lag.
\end_layout

\end_inset

 The coefficient vector 
\begin_inset Formula $\bm{\beta}$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:spatial-regression"

\end_inset

) is a fixed constant.
 The model can be made more flexible if the coefficients are described by
 a stationary random field.
 Such a model is written 
\begin_inset Formula 
\begin{align}
Y(\bm{s})=\bm{X}(\bm{s})'\bm{\beta}(\bm{s})+\varepsilon(\bm{s})\label{eq:SVCR-process}
\end{align}

\end_inset

where 
\begin_inset Formula $\bm{\beta}(\bm{s})$
\end_inset

 is a random coefficient field with a Matérn-class covariance function and
 the spatial random effect 
\begin_inset Formula $W(\bm{s})$
\end_inset

 included in the intercept 
\begin_inset Formula $\beta_{0}(\bm{s})$
\end_inset

.
 The random coefficient field 
\begin_inset Formula $\bm{\beta}(\bm{s})$
\end_inset

 can be estimated by Markov Chain Monte Carlo (MCMC) methods under the assumptio
n that 
\begin_inset Formula $\bm{\beta}(\bm{s})$
\end_inset

 is stationary 
\begin_inset CommandInset citation
LatexCommand citep
key "Gelfand:2003"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%The spatial random effect describes the spatial pattern in the deviations
 from the systematic part of the model.
 When fitting the spatial regression model (
\backslash
ref{eq:spatial-regression}), it is usually required that the the fitted
 values of the spatial random effect and of the residuals sum to zero, i.e.
 $
\backslash
sum
\backslash
limits_{i=1}^n
\backslash
hat{W}(
\backslash
bm{s}_i) = 0$ and $
\backslash
sum
\backslash
limits_{i=1}^n
\backslash
hat{
\backslash
varepsilon}(
\backslash
bm{s}_i) = 0$.
 This mode of analysis is appropriate when the systematic part of the regression
 model does not vary between locations.
 On the other hand, a VCR model is appropriate for the case where the systematic
 part of the regression model does vary across locations.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%Spatial VCR
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Alternatively, kernel-based and spline-based methods can be considered for
 fitting VCR models without assuming the coefficients are described by a
 stationary random field.
\end_layout

\begin_layout Standard
Coefficients for a spline-based VCR model are estimated by maximizing a
 penalized global likelihood, with the penalty calculated from the wiggliness
 of the coefficient surface 
\begin_inset CommandInset citation
LatexCommand citep
key "Wood:2006"

\end_inset

.
 This contrasts to kernel-based estimates of the coefficients in a VCR model,
 which maximize a local likelihood to estimate the local coefficients at
 each sampling location 
\begin_inset CommandInset citation
LatexCommand citep
key "Loader:1999"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Fan:1999"

\end_inset

 demonstrated that the optimal kernel bandwidth estimate for a VCR model
 can be found via a two-step technique.
\end_layout

\begin_layout Standard
Model selection in VCR models may be local or global.
 Global selection means including or excluding variables everywhere in the
 spatial domain, while local selection means including or excluding variables
 at individual locations within the spatial domain.
 For global model selection in spline-based VCR models, 
\begin_inset CommandInset citation
LatexCommand citet
key "Wang:2008a"

\end_inset

 proposed a SCAD penalty 
\begin_inset CommandInset citation
LatexCommand citep
key "Fan:2001"

\end_inset

 for variable selection in spline-based VCR models with a univariate effect-modi
fying variable.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Antoniadis:2012a"

\end_inset

 used the nonnegative Garrote penalty 
\begin_inset CommandInset citation
LatexCommand citep
key "Breiman:1995"

\end_inset

 in P-spline-based VCR models having a univariate effect-modifying variable.
\end_layout

\begin_layout Standard
Wavelet methods for fitting SVCR models were explored by 
\begin_inset CommandInset citation
LatexCommand citet
key "Shang-2011"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang-2011"

\end_inset

.
 Sparsity in the wavelet coefficients is achieved either by 
\begin_inset Formula $\ell_{1}$
\end_inset

-penalization (also known as the Lasso 
\begin_inset CommandInset citation
LatexCommand citep
key "Tibshirani:1996"

\end_inset

) 
\begin_inset CommandInset citation
LatexCommand citep
key "Shang-2011"

\end_inset

 or by Bayesian variable selection 
\begin_inset CommandInset citation
LatexCommand citep
key "Zhang-2011"

\end_inset

.
 Sparsity in the wavelet domain does not imply sparsity in the covariates,
 though, so neither method is suitable for local variable selection.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%GWR
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Geographically weighted regression (GWR) is a kernel-based method for estimating
 the coefficients of an SVCR model where the kernel weights are based on
 the distance between sampling locations 
\begin_inset CommandInset citation
LatexCommand citep
key "Brundson:1998a, Fotheringham:2002"

\end_inset

.
 At each sampling location, traditional GWR estimates the local regression
 coefficients by the local likelihood 
\begin_inset CommandInset citation
LatexCommand citep
key "Loader:1999"

\end_inset

.
 As a kernel-based smoother for regression coefficients, traditional GWR
 tends to exhibit bias near the boundary of the region being modeled 
\begin_inset CommandInset citation
LatexCommand citep
key "Hastie:1993b"

\end_inset

.
 One way to reduce the boundary-effect bias is to model the coefficient
 surface as locally linear rather than locally constant by including coefficient
-by-location interactions 
\begin_inset CommandInset citation
LatexCommand citep
key "Wang:2008b"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%Local variable selection
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Traditional GWR relies on 
\emph on
a priori
\emph default
 global model selection to decide which variables should be included in
 the model.
 The idea of using Lasso regularization for local variable selection in
 a GWR model appears in the literature as the geographically weighted Lasso
 (GWL) 
\begin_inset CommandInset citation
LatexCommand citep
key "Wheeler:2009"

\end_inset

.
 The GWL applies the Lasso for local variable selection and uses a jackknife
 criterion for selection of the Lasso tuning parameters.
 Because the jackknife criterion can only be computed at sampling locations
 where the response variable is observed, the GWL cannot be used to impute
 missing values of the response variable nor to interpolate the coefficient
 surface and/or the response variable between sampling locations.
\end_layout

\begin_layout Standard
Lasso regularization for model selection, while popular, can leave relevant
 covariates out of the model when they are correlated with other covariates,
 and the predictive performance of the Lasso may be dominated in such a
 case by ridge regression, which does not allow for local model selection
 
\begin_inset CommandInset citation
LatexCommand citep
key "Tibshirani:1996"

\end_inset

.
 The elastic net is a regularization method that combines a 
\begin_inset Formula $\ell_{1}$
\end_inset

 (Lasso) and a 
\begin_inset Formula $\ell_{2}$
\end_inset

 (ridge) penalty on the estimated coefficients, overcoming these drawbacks
 of the Lasso 
\begin_inset CommandInset citation
LatexCommand citep
key "Zou:2005"

\end_inset

.
\end_layout

\begin_layout Standard
Additionally, Lasso regularization does not generally produce consistent
 estimates of the relevant covariates 
\begin_inset CommandInset citation
LatexCommand citep
key "Leng-2006"

\end_inset

.
 The adaptive Lasso (AL) 
\begin_inset CommandInset citation
LatexCommand citep
key "Zou:2006"

\end_inset

 is an improvement to the Lasso that does produce consistent estimates of
 the coefficients and has been shown to have appealing properties for automating
 variable selection, which under suitable conditions include the 
\begin_inset Quotes eld
\end_inset

oracle" property of asymptotically selecting exactly the correct set of
 covariates for inclusion in a regression model.
\end_layout

\begin_layout Standard
The remainder of this document is organized as follows.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "section:simulation"

\end_inset

, a simulation study is conducted to assess the performance of the GWEN
 in variable selection and coefficient estimation.
 An application to real data is presented in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "section:data-analysis"

\end_inset

.
\end_layout

\begin_layout Section
Spatially varying coefficients regression 
\begin_inset CommandInset label
LatexCommand label
name "section:SVCR"

\end_inset


\end_layout

\begin_layout Subsection
Model
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $n$
\end_inset

 data points, observed at sampling locations 
\begin_inset Formula $\bm{s}_{i}=(s_{i,1}\;\; s_{i,2})^{T}$
\end_inset

 for 
\begin_inset Formula $i=1,\dots,\bm{s}_{n}$
\end_inset

, which are distributed in a spatial domain 
\begin_inset Formula $D\subset\mathbb{R}^{2}$
\end_inset

 according to a density 
\begin_inset Formula $f(\bm{s})$
\end_inset

.
 For 
\begin_inset Formula $i=1,\dots,n$
\end_inset

, let 
\begin_inset Formula $y(\bm{s}_{i})$
\end_inset

 and 
\begin_inset Formula $\bm{x}(\bm{s}_{i})$
\end_inset

 denote, respectively, the univariate response and the 
\begin_inset Formula $(p+1)$
\end_inset

-variate vector of covariates measured at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

.
 At each location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

, assume that the outcome is related to the covariates by a linear model
 where the coefficients 
\begin_inset Formula $\bm{\beta}(\bm{s}_{i})$
\end_inset

 may be spatially-varying and 
\begin_inset Formula $\varepsilon(\bm{s}_{i})$
\end_inset

 is random error at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

.
 That is, 
\begin_inset Formula 
\begin{align}
y(\bm{s}_{i})=\bm{x}(\bm{s}_{i})'\bm{\beta}(\bm{s}_{i})+\varepsilon(\bm{s}_{i}).\label{eq:lm(s)}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Further assume that the error term 
\begin_inset Formula $\varepsilon(\bm{s}_{i})$
\end_inset

 is normally distributed with zero mean and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

, and that 
\begin_inset Formula $\varepsilon(\bm{s}_{i})$
\end_inset

, 
\begin_inset Formula $i=1,\dots,n$
\end_inset

 are independent.
 That is, 
\begin_inset Formula 
\begin{align}
\bm{\varepsilon}\overset{iid}{\sim}\mathcal{N}\left(0,\sigma^{2}\right).\label{eq:err}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
In the context of nonparametric regression, the boundary-effect bias can
 be reduced by local polynomial modeling, usually in the form of a locally
 linear model 
\begin_inset CommandInset citation
LatexCommand citep
key "Fan-1996"

\end_inset

.
 Here, locally linear coefficients are estimated by augmenting the local
 design matrix with covariate-by-location interactions in two dimensions
 as proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "Wang:2008b"

\end_inset

.
 The augmented local design matrix at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

 is 
\begin_inset Formula 
\begin{align}
\bm{Z}(\bm{s}_{i})=\left(\bm{X}\:\: L_{i}\bm{X}\:\: M_{i}\bm{X}\right)
\end{align}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\bm{X}$
\end_inset

 is the unaugmented matrix of covariates, 
\begin_inset Formula $L_{i}=\text{diag}\{s_{i'_{1}}-s_{i_{1}}\}$
\end_inset

 and 
\begin_inset Formula $M_{i}=\text{diag}\{s_{i'_{2}}-s_{i_{2}}\}$
\end_inset

 for 
\begin_inset Formula $i'=1,\dots,n$
\end_inset

.
\end_layout

\begin_layout Standard
Now we have that 
\begin_inset Formula $Y(\bm{s}_{i})=\left\{ \bm{Z}(\bm{s}_{i})\right\} _{i}^{T}\bm{\zeta}(\bm{s}_{i})+\varepsilon(\bm{s}_{i})$
\end_inset

, where 
\begin_inset Formula $\left\{ \bm{Z}(\bm{s}_{i})\right\} _{i}^{T}$
\end_inset

 is the 
\begin_inset Formula $i$
\end_inset

th row of the matrix 
\begin_inset Formula $\bm{Z}(\bm{s}_{i})$
\end_inset

 as a row vector, and 
\begin_inset Formula $\bm{\zeta}(\bm{s}_{i})$
\end_inset

 is the vector of local coefficients at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

, augmented with the local gradients of the coefficient surfaces in the
 two spatial dimensions, indicated by 
\begin_inset Formula $\nabla_{u}$
\end_inset

 and 
\begin_inset Formula $\nabla_{v}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bm{\zeta}(\bm{s}_{i})=\left(\bm{\beta}(\bm{s}_{i})^{T}\;\;\nabla_{u}\bm{\beta}(\bm{s}_{i})^{T}\;\;\nabla_{v}\bm{\beta}(\bm{s}_{i})^{T}\right)^{T}
\]

\end_inset


\end_layout

\begin_layout Subsection
Estimation
\end_layout

\begin_layout Standard
The total log-likelihood of the observed data is the sum of the log-likelihood
 of each individual observation: 
\begin_inset Formula 
\begin{align}
\ell\left\{ \bm{\zeta}\right\} =-(1/2)\sum_{i=1}^{n}\left[\log{\sigma^{2}}+\sigma^{-2}\left\{ y(\bm{s}_{i})-\bm{z}'(\bm{s}_{i})\bm{\zeta}(\bm{s}_{i})\right\} ^{2}\right].\label{eq:coefficients}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Since there are a total of 
\begin_inset Formula $n\times3(p+1)+1$
\end_inset

 parameters for 
\begin_inset Formula $n$
\end_inset

 observations, the model is not identifiable and it is not possible to directly
 maximize the total likelihood.
 But since the coefficient functions are smooth, the coefficients at location
 
\begin_inset Formula $\bm{s}$
\end_inset

 can approximate the coefficients within some neighborhood of 
\begin_inset Formula $\bm{s}$
\end_inset

, with the quality of the approximation declining as the distance from 
\begin_inset Formula $\bm{s}$
\end_inset

 increases.
\end_layout

\begin_layout Standard
This intuition is formalized by the local likelihood, which is maximized
 at location 
\begin_inset Formula $\bm{s}$
\end_inset

 to estimate the local coefficients 
\begin_inset Formula $\bm{\zeta}(\bm{s})$
\end_inset

: 
\begin_inset Formula 
\begin{align}
\mathcal{L}\left\{ \bm{\zeta}(\bm{s})\right\}  & =\prod_{i=1}^{n}\left\{ \left(2\pi\sigma^{2}\right)^{-1/2}\exp\left[-(1/2)\sigma^{-2}\left\{ y(\bm{s}_{i})-\bm{z}'(\bm{s}_{i})\bm{\zeta}(\bm{s})\right\} ^{2}\right]\right\} ^{K_{h}(\|\bm{s}-\bm{s}_{i}\|)},\label{eq:local-likelihood}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
The weights are computed from a kernel function 
\begin_inset Formula $K_{h}(\cdot)$
\end_inset

 such as the Epanechnikov kernel: 
\begin_inset Formula 
\begin{align}
K_{h}(\|\bm{s}_{i}-\bm{s}_{i'}\|) & =h^{-2}K\left(h^{-1}\|\bm{s}_{i}-\bm{s}_{i'}\|\right)\notag\label{eq:epanechnikov}\\
K(x) & =\begin{cases}
(3/4)(1-x^{2}) & \mbox{ if }x<1,\\
0 & \mbox{ if }x\geq1.
\end{cases}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Thus, the local log-likelihood function is, up to an additive constant:
 
\begin_inset Formula 
\begin{align}
\ell\left\{ \bm{\zeta}(\bm{s})\right\}  & =-(1/2)\sum_{i=1}^{n}K_{h}(\|\bm{s}-\bm{s}_{i}\|)\left[\log{\sigma^{2}}+\sigma^{-2}\left\{ y(\bm{s}_{i})-\bm{z}'(\bm{s}_{i})\bm{\zeta}(\bm{s})\right\} ^{2}\right].\label{eq:local-log-likelihood}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Letting 
\begin_inset Formula $\bm{W}(\bm{s})$
\end_inset

 be a diagonal weight matrix where 
\begin_inset Formula $W_{ii}(\bm{s})=K_{h}(\|\bm{s}-\bm{s}_{i}\|)$
\end_inset

, the local likelihood is maximized by weighted least squares: 
\begin_inset Formula 
\begin{align}
\mathcal{S}\left\{ \bm{\zeta}(\bm{s})\right\}  & =(1/2)\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} ^{T}\bm{W}(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} ^{T}\notag\label{eq:zeta-hat}\\
\therefore\tilde{\bm{\zeta}}(\bm{s}) & =\left\{ \bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}(\bm{s})\right\} ^{-1}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Y}
\end{align}

\end_inset


\end_layout

\begin_layout Section
Local variable selection and parameter estimation
\begin_inset CommandInset label
LatexCommand label
name "section:model-selection"

\end_inset


\end_layout

\begin_layout Standard
Estimating the local coefficients by (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:zeta-hat"

\end_inset

) relies on 
\emph on
a priori
\emph default
 variable selection.
 The goal of local adaptive grouped regularization (LAGR) is to simultaneously
 select the locally relevant predictors and estimate the local coefficients.
\end_layout

\begin_layout Subsection
Local variable selection
\end_layout

\begin_layout Standard
The proposed LAGR penalty is an adaptive 
\begin_inset Formula $\ell_{1}$
\end_inset

 penalty akin to the adaptive group lasso 
\begin_inset CommandInset citation
LatexCommand citep
key "Wang-Leng-2008,Zou-2006"

\end_inset

.
 Grouped variables are selected together for inclusion in the model.
 Each group in a LAGR model consists of one covariate and its gradients
 on the two dimensions of spatial location.
 That is, 
\begin_inset Formula $\bm{\zeta}_{j}(\bm{s})=\left(\beta_{j}(\bm{s})\;\;\;\nabla_{u}\beta_{j}(\bm{s})\;\;\;\nabla_{v}\beta_{j}(\bm{s})\right)^{T}$
\end_inset

 for 
\begin_inset Formula $j=1,\dots,p$
\end_inset

.
\end_layout

\begin_layout Standard
The objective function for the LAGR at location 
\begin_inset Formula $\bm{s}$
\end_inset

 is the penalized local sum of squares: 
\begin_inset Formula 
\begin{align}
Q\{\bm{\zeta}(\bm{s})\} & =\mathcal{S}\left\{ \bm{\zeta}(\bm{s})\right\} +\mathcal{J}\{\bm{\zeta}(\bm{s})\}\notag\label{eq:adaptive-lasso-WLS}\\
 & =(1/2)\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} ^{T}\bm{W}(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} ^{T}+\sum_{j=1}^{p}\phi_{j}(\bm{s})\|\bm{\zeta}_{j}(\bm{s})\|
\end{align}

\end_inset


\end_layout

\begin_layout Standard
which is the sum of the weighted sum of squares 
\begin_inset Formula $\mathcal{S}\left\{ \bm{\zeta}(\bm{s})\right\} $
\end_inset

 and the LAGR penalty 
\begin_inset Formula $\mathcal{J}\{\bm{\zeta}(\bm{s})\}$
\end_inset

.
\end_layout

\begin_layout Standard
The LAGR penalty for the 
\begin_inset Formula $j$
\end_inset

th group of coefficients 
\begin_inset Formula $\bm{\zeta}_{j}(\bm{s})$
\end_inset

 at location 
\begin_inset Formula $\bm{s}$
\end_inset

 is 
\begin_inset Formula $\phi_{j}(\bm{s})=\lambda_{n}(\bm{s})\|\tilde{\bm{\zeta}}_{j}(\bm{s})\|^{-\gamma}$
\end_inset

, where 
\begin_inset Formula $\lambda_{n}(\bm{s})>0$
\end_inset

 is a the local tuning parameter applied to all coefficients at location
 
\begin_inset Formula $\bm{s}$
\end_inset

 and 
\begin_inset Formula $\tilde{\bm{\zeta}}_{j}(\bm{s})$
\end_inset

 is the vector of unpenalized local coefficients from (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:zeta-hat"

\end_inset

).
\end_layout

\begin_layout Subsection
Computation
\end_layout

\begin_layout Subsubsection
Tuning parameter selection
\end_layout

\begin_layout Standard
Implementing LAGR requires the selection of local tuning parameters.
 The criteria commonly used for selecting tuning parameters in lasso-type
 models are appropriate here, including GCV 
\begin_inset CommandInset citation
LatexCommand citep
key "Wahba:1990"

\end_inset

, Cp 
\begin_inset CommandInset citation
LatexCommand citep
key "Mallows-1973"

\end_inset

, AIC 
\begin_inset CommandInset citation
LatexCommand citep
key "Akaike-1973"

\end_inset

, and BIC 
\begin_inset CommandInset citation
LatexCommand citep
key "Schwarz-1978"

\end_inset

.
 All of the examples and simulations presented herein used the corrected
 AIC (AICc) 
\begin_inset CommandInset citation
LatexCommand citep
key "Hurvich-1998"

\end_inset

 for tuning parameter selection:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\text{AIC}_{\text{c}}(\bm{s})=\hat{\sigma}^{-2}(\bm{s})\|\bm{Y}-\bm{Z}(\bm{s})\hat{\bm{\zeta}}(\bm{s})\|^{2}+2df+2\frac{df(df+1)}{\sum_{i=1}^{n}K_{h}(\|\bm{s}-\bm{s}_{i}\|)-df-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $df$
\end_inset

 is the degrees of freedom as defined in 
\begin_inset CommandInset citation
LatexCommand citet
key "Yuan-Lin-2006"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
df=\sum_{j=1}^{p}I\left\{ \|\hat{\bm{\zeta}}_{j}(\bm{s})\|\right\} +2\sum_{j=1}^{p}\frac{\|\hat{\bm{\zeta}}_{j}(\bm{s})\|}{\|\tilde{\bm{\zeta}}_{j}(\bm{s})\|}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Bandwidth selection
\end_layout

\begin_layout Standard
The bandwidth for a LAGR model is selected by the AICc.
 This requires an expression for the degrees of freedom for a LAGR model.
 For nonparametric regression models, the degrees of freedom are computed
 by the covariance between observations and their fitted values:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
df=\Sum_{i=1}^{n}cov(Y_{i}.\hat{Y}_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
In a LAGR model, this covariance is estimated using the local models.
 For a single local model, the covariance of the fitted values with the
 observations is a weighted average of the covariances for each data point.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\]

\end_inset


\end_layout

\begin_layout Section
Asymptotic properties
\end_layout

\begin_layout Subsection
Notation and assumptions
\end_layout

\begin_layout Standard
Consider the local model at location 
\begin_inset Formula $\bm{s}$
\end_inset

 where there are 
\begin_inset Formula $p_{0}<p$
\end_inset

 covariates 
\begin_inset Formula $\bm{X}_{(a)}(\bm{s})$
\end_inset

 with nonzero local regression coefficients, indicated by 
\begin_inset Formula $\bm{\beta}_{(a)}(\bm{s})$
\end_inset

.
 The remaining covariates 
\begin_inset Formula $\bm{X}_{(b)}(\bm{s})$
\end_inset

 have true coefficients equal to zero, indicated by 
\begin_inset Formula $\bm{\beta}_{(b)}(\bm{s})$
\end_inset

.
 Without loss of generality, assume these are covariates 
\begin_inset Formula $1,\dots,p_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\Psi=E\left\{ \bm{X}(\bm{s}_{1})\bm{X}^{T}(\bm{s}_{1})\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $h=O(n^{-1/6})$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $a_{n}=\max\{\phi_{j}(\bm{s}),j\le p_{0}\}$
\end_inset

 be the largest penalty applied to a covariate group whose true coefficient
 norm is nonzero, and 
\begin_inset Formula $b_{n}=\min\{\phi_{j}(\bm{s}),j>p_{0}\}$
\end_inset

 be the smallest penalty applied to a covariate group whose true coefficient
 norm is zero.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\bm{Z}_{k}(\bm{s})$
\end_inset

 be the design matrix for covariate group 
\begin_inset Formula $k$
\end_inset

, and 
\begin_inset Formula $\bm{Z}_{-k}(\bm{s})$
\end_inset

 be the design matrix for all the data except covariate group 
\begin_inset Formula $k$
\end_inset

, respectively.
 Similarly, let 
\begin_inset Formula $\bm{\zeta}_{k}(\bm{s})$
\end_inset

 be the coefficients for covariate group 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $\bm{\zeta}_{-k}(\bm{s})$
\end_inset

 be the coefficients for all covariate groups except 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Standard
Finally, let 
\begin_inset Formula $\kappa_{0}=\int_{R^{2}}K(\|\bm{s}\|)ds$
\end_inset

, 
\begin_inset Formula $\kappa_{2}=\int_{R^{2}}[(1,0)\bm{s}]^{2}K(\|\bm{s}\|)ds=\int_{R^{2}}[(0,1)\bm{s}]^{2}K(\|\bm{s}\|)ds$
\end_inset

, and 
\begin_inset Formula $\nu_{0}=\int_{R^{2}}K^{2}(\|\bm{s}\|)ds$
\end_inset

.
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Paragraph
Asymptotic normality
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{theorem}
\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "theorem:normality"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
setstretch
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

2
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 If 
\begin_inset Formula $h^{-1}n^{-1/2}a_{n}\xrightarrow{p}0$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

 then 
\begin_inset Formula 
\[
h\sqrt{n}\left[\hat{\bm{\beta}}_{(a)}(\bm{s})-\bm{\beta}_{(a)}(\bm{s})-\frac{\kappa_{2}h^{2}}{2\kappa_{0}}\{\nabla_{uu}^{2}\bm{\beta}_{(a)}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{(a)}(\bm{s})\}\right]\xrightarrow{d}N(0,f(\bm{s})^{-1}\kappa_{0}^{-2}\nu_{0}\sigma^{2}\Psi^{-1})
\]

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{theorem}
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Selection
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{theorem}
\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "theorem:selection"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
setstretch
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

2
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 If 
\begin_inset Formula $h^{-1}n^{-1/2}a_{n}\xrightarrow{p}\infty$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

 then 
\begin_inset Formula $P\left\{ \|\hat{\bm{\zeta}}_{j}(\bm{s})\|=0\right\} \to0$
\end_inset

 if 
\begin_inset Formula $j\le p_{0}$
\end_inset

 and 
\begin_inset Formula $P\left\{ \|\hat{\bm{\zeta}}_{j}(\bm{s})\|=0\right\} \to1$
\end_inset

 if 
\begin_inset Formula $j>p_{0}$
\end_inset

.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{theorem}
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Remarks
\end_layout

\begin_layout Standard
Together, TheoremÂ 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset

 and Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:selection"

\end_inset

 indicate that the LAGR estimates have the same asymptotic distribution
 as a local regression model where the nonzero coefficients are known in
 advance 
\begin_inset CommandInset citation
LatexCommand citep
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

, and that the LAGR estimates of true zero coefficients go to zero with
 probability one.
 Thus, selection and estimation by LAGR has the oracle property.
\end_layout

\begin_layout Paragraph
A note on rates
\end_layout

\begin_layout Standard
To prove the oracle properties of LAGR, we assumed that 
\begin_inset Formula $h^{-1}n^{-1/2}a_{n}\xrightarrow{p}0$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

.
 Therefore, 
\begin_inset Formula $h^{-1}n^{-1/2}\lambda_{n}(\bm{s})\to0$
\end_inset

 for 
\begin_inset Formula $j\le p_{0}$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}\lambda_{n}(\bm{s})\|\bm{\zeta}_{j}(\bm{s})\|^{-\gamma}\to\infty$
\end_inset

 for 
\begin_inset Formula $j>p_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
We require that 
\begin_inset Formula $\lambda_{n}(\bm{s})$
\end_inset

 can satisfy both assumptions.
 Suppose 
\begin_inset Formula $\lambda_{n}(\bm{s})=n^{\alpha}$
\end_inset

, and recall that 
\begin_inset Formula $h=O(n^{-1/6})$
\end_inset

 and 
\begin_inset Formula $\|\tilde{\bm{\zeta}}_{p}(\bm{s})\|=O(h^{-1}n^{-1/2})$
\end_inset

.
 Then 
\begin_inset Formula $h^{-1}n^{-1/2}\lambda_{n}(\bm{s})=O(n^{-1/3+\alpha})$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}\lambda_{n}(\bm{s})\|\tilde{\bm{\zeta}}_{p}(\bm{s}\|^{-\gamma}=O(n^{-2/3+\alpha+\gamma/3})$
\end_inset

.
\end_layout

\begin_layout Standard
So 
\begin_inset Formula $(2-\gamma)/3<\alpha<1/3$
\end_inset

, which can only be satisfied for 
\begin_inset Formula $\gamma>1$
\end_inset

.
\end_layout

\begin_layout Section
Simulations
\end_layout

\begin_layout Standard
A simulation study was undertaken to assess the performance of LAGR for
 local variable selection and coefficient estimation.
\end_layout

\begin_layout Section
Data example
\end_layout

\begin_layout Standard
Here we present an application of LAGR to a real data set.
 The data is the Boston house price data from 
\begin_inset CommandInset citation
LatexCommand citet
key "Harrison-Rubinfeld-1978; Gilley-Pace-1996; Pace-Gilley-1997"

\end_inset

.
 This is areal data, measured on census tracts in Boston, Massachusetts
 for the 1978 Harrison and Rubinfeld paper.
 The response variable, MEDV, is the median selling price of homes in the
 census block (capped at USD 50,000).
 Predictors whose local coefficients were estimated via LAGR are CRIM, represent
ing the per capita crime rate; RM, the average number of rooms for houses
 sold within the census tract; RAD, which measures the accessibility of
 radial roads from the tract; TAX, the full-value property tax rate per
 USD 10,000 (constant for all Boston tracts); and LSTAT, the percentage
 of the population in a tract considered 
\begin_inset Quotes eld
\end_inset

lower status".
 The same data was analyzed in 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

.
\end_layout

\begin_layout Standard
An adaptive bandwidth was used, with the bandwidth at each location set
 such that the sum of the kernel weights for the local model was 17% of
 the number of observations.
 The results are
\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%dummy comment inserted by tex2lyx to ensure that this paragraph is not
 empty
\end_layout

\end_inset


\end_layout

\begin_layout Section
Proofs of theorems
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "app:proofs"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{proof}
\end_layout

\end_inset

[Proof of theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset

] 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
setstretch
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

2
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 Define 
\begin_inset Formula $V_{4}^{(n)}(\bm{u})$
\end_inset

 to be the 
\begin_inset Formula 
\begin{align}
\mkern-36muV_{4}^{(n)}(\bm{u}) & =Q\left\{ \bm{\zeta}(\bm{s})+h^{-1}n^{-1/2}\bm{u}\right\} -Q\left\{ \bm{\zeta}(\bm{s})\right\} \notag\label{eq:consistency}\\
 & \mkern-36mu=(1/2)\left[\bm{Y}-\bm{Z}(\bm{s})\left\{ \bm{\zeta}(\bm{s})+h^{-1}n^{-1/2}\bm{u}\right\} \right]^{T}\bm{W}(\bm{s})\left[\bm{Y}-\bm{Z}(\bm{s})\left\{ \bm{\zeta}(\bm{s})+h^{-1}n^{-1/2}\bm{u}\right\} \right]\notag\\
 & +\sum_{j=1}^{p}\phi_{j}(\bm{s})\|\bm{\zeta}_{j}(\bm{s})+h^{-1}n^{-1/2}\bm{u}_{j}\|\notag\\
 & -(1/2)\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} ^{T}\bm{W}(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} -\sum_{j=1}^{p}\phi_{j}(\bm{s})\|\bm{\zeta}_{j}(\bm{s})\|\notag\\
 & \mkern-36mu=(1/2)\bm{u}^{T}\left\{ h^{-2}n^{-1}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}(\bm{s})\right\} \bm{u}-\bm{u}^{T}\left[h^{-1}n^{-1/2}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} \right]\notag\\
 & +\sum_{j=1}^{p}n^{-1/2}\phi_{j}(\bm{s})n^{1/2}\left\{ \|\bm{\zeta}_{j}(\bm{s})+h^{-1}n^{-1/2}\bm{u}_{j}\|-\|\bm{\zeta}_{j}(\bm{s})\|\right\} 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Note the different limiting behavior of the third term between the cases
 
\begin_inset Formula $j\le p_{0}$
\end_inset

 and 
\begin_inset Formula $j>p_{0}$
\end_inset

:
\end_layout

\begin_layout Paragraph
Case 
\begin_inset Formula $j\le p_{0}$
\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $j\le p_{0}$
\end_inset

 then 
\begin_inset Formula $n^{-1/2}\phi_{j}(\bm{s})\to n^{-1/2}\lambda_{n}(\bm{s})\|\bm{\zeta}_{j}(\bm{s})\|^{-\gamma}$
\end_inset

 and 
\begin_inset Formula $|\sqrt{n}\left\{ \|\bm{\zeta}_{j}(\bm{s})+h^{-1}n^{-1/2}\bm{u}_{j}\|-\|\bm{\zeta}_{j}(\bm{s})\|\right\} |\le h^{-1}\|\bm{u}_{j}\|$
\end_inset

 so 
\begin_inset Formula 
\[
\lim\limits _{n\to\infty}\phi_{j}(\bm{s})\left(\|\bm{\zeta}_{j}(\bm{s})+h^{-1}n^{-1/2}\bm{u}_{j}\|-\|\bm{\zeta}_{j}(\bm{s})\|\right)\le h^{-1}n^{-1/2}\phi_{j}(\bm{s})\|\bm{u}_{j}\|\le h^{-1}n^{-1/2}a_{n}\|\bm{u}_{j}\|\to0
\]

\end_inset


\end_layout

\begin_layout Paragraph
Case 
\begin_inset Formula $j>p_{0}$
\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $j>p_{0}$
\end_inset

 then 
\begin_inset Formula $\phi_{j}(\bm{s})\left(\|\bm{\zeta}_{j}(\bm{s})+h^{-1}n^{-1/2}\bm{u}_{j}\|-\|\bm{\zeta}_{j}(\bm{s})\|\right)=\phi_{j}(\bm{s})h^{-1}n^{-1/2}\|\bm{u}_{j}\|$
\end_inset

.
\end_layout

\begin_layout Standard
And note that 
\begin_inset Formula $h=O(n^{-1/6})$
\end_inset

 so that if 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

 then 
\begin_inset Formula $h^{-1}n^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

.
\end_layout

\begin_layout Standard
Now, if 
\begin_inset Formula $\|\bm{u}_{j}\|\ne0$
\end_inset

 then 
\begin_inset Formula 
\[
h^{-1}n^{-1/2}\phi_{j}(\bm{s})\|\bm{u}_{j}\|\ge h^{-1}n^{-1/2}b_{n}\|\bm{u}_{j}\|\to\infty
\]

\end_inset

.
 On the other hand, if 
\begin_inset Formula $\|\bm{u}_{j}\|=0$
\end_inset

 then 
\begin_inset Formula $h^{-1}n^{-1/2}\phi_{j}(\bm{s})\|\bm{u}_{j}\|=0$
\end_inset

.
\end_layout

\begin_layout Standard
Thus, the limit of 
\begin_inset Formula $V_{4}^{(n)}(\bm{u})$
\end_inset

 is the same as the limit of 
\begin_inset Formula $V_{4}^{*(n)}(\bm{u})$
\end_inset

 where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mkern-72muV_{4}^{*(n)}(\bm{u})=\begin{cases}
(1/2)\bm{u}^{T}\left\{ h^{-2}n^{-1}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}(\bm{s})\right\} \bm{u}-\bm{u}^{T}\left[h^{-1}n^{-1/2}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} \right] & \mbox{ if }\|\bm{u}_{j}\|=0\;\forall j>p_{0}\\
\infty & \mbox{ otherwise }
\end{cases}.
\]

\end_inset


\end_layout

\begin_layout Standard
From which it is clear that 
\begin_inset Formula $V_{4}^{*(n)}(\bm{u})$
\end_inset

 is convex and its unique minimizer is 
\begin_inset Formula $\hat{\bm{u}}^{(n)}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
0 & =\left\{ h^{-2}n^{-1}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}(\bm{s})\right\} \hat{\bm{u}}^{(n)}-\left[h^{-1}n^{-1/2}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} \right]\notag\label{eq:limit}\\
\therefore\hat{\bm{u}}^{(n)} & =\left\{ n^{-1}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}(\bm{s})\right\} ^{-1}\left[hn^{-1/2}\bm{Z}^{T}(\bm{s})\bm{W}(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} \right]\notag\\
\end{align}

\end_inset


\end_layout

\begin_layout Standard
By the epiconvergence results of 
\begin_inset CommandInset citation
LatexCommand citet
key "Geyer-1994"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "Knight-Fu-2000"

\end_inset

, the minimizer of the limiting function is the limit of the minimizers
 
\begin_inset Formula $\hat{\bm{u}}^{(n)}$
\end_inset

.
 And since, by Lemma 2 of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{\bm{u}}^{(n)}\xrightarrow{d}N\left(\frac{\kappa_{2}h^{2}}{2\kappa_{0}}\{\nabla_{uu}^{2}\bm{\zeta}_{j}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{j}(\bm{s})\},f(\bm{s})\kappa_{0}^{-2}\nu_{0}\sigma^{2}\Psi^{-1}\right)
\end{equation}

\end_inset

the result is proven.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{proof}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{proof}
\end_layout

\end_inset

[Proof of theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:selection"

\end_inset

] 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
setstretch
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

2
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 We showed in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset

 that 
\begin_inset Formula $\hat{\bm{\zeta}}_{j}(\bm{s})\xrightarrow{p}\bm{\zeta}_{j}(\bm{s})+\frac{\kappa_{2}h^{2}}{2\kappa_{0}}\{\nabla_{uu}^{2}\bm{\zeta}_{j}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{j}(\bm{s})\}$
\end_inset

, so to complete the proof of selection consistency, it only remains to
 show that 
\begin_inset Formula $P\left\{ \hat{\bm{\zeta}}_{j}(\bm{s})=0\right\} \to1$
\end_inset

 if 
\begin_inset Formula $j>p_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
The proof is by contradiction.
 Without loss of generality we consider only the case 
\begin_inset Formula $j=p$
\end_inset

.
\end_layout

\begin_layout Standard
Assume 
\begin_inset Formula $\|\hat{\bm{\zeta}}_{p}(\bm{s})\|\ne0$
\end_inset

.
 Then 
\begin_inset Formula $Q\left\{ \bm{\zeta}(\bm{s})\right\} $
\end_inset

 is differentiable w.r.t.
 
\begin_inset Formula $\bm{\zeta}_{p}(\bm{s})$
\end_inset

 and is minimized where 
\begin_inset Formula 
\begin{align}
0 & =\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\left\{ \bm{Y}-\bm{Z}_{-p}(\bm{s})\hat{\bm{\zeta}}_{-p}(\bm{s})-\bm{Z}_{p}(\bm{s})\hat{\bm{\zeta}}_{p}(\bm{s})\right\} -\phi_{p}(\bm{s})\frac{\hat{\bm{\zeta}}_{p}(\bm{s})}{\|\hat{\bm{\zeta}}_{p}(\bm{s})\|}\notag\\
 & =\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\left[\bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})-\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}(\bm{s})\right\} \right]\notag\\
 & \mkern+72mu+\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}_{-p}(\bm{s})\left[\bm{\zeta}_{-p}(\bm{s})+\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{-p}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{-p}(\bm{s})\right\} -\hat{\bm{\zeta}}_{-p}(\bm{s})\right]\notag\\
 & \mkern+72mu+\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}_{p}(\bm{s})\left[\bm{\zeta}_{p}(\bm{s})+\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{p}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{p}(\bm{s})\right\} -\hat{\bm{\zeta}}_{p}(\bm{s})\right]\notag\\
 & \mkern+72mu-\phi_{p}(\bm{s})\frac{\hat{\bm{\zeta}}_{p}(\bm{s})}{\|\hat{\bm{\zeta}}_{p}(\bm{s})\|}\notag\\
\end{align}

\end_inset


\end_layout

\begin_layout Standard
So 
\begin_inset Formula 
\begin{align}
\frac{h}{\sqrt{n}}\phi_{p}(\bm{s})\frac{\hat{\bm{\zeta}}_{p}(\bm{s})}{\|\hat{\bm{\zeta}}_{p}(\bm{s})\|} & =\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\frac{h}{\sqrt{n}}\left[\bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})-\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}(\bm{s})\right\} \right]\notag\label{eq:selection}\\
 & +\left\{ n^{-1}\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}_{-p}(\bm{s})\right\} h\sqrt{n}\left[\bm{\zeta}_{-p}(\bm{s})+\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{-p}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{-p}(\bm{s})\right\} -\hat{\bm{\zeta}}_{-p}(\bm{s})\right]\notag\\
 & +\left\{ n^{-1}\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}_{p}(\bm{s})\right\} h\sqrt{n}\left[\bm{\zeta}_{p}(\bm{s})+\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{p}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{p}(\bm{s})\right\} -\hat{\bm{\zeta}}_{p}(\bm{s})\right]
\end{align}

\end_inset


\end_layout

\begin_layout Standard
From Lemma 2 of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

, 
\begin_inset Formula $\left\{ n^{-1}\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}_{-p}(\bm{s})\right\} =O_{p}(1)$
\end_inset

 and 
\begin_inset Formula $\left\{ n^{-1}\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\bm{Z}_{p}(\bm{s})\right\} =O_{p}(1)$
\end_inset

.
\end_layout

\begin_layout Standard
From Theorem 3 of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

, we have that 
\begin_inset Formula $h\sqrt{n}\left[\hat{\bm{\zeta}}_{-p}(\bm{s})-\bm{\zeta}_{-p}(\bm{s})-\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\zeta_{-p}(\bm{s})+\nabla_{vv}^{2}\zeta_{-p}(\bm{s})\right\} \right]=O_{p}(1)$
\end_inset

 and 
\begin_inset Formula $h\sqrt{n}\left[\hat{\bm{\zeta}}_{p}(\bm{s})-\bm{\zeta}_{p}(\bm{s})-\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\zeta_{p}(\bm{s})+\nabla_{vv}^{2}\zeta_{p}(\bm{s})\right\} \right]=O_{p}(1)$
\end_inset

.
\end_layout

\begin_layout Standard
So the second and third terms of the sum in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:selection"

\end_inset

) are 
\begin_inset Formula $O_{p}(1)$
\end_inset

.
\end_layout

\begin_layout Standard
We showed in the proof of 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset

 that 
\begin_inset Formula $h\sqrt{n}\bm{Z}_{p}^{T}(\bm{s})\bm{W}(\bm{s})\left[\bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})-\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}(\bm{s})\right\} \right]=O_{p}(1)$
\end_inset

.
\end_layout

\begin_layout Standard
The three terms of the sum to the right of the equals sign in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:selection"

\end_inset

) are 
\begin_inset Formula $O_{p}(1)$
\end_inset

, so for 
\begin_inset Formula $\hat{\bm{\zeta}}_{p}(\bm{s})$
\end_inset

 to be a solution, we must have that 
\begin_inset Formula $hn^{-1/2}\phi_{p}(\bm{s})\hat{\bm{\zeta}}_{p}(\bm{s})/\|\hat{\bm{\zeta}}_{p}(\bm{s})\|=O_{p}(1)$
\end_inset

.
\end_layout

\begin_layout Standard
But since by assumption 
\begin_inset Formula $\hat{\bm{\zeta}}_{p}(\bm{s})\ne0$
\end_inset

, there must be some 
\begin_inset Formula $k\in\{1,\dots,3\}$
\end_inset

 such that 
\begin_inset Formula $|\hat{\zeta}_{p_{k}}(\bm{s})|=\max\{|\hat{\zeta}_{p_{k'}}(\bm{s})|:1\le k'\le3\}$
\end_inset

.
 And for this 
\begin_inset Formula $k$
\end_inset

, we have that 
\begin_inset Formula $|\hat{\zeta}_{p_{k}}(\bm{s})|/\|\hat{\bm{\zeta}}_{p}(\bm{s})\|\ge1/\sqrt{3}>0$
\end_inset

.
\end_layout

\begin_layout Standard
Now since 
\begin_inset Formula $hn^{-1/2}b_{n}\to\infty$
\end_inset

, we have that 
\begin_inset Formula $hn^{-1/2}\phi_{p}(\bm{s})\hat{\bm{\zeta}}_{p}(\bm{s})/\|\hat{\bm{\zeta}}_{p}(\bm{s})\|\ge hb_{n}/\sqrt{3n}\to\infty$
\end_inset

 and therefore the term to the left of the equals sign dominates the sum
 to the right of the equals sign in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:selection"

\end_inset

).
 So for large enough 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $\hat{\bm{\zeta}}_{p}(\bm{s})\ne0$
\end_inset

 cannot maximize 
\begin_inset Formula $Q$
\end_inset

.
\end_layout

\begin_layout Standard
So 
\begin_inset Formula $P\left\{ \hat{\bm{\zeta}}_{(b)}(\bm{s})=0\right\} \to1$
\end_inset

.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{proof}
\end_layout

\end_inset


\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../references/gwr"
options "chicago"

\end_inset


\end_layout

\end_body
\end_document
