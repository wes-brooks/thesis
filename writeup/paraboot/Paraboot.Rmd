---
title: "Parametric bootstrap in a varying coefficient regression model estimated by local adaptive grouped regularization"
author: "Wesley Brooks"
output:
  html_document:
    fig_caption: yes
---


### Some things to know about the topic:

 - LAGR is a kernel smoothing method. Estimates of the coefficient surfaces are biased.
 - The bias at $s$ is proportional to the curvature (second derivative) of the coefficient surface at $s$.
 - Observations are not exchangeable because of the varying coefficients.
 - The coefficients of the VCR are estimated pointwise.
 - There is a global bandwidth parameter, $\phi$. In the case of the distance-based bandwidth, $h=\phi$.
 - For estimating the bandwidth parameter, aim to fit the observed data as well as possible, while using the fewest parameters possible.
 - "Fitting the data as well as possible" requires estimating the coefficient surfaces at the locations where data are observed.
 - Evaluating the fitted value at $s$ requires estimating the value of the coefficient surface at $s$.
 - The estimated coeficient surface at $s$ can be used to evaluate the fitted value $\hat{y}$ at $s$, but not at any other location.

## General approach for inference
Ours is a Monte Carlo approach to inference. The distribution of an adaptive lasso is complicated, so rather than calculate the distribution of our estimator, we will simulate it by making Monte Carlo draws. Our approach to simulating the distribution of the estimator borrows from Bayesian methods, but the hierarchical model is somewhat implicit.

## Why not use the nonparametric bootstrap?
The nonparametric bootstrap simulated the sampling distribution by drawing with replacement from the observed data. In a VCR model, the observations are not exchangeable, but a joint bootstrap procedure is feasible. In this case, that procedure would involve drawing observations, covariates, and locations jointly. That is, make $n$ draws of $(\boldsymbol{x}_i, y_i, s_i)$ with replacement.

However, pursuing inference via this nonparametric bootstrap procedure is problematic. Confidence statements about model parameters must acknowledge uncertainty in the bandwidth parameter. We evaluate the likelihood of any particular bandwidth parameter by using it in estimation and evaluating the resulting model's accuracy in fitting the data. In a nonparametric bootstrap setting, some observations are included multiple times in a bootstrap sample, while other observations are left out. Because the fits are local, the fitted values at locations that were not included in the bootstrap sample are actually predictions, which are not comparable to fitted values, even when adjusted with a covariance penalty [there's an Efron reference (the 632+ CV paper?) for this].

## And the parametric bootstrap?
Simulating the sampling distribution via the parametric bootstrap does not require exchangeable data, nor is there any problem with estimation locations. The original data set can be resampled by simulating a new observation at each location from a model, as long as the model's parameters are estimated via a consistent (unbiased?) procedure. Then the resampled data would be used to re-estimate the model parameters, with each simulated data set producing one draw of the model parameters.

The problem is that the method of LAGR is not unbiased nor consistent for the model parameters. Like kernel smoothing, to which LAGR is closely related, the local coefficient estimates have bias proportional to the curvature (second derivative) of the true coefficient surface at $s$. If the data is simulated from the biased model, then the simulated data does not share the distribution of the original data. Furthermore, estimates based on the resampled data will be made with additional bias related to the second derivative of the _estimated_ coefficient surface.

The parametric bootstrap could be applied with a bias correction, but estimating the second derivatives of the coefficients is tricky. A reasonable approximation can be reached by a local linear smoothing of the estimated first derivatives (we get the first derivatives from the local linear approximation used in estimating the coefficient surface). The interaction of the first derivative on location is the second derivative. However, this method ignores covariance between the coefficients when estimating their curvature, so the estimates are at best first-order accurate and couldn't be used to make draws from the _joint distribution_ of the coefficients and their first and second derivatives. That's the distribution we need to draw coefficients from in order to simulate the sampling distribution of the original data.

## Use the Bayesian bootstrap instead
The Bayesian bootstrap (BB) [@Rubin-1981] and the weighted likelihood bootstrap (WLB) [@Newton-Raftery-1994] are related methods that link Bayesian methods with the bootstrap. Rather than simulate the sampling distribution of the data by resampling, the BB and WLB put a distribution on the observation weights in a weighted likelihood procedure. When the weights are from a uniform Dirichlet distribution, the BB is very much like the nonparametric bootstrap.



## Empirical Bayes bandwidth inference
We estimate the bandwidth parameter by minimizing a penalized information criterion, such as the AIC. The AIC is an estimate of the expected log likelihood of new data observed from a model using the MLEs as parameters (?) 

$$AIC = E_f (y^* | \hat{\theta}(y))$$

The AIC is evaluated at the MLE of the model paramters, so it ignores variability in those model parameters (here, the coefficients). However, we can estimate the variability in the AIC during our bootstrap procedure. Estimating $\hat{h}$ is much more expensive than estimating just the coefficients, so we do that as few times as possible to get a good idea of the distribution of the bandwidth. Then the bootstrap samples can be reweighted by sampling-importance resampling (SIR) to improve the quality of our inferences.

This is very much like the WLB...


#Methods
Estimation procedure:

 + Estimate a VCR model
 + Estimate the bias in coefficient estimation
 + Draw from the sampling distribution of the coefficients, correcting for bias
 + Use the resampled coefficients to generate resampled response observations
 + From the resampled response, estimate the coefficients
 + Collect these estimates.
 
```{r import-packages, echo=FALSE, results='hide', message=FALSE} 
library(lagr)
library(dplyr)
library(knitr)
library(expm)
library(xtable)
library(doMC)
registerDoMC(3)

opts_chunk$set(echo=FALSE, message=FALSE, results='hide')
```

```{r paraboot}
read_chunk('~/git/thesis/code/inference-paper/paraboot.r')
```

```{r paraboot-run, cache=TRUE}
<<simulate-data>>
<<asymptotic-bias>>
<<empirical-bias>>
```
 
##Estimate a VCR model
The estimated value of the coefficient surface converges thus:
$$\{n h^d f(s) \}^{1/2} \left(\hat{\beta}(s) - \beta(s) - (2 \kappa_0)^{-1} \kappa_2 h^d \nabla^2 \beta(s) \right) \xrightarrow{D} N\left( 0, \kappa_0^{-2} \nu_0 \Gamma(s)^{-1} \right)$$

##Bias in coefficient estimation
The asymptotic distribution of the LAGR-estimated VCR coefficients indicates bias. The asymptotic bias is
$$(2 \kappa_0)^{-1} \kappa_2 h^d \nabla^2 \beta(s).\label{bias}$$
The bias is proportional to the second derivative of the coefficient surface. Therefore, we need to estimate $\nabla^2 \beta(s)$ in order to estimate the bias.

If the bias in estimating $\boldsymbol{\beta}(s)$ is ignored, then the biased estimate will be used for generating the parametric bootstrap draws $\boldsymbol{Y}^*_1, \dots, \boldsymbol{Y}^*_B$. Then the estimation step to reach the bootstrap estimates $\boldsymbol{\beta}^*_1(s), \dots, \boldsymbol{\beta}^*_B(s)$ will introduce another round of bias. Thus, in order use the parametric bootstrap to estimate the distribution of our estimates $\hat{\boldsymbol{\beta}}(s)$, the bias in the original estimate must be corrected.

###Bias correction
In order to estimate the bias, we must estimate the second derivative of the coefficient functions. This is done via kernel smoothing. Letting $\gamma_{1j}(s) = \nabla \hat{\beta}_j(s)$ and $\gamma_2(s) = \nabla^2 \hat{\beta}_j(s)$, the smoothed estimate of the first derivative of coefficient $j$ is
$$\nabla \hat{\beta_j}(s) = \alpha_0 \gamma_{1j}(s) + \alpha_1 \nabla^2 \gamma_{2j}(s) + \varepsilon.$$

Since we have smoothed the first derivative via a locally linear fit, the estimated second derivative is the slope of the first derivative, or $\hat{\alpha}_2(s)$. This estimate is used for estimating the bias of $\hat{\beta}_j(s)$.

This method of estimating the bias of $\hat{\beta}_j(s)$ ignores covariance does not explicitly model the covariance between the second derivatives of different coefficient functions, e.g., $\nabla^2 \hat{\beta}_1(s), \dots, \nabla^2 \hat{\beta}_p(s)$. Whether this is a problem is yet to be determined.

```{r fig.width=9, fig.height=4, fig.cap="Left to right: the intercept, $\\beta_1(s)$, and $\\beta_2(s)$ functions used to generate data for the simulation. In each plot, the true coefficient function is the solid black line, the dashed blue line is the coefficient plus the first-order asymptotic estimation bias, and the dotted red line is the coefficient plus the first- and second-order asymptotic estimation bias. The first-order estimation bias is incurred in estimating the coefficient functions from the data. The second-order estimation bias is incurred when estimating a bootstrap draw of the coefficient surface from a first-order estimate without bias correction."}
<<plot-bias>>
```

##Parametric bootstrap
Statistics are functions of the data. The distribution of a statistic is a function of the distribution of the data, and the bootstrap is a method of estimation for the data distribution. That is, we assume that $Y \sim G$ and that $F_{\boldsymbol{\theta}_0}$ is a good approximation to $G$, where $\boldsymbol{\theta}_0$ is a parameter vector and our interest is inference on $\boldsymbol{\theta}$. Then the MLE is a function of the observed data $\hat{\boldsymbol{\theta}} = T(Y)$.

The distribution of $\hat{\boldsymbol{\theta}}$ is a function of the distribution $G$, as approximated by $F_{\hat{\boldsymbol{\theta}}}$. The parametric bootstrap is used to simulate $F_{\hat{\boldsymbol{\theta}}}$, then estimation proceeds from the simulated data.

##Simulating from the estimated distribution
Note that $Y_1 \sim N(X_1^T \boldsymbol{\beta}_1, \sigma^2)$ so 
$$(X_1^T W_1 X_1)^{-1} X_1^T W_1 (Y_1 - X_1^T \boldsymbol{\beta}_1) \sim N \left(0, \sigma^2 (X_1^T W_1 X_1)^{-1} X_1^T W_1 W_1 X_1 (X_1^T W_1 X_1)^{-1}  \right).$$

From this distribution we get the MLE $\hat{\boldsymbol{\beta}}(s)$ and we can draw from the joint distribution of $\boldsymbol{\beta}(s)$.

To estimate the necessary bias correction, the second derivative of $\beta(s)$ is estimated. Since the second derivative is the slope of the slope of $\beta(s)$, we make use of the existing coefficient estimates. Leting $\nabla \hat{\beta}(s)$ be the estimated slopes of the coefficient functions (part of the MLE), we find $\nabla^2 \hat{\beta}(s)$ via a locally linear smooth of $\nabla \hat{\beta}(s)$.

Note: 
$$\beta^{*} \sim N \left(0, \sigma^2 (X_1^T W_1 X_1)^{-1} X_1^T W_1 W_1 X_1 (X_1^T W_1 X_1)^{-1}  \right).$$

Want:
$$\tilde{\beta} \xrightarrow{P} truth.$$



#Execute

##Bandwidth estimation
The bandwidth is estimated to minimize the AIC. The profile AIC is plotted in Figure \ref{fig:profile-AIC}.

```{r profile-AIC, fig.width=6, fig.height=4, fig.cap="Profile AIC trace calculated while finding the bandwidth that maximizes the AIC."}
plot(bw$trace$bw, -bw$trace$loss, type='l', bty='n', xlab="bw", ylab="-AIC", xlim=c(0,5))
```

The AIC is an approximation to the log likeliood, so we treat the profile AIC as a profile log-likelihood. Because the bandwidth is strictly positive and because of the shape of Figure ?, in this case the bandwidth is modeled with a log-normal distribution. This is an empirical Bayes procedure, under a hierarchical model with the bandwidth at a level below the coefficients. In truth, the empirical Bayes distribution for the bandwidth parameter should have some additional variability due to uncertainty in estimating the hyperparameters.

The region of greatest density includes the ten bandwidths with the smallest AIC's from the profiling step. These ten are:
```{r results='asis'}
print(xtable(bw$trace[order(bw$trace$loss)[1:10],1:2]), type='html', comment=FALSE, include.rownames=FALSE)
```



Histogram of bandwidths drawn from the estimated bandwidth distribution:

```{r fig.cap="Histogram of draws from the posterior distribution for the bandwidth parameter."}

hist(exp(rnorm(1000, mean=mu, sd=sd)), xlab='bw', ylab='Frequency', main=NA, breaks=20)
```

$\mu = \log{ \hat{h} } = `r round(mu,2)`$, $\hat{\sigma}^2 = `r signif(sd^2,2) `$

##Estimate the best fit
The means of 20 realizations of a VCR model estimated by locally linear kernel smoothing are plotted in Figure (2).

```{r realization, fig.width=9, fig.height=4, fig.cap="Left to right: the intercept, $\\beta_1(s)$, and $\\beta_2(s)$ functions, the realizable (biased) versions of the same, their estimates, and the bias-corrected estimates."}
<<plot-realization>>
```


##Make draws from the parametric bootstrap distribution
```{r paraboot-draw, cache=TRUE}
<<linked-bootstrap-sample>>
<<linked-bootstrap-estimate>>
```


