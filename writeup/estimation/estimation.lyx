#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{multirow}

\mathchardef\mhyphen="2D % Define a "math hyphen"

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}
\end_preamble
\options authoryear, review
\use_default_options true
\begin_modules
eqs-within-sections
figs-within-sections
theorems-ams-bytype
natbibapa
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 2
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\bulletLaTeX 1 ""
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Local Adaptive Grouped Regularization and its Oracle Properties
\end_layout

\begin_layout Author
Wesley Brooks, Jun Zhu, Zudi Lu
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Whereas the coefficients in traditional linear regression are scalar constants,
 the coefficients in a varying coefficient regression (VCR) model are functions
 - often 
\emph on
smooth
\emph default
 functions - of some effect-modifying variable 
\begin_inset CommandInset citation
LatexCommand citep
key "Cleveland-Grosse-1991,Hastie-Tibshirani-1993"

\end_inset

.
 Here we treat the case of a VCR model on a spatial domain where the spatial
 locaton is a two-dimensional effect-modifying parameter.
 Current practice for VCR models relies on global model selection to decide
 which variables should be included in the model, meaning that covariates
 are selected for inclusion or exclusion over the entire spatial domain.
 Various methods have been developed by using, for example, P-splines 
\begin_inset CommandInset citation
LatexCommand citep
key "Antoniadis:2012a"

\end_inset

, basis expansion 
\begin_inset CommandInset citation
LatexCommand citep
key "Wang-2008a"

\end_inset

, and local regression 
\begin_inset CommandInset citation
LatexCommand citep
key "Wang-Xia-2009"

\end_inset

.
 Since the coefficients vary in a VCR model, in principle there is no reason
 that the best model must use the same set of covariates everywhere on the
 domain - that is, some of the coefficients may be zero in part of the domain.
 New methodology is developed here for guiding the decision of which covariates
 belong in the VCR model at a specific location, or local variable selection,
 as the literature on how to do so is currently scarce.
\end_layout

\begin_layout Standard
Specifically, local adaptive grouped regularization (LAGR) is developed
 here as a method of local variable selection at any location in the domain
 of a VCR model.
 The method of LAGR applies to VCR models where the coefficients are estimated
 using locally linear kernel smoothing.
 Using kernel smoothing for nonparametric regression is described in detail
 in 
\begin_inset CommandInset citation
LatexCommand citet*
key "Fan-Gijbels-1996"

\end_inset

.
 The extension to estimating VCR models is made by 
\begin_inset CommandInset citation
LatexCommand citet
key "Fan-Zhang-1999"

\end_inset

 for a VCR with a univariate effect-modifying variable, and by 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

 for a two-dimensional effect-modifying variable and autocorrelation among
 the obverved response.
 These methods minimize the boundary effect 
\begin_inset CommandInset citation
LatexCommand citep
key "Hastie:1993b"

\end_inset

 by estimating the coefficients as local polynomials of odd degree (usually
 locally linear).
 In this work, we assume a two dimensional effect modifying parameter but
 changing its dimensionality affects only the rate of convergence.
\end_layout

\begin_layout Standard
For standard linear regression models, the least absolute shrinkage and
 selection operator (Lasso) is a penalized regression method that simultaneously
 selects variables for the regression model and shrinks the coefficient
 estimates toward zero 
\begin_inset CommandInset citation
LatexCommand citep
key "Tibshirani-1996"

\end_inset

.
 However, the Lasso can be inconsistent for variable selection and inefficient
 for coefficient estimation 
\begin_inset CommandInset citation
LatexCommand citep
key "Zou-2006"

\end_inset

.
 The adaptive Lasso (AL) is a refinement of the Lasso that produces consistent
 estimates of the coefficients and has been shown to have appealing properties
 for variable selection, which under suitable conditions include the 
\begin_inset Quotes eld
\end_inset

oracle
\begin_inset Quotes erd
\end_inset

 property of asymptotically including exactly the correct set of covariates
 and estimating their coefficients as well as if the correct covariates
 were known in advance 
\begin_inset CommandInset citation
LatexCommand citep
key "Zou-2006"

\end_inset

.
 For data where the obvserved variables fall into mutually exclusive groups
 that are known in advance, the adaptive group Lasso has similar oracle
 properties to the adaptive Lasso but does selection on groups rather than
 individual variables 
\begin_inset CommandInset citation
LatexCommand citep
key "Yuan-Lin-2006,Wang-Leng-2008"

\end_inset

.
 The main innovation of the proposed LAGR method is to use the adaptive
 group Lasso for local variable selection and coefficient estimation in
 a locally linear regression model, where each group consists of a single
 covariate and its interactions with location.
 Further, we extend the method to non-Gaussian responses.
\end_layout

\begin_layout Standard
We show that for both LAGR posesses the oracle properties of asymptotically
 selecting exactly the correct local covariates and estimating their local
 coefficients as accurately as would be possible if the identity of the
 nonzero coefficients for the local model were known in advance.
 The remainder of this document is organized as follows.
 The kernel-based estimation of a VCR model is described in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:vcr"

\end_inset

.
 The proposed LAGR technique and its oracle properties are presented in
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:lagr-gaussian"

\end_inset

.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simulations"

\end_inset

, the performance of the proposed LAGR technique is evaluated in a simulation
 study, and in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:example"

\end_inset

 the proposed method is applied to the Boston house price dataset.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:lagr-gllm"

\end_inset

, LAGR pis extended to varying coefficient generalized linear regression
 and the oracle properties for this setting are established.
 Technical proofs are left to the appendix.
\end_layout

\begin_layout Section
Varying Coefficient Regression
\begin_inset CommandInset label
LatexCommand label
name "sec:vcr"

\end_inset


\end_layout

\begin_layout Subsection
Varying Coefficient Model
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $n$
\end_inset

 data points, observed at sampling locations 
\begin_inset Formula $\bm{s}_{i}=(s_{i,1},\; s_{i,2})^{T}$
\end_inset

 for 
\begin_inset Formula $i=1,\dots,n$
\end_inset

, which are distributed in a domain 
\begin_inset Formula $\mathcal{D}\subset\mathbb{R}^{2}$
\end_inset

 according to a density 
\begin_inset Formula $f$
\end_inset

.
 For 
\begin_inset Formula $i=1,\dots,n$
\end_inset

, let 
\begin_inset Formula $y_{i}=y(\bm{s}_{i})$
\end_inset

 and 
\begin_inset Formula $\bm{x}_{i}=\bm{x}(\bm{s}_{i})$
\end_inset

 denote, respectively, the univariate response and the 
\begin_inset Formula $(p+1)$
\end_inset

-variate vector of covariates measured at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

.
 At each location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

, assume that the outcome is related to the covariates by a linear regression
 where the coefficients 
\begin_inset Formula $\bm{\beta}(\bm{s}_{i})$
\end_inset

 are functions in two dimensions and 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 is random error at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

.
 That is, 
\begin_inset Formula 
\begin{align}
y_{i}=\bm{x}_{i}^{T}\bm{\beta}_{i}(\bm{s}_{i})+\varepsilon_{i}.\label{eq:lm(s)}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Further assume that the error term 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 is normally distributed with zero mean and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

, and that 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

, 
\begin_inset Formula $i=1,\dots,n$
\end_inset

 are independent.
 That is, for 
\begin_inset Formula $\bm{\varepsilon}=\left(\varepsilon_{1},\dots,\varepsilon_{n}\right)^{T}$
\end_inset

, 
\begin_inset Formula $\bm{\varepsilon}\sim N\left(\bm{0},\sigma^{2}\mathrm{I}_{n}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
In the context of nonparametric regression, the boundary-effect bias can
 be reduced by local polynomial modeling, usually in the form of a locally
 linear model 
\begin_inset CommandInset citation
LatexCommand citep
key "Fan-Gijbels-1996"

\end_inset

.
 Here, to prepare for the estimation of locally linear coefficients, we
 augment the local design matrix with covariate-by-location interactions
 in two dimensions 
\begin_inset CommandInset citation
LatexCommand citep
key "Wang-2008b"

\end_inset

.
 Let 
\begin_inset Formula $\bm{X}=\left(\bm{X}_{1},\dots,\bm{X}_{n}\right)^{T}$
\end_inset

 be the design matrix of observed covariate values.
 Then the augmented local design matrix at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

 is defined to be 
\begin_inset Formula $\bm{Z}(\bm{s}_{i})=\left(\bm{X}\ \:\bm{L}_{i}\bm{X}\ \:\bm{M}_{i}\bm{X}\right),$
\end_inset

 where 
\begin_inset Formula $\bm{L}_{i}=\text{diag}\{s_{i',1}-s_{i,1}\}_{i'=1}^{n}$
\end_inset

 and 
\begin_inset Formula $\bm{M}_{i}=\text{diag}\{s_{i',2}-s_{i,2}\}_{i'=1}^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\bm{Z}_{i}=\left\{ \bm{Z}(\bm{s}_{i})\right\} _{i}$
\end_inset

 denote the 
\begin_inset Formula $i$
\end_inset

th row of the matrix 
\begin_inset Formula $\bm{Z}(\bm{s}_{i})$
\end_inset

 as a column vector.
 Let 
\begin_inset Formula $\bm{\zeta}(\bm{s}_{i})=\left(\bm{\beta}(\bm{s}_{i})^{T},\;\nabla_{u}\bm{\beta}(\bm{s}_{i})^{T},\;\nabla_{v}\bm{\beta}(\bm{s}_{i})^{T}\right)^{T}$
\end_inset

 denote the vector of local coefficients at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

, augmented with the local gradients of the coefficient surfaces in the
 two dimensions, denoted 
\begin_inset Formula $\nabla_{u}$
\end_inset

 and 
\begin_inset Formula $\nabla_{v}$
\end_inset

.
 Now we have 
\begin_inset Formula $Y_{i}=\bm{Z}_{i}^{T}\bm{\zeta}_{i}+\varepsilon_{i}$
\end_inset

.
\end_layout

\begin_layout Subsection
Coefficient Estimation via Local Likelihood
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\bm{\zeta}=\left(\bm{\zeta}(\bm{s}_{1}),\dots,\bm{\zeta}(\bm{s}_{n})\right)^{T}$
\end_inset

 denote a matrix of the local coefficients at all observation locations
 
\begin_inset Formula $\bm{s}_{1},\dots,\bm{s}_{n}$
\end_inset

.
 The total log-likelihood of the observed data is the sum of the log-likelihood
 of each individual observation: 
\begin_inset Formula 
\begin{align}
\ell\left(\bm{\zeta}\right)= & -(1/2)\sum_{i=1}^{n}\left[\log{\sigma^{2}}+\sigma^{-2}\left\{ y_{i}-\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i})\right\} ^{2}\right].\label{eq:coefficients}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Since there are a total of 
\begin_inset Formula $n\times3(p+1)+1$
\end_inset

 parameters for 
\begin_inset Formula $n$
\end_inset

 observations, the model is not identifiable and it is not possible to directly
 maximize the total likelihood.
 When the coefficient functions are smooth, though, the coefficients 
\begin_inset Formula $\bm{\zeta}(\bm{s})$
\end_inset

 at location 
\begin_inset Formula $\bm{s}$
\end_inset

 can be approximated by the coefficients 
\begin_inset Formula $\bm{\zeta}(\bm{t})$
\end_inset

 , where 
\begin_inset Formula $\bm{t}$
\end_inset

 is within some neighborhood of 
\begin_inset Formula $\bm{s}$
\end_inset

.
 This intuition is formalized by the local log-likelihood at location 
\begin_inset Formula $\bm{s}\in\mathcal{D}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\ell\left(\bm{\zeta}(\bm{s})\right)= & -(1/2)\sum_{i=1}^{n}K_{h}(\|\bm{s}-\bm{s}_{i}\|)\left[\log\sigma^{2}+\sigma^{-2}\left\{ y_{i}-\bm{z}_{i}^{T}\bm{\zeta}(\bm{s})\right\} ^{2}\right]\label{eq:local-log-likelihood}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $h$
\end_inset

 is a bandwidth parameter and the 
\begin_inset Formula $K_{h}(\|\bm{s}-\bm{s}_{i}\|)$
\end_inset

 for 
\begin_inset Formula $i=1,\dots,n$
\end_inset

 are local weights from a kernel function.
 For instance, the Epanechnikov kernel is defined as 
\begin_inset CommandInset citation
LatexCommand citep
key "Samiuddin-el-Sayyad-1990"

\end_inset

: 
\begin_inset Formula 
\begin{align}
K_{h}(\|\bm{s}_{i}-\bm{s}_{i'}\|) & =h^{-2}K\left(h^{-1}\|\bm{s}_{i}-\bm{s}_{i'}\|\right)\notag\label{eq:epanechnikov}\\
K(x) & =\begin{cases}
(3/4)(1-x^{2}) & \mbox{ if }x<1,\\
0 & \mbox{ if }x\geq1.
\end{cases}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
The local log-likelihood at 
\begin_inset Formula $\bm{s}$
\end_inset

 is maximized to obtain an estimate 
\begin_inset Formula $\tilde{\bm{\zeta}}(\bm{s})$
\end_inset

 of the local coefficients.
 Let 
\begin_inset Formula $\bm{W}\!(\bm{s})={\rm diag}\left\{ K_{h}(\|\bm{s}-\bm{s}_{i}\|)\right\} _{i'=1}^{n}$
\end_inset

 denote a diagonal matrix of kernel weights.
 The local likelihood can be maximized by minimizing a locally weighted
 least squares: 
\begin_inset Formula 
\begin{equation}
\mathcal{S}\left(\bm{\zeta}(\bm{s})\right)=(1/2)\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} ^{T}\bm{W}\!(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} ,^{T}\label{eq:local-sum-of-squares}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
and the minimizer is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\tilde{\bm{\zeta}}(\bm{s})=\left\{ \bm{Z}^{T}(\bm{s})\bm{W}\!(\bm{s})\bm{Z}(\bm{s})\right\} ^{-1}\bm{Z}^{T}(\bm{s})\bm{W}\!(\bm{s})\bm{Y}.\label{eq:zeta-hat}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
By Theorem 3 of 
\begin_inset CommandInset citation
LatexCommand cite
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

, for any given 
\begin_inset Formula $\bm{s}$
\end_inset

, the estimated local coefficients 
\begin_inset Formula $\tilde{\bm{\beta}}(\bm{s})=\left(\tilde{\zeta}_{1}(\bm{s})^{T},\dots,\tilde{\zeta}_{p}(\bm{s})^{T}\right)^{T}$
\end_inset

 converge in probability to 
\begin_inset Formula $\bm{\beta}(\bm{s})+2^{-1}\kappa_{0}^{-1}\kappa_{2}h^{2}\left\{ \nabla_{uu}^{2}\bm{\beta}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}(\bm{s})\right\} $
\end_inset

 are asymptotically normally distributed and the true 
\begin_inset Formula $\bm{\beta}(\bm{s})$
\end_inset

 at the optimal rate of 
\begin_inset Formula $O\left(n^{-1/3}\right)$
\end_inset

.
 The estimated local coefficients are asymptotically unbiased, with finite-sampl
e bias proportional to the second derivatives of the true coefficient functions.
\end_layout

\begin_layout Section
Local Variable Selection with LAGR
\begin_inset CommandInset label
LatexCommand label
name "sec:lagr-gaussian"

\end_inset


\end_layout

\begin_layout Subsection
LAGR-Penalized Local Likelihood
\end_layout

\begin_layout Standard
Estimating the local coefficients by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:zeta-hat"

\end_inset

 relies on 
\emph on
a priori
\emph default
 variable selection.
 Here we develop a new method of penalized regression to simultaneously
 select local covariates and estimate the local coefficients.
 For this purpose, each raw covariate is grouped with its covariate-by-location
 interactions.
 That is, 
\begin_inset Formula $\bm{\zeta}_{(j)}(\bm{s})=\left(\beta_{j}(\bm{s}),\;\;\nabla_{u}\beta_{j}(\bm{s}),\;\;\nabla_{v}\beta_{j}(\bm{s})\right)^{T}$
\end_inset

 for 
\begin_inset Formula $j=1,\dots,p$
\end_inset

.
 The proposed LAGR penalty is an adaptive 
\begin_inset Formula $\ell_{1}$
\end_inset

 penalty akin to the adaptive group Lasso 
\begin_inset CommandInset citation
LatexCommand citep
key "Yuan-Lin-2006,Wang-Leng-2008"

\end_inset

.
 By the mechanism of the adaptive group Lasso, variables within the same
 group are included in or dropped from the model together.
 The intercept group is left unpenalized.
\end_layout

\begin_layout Standard
To select and estimate the local coefficients at location 
\begin_inset Formula $\bm{s}$
\end_inset

, we minimize a penalized local sum of squares at location 
\begin_inset Formula $\bm{s}$
\end_inset

: 
\begin_inset Formula 
\begin{align*}
\mathcal{J}\left(\bm{\zeta}(\bm{s})\right) & =\mathcal{S}\left(\bm{\zeta}(\bm{s})\right)+\mathcal{P}\left(\bm{\zeta}(\bm{s})\right),
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathcal{S}\left(\bm{\zeta}\left(\bm{s}\right)\right)$
\end_inset

 is defined in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:local-sum-of-squares"

\end_inset

, 
\begin_inset Formula $\mathcal{P}\left(\bm{\zeta}(\bm{s})\right)=\sum_{j=1}^{p}\phi_{j}(\bm{s})\|\bm{\zeta}_{(j)}(\bm{s})\|$
\end_inset

 is a local adaptive grouped regularization (LAGR) penalty, and 
\begin_inset Formula $\|\cdot\|$
\end_inset

 is the 
\begin_inset Formula $L_{2}$
\end_inset

-norm.
 The LAGR penalty for the 
\begin_inset Formula $j$
\end_inset

th group of coefficients at location 
\begin_inset Formula $\bm{s}$
\end_inset

 is 
\begin_inset Formula $\phi_{j}(\bm{s})=\lambda_{n}\|\tilde{\bm{\zeta}}_{(j)}(\bm{s})\|^{-\gamma}$
\end_inset

, where 
\begin_inset Formula $\lambda_{n}>0$
\end_inset

 is a local tuning parameter applied to all coefficients at location 
\begin_inset Formula $\bm{s}$
\end_inset

 and 
\begin_inset Formula $\tilde{\bm{\zeta}}_{(j)}(\bm{s})$
\end_inset

 is a subset of the vector of unpenalized local coefficients from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:zeta-hat"

\end_inset

.
\end_layout

\begin_layout Subsection
Oracle Properties
\begin_inset CommandInset label
LatexCommand label
name "sub:oracle-properties"

\end_inset


\end_layout

\begin_layout Standard
For a local model at location 
\begin_inset Formula $\bm{s}$
\end_inset

, we let 
\begin_inset Formula $a_{n}=\max\left\{ \phi_{j}(\bm{s}),j\le p_{0}\right\} $
\end_inset

 be the largest penalty applied to a covariate group whose true coefficient
 norm is nonzero and 
\begin_inset Formula $b_{n}=\min\left\{ \phi_{j}(\bm{s}),j>p_{0}\right\} $
\end_inset

 be the smallest penalty applied to a covariate group whose true coefficient
 norm is zero.
 Let 
\begin_inset Formula $\bm{Z}_{(k)}(\bm{s})$
\end_inset

 be the augmented design matrix for covariate group 
\begin_inset Formula $k$
\end_inset

, and let 
\begin_inset Formula $\bm{Z}_{(\mhyphen k)}(\bm{s})$
\end_inset

 be the augmented design matrix for all the data except covariate group
 
\begin_inset Formula $k$
\end_inset

.
 Similarly, let 
\begin_inset Formula $\bm{\zeta}_{(k)}(\bm{s})$
\end_inset

 be the augmented coefficients for covariate group 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $\bm{\zeta}_{(\mhyphen k)}(\bm{s})$
\end_inset

 be the augmented coefficients for all covariate groups except 
\begin_inset Formula $k$
\end_inset

.
 Let 
\begin_inset Formula $\nabla\zeta_{k}(\bm{s})=\left(\nabla_{u}\zeta_{k}(\bm{s}),\nabla_{v}\zeta_{k}(\bm{s})\right)^{T}$
\end_inset

 and 
\begin_inset Formula $\nabla^{2}\zeta_{j}(\bm{s})=\left(\begin{array}{cc}
\nabla_{uu}^{2}\zeta_{k}(\bm{s}) & \nabla_{uv}^{2}\zeta_{k}(\bm{s})\\
\nabla_{vu}^{2}\zeta_{k}(\bm{s}) & \nabla_{vv}^{2}\zeta_{k}(\bm{s})
\end{array}\right)$
\end_inset

.
 Let 
\begin_inset Formula $\kappa_{0}=\int_{R^{2}}K(\|\bm{s}\|)ds$
\end_inset

, 
\begin_inset Formula $\kappa_{2}=\int_{R^{2}}[(1,0)\bm{s}]^{2}K(\|\bm{s}\|)ds=\int_{R^{2}}[(0,1)\bm{s}]^{2}K(\|\bm{s}\|)ds$
\end_inset

, and 
\begin_inset Formula $\nu_{0}=\int_{R^{2}}K^{2}(\|\bm{s}\|)ds$
\end_inset

.
 Finally, let 
\begin_inset Formula $\xrightarrow{p}$
\end_inset

 and 
\begin_inset Formula $\xrightarrow{d}$
\end_inset

 denote convergence in probability and distribution, respectively, as 
\begin_inset Formula $n\to\infty$
\end_inset

.
\end_layout

\begin_layout Standard
Assume the following conditions.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.1)
\end_layout

\end_inset

The kernel function 
\begin_inset Formula $K(\cdot)$
\end_inset

 is bounded, positive, symmetric, and Lipschitz continuous on 
\begin_inset Formula $\mathbb{R}$
\end_inset

, and has bounded support.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.2)
\end_layout

\end_inset

There are 
\begin_inset Formula $p_{0}<p$
\end_inset

 covariates 
\begin_inset Formula $\bm{X}_{(a)}(\bm{s})$
\end_inset

 with nonzero local regression coefficients, denoted 
\begin_inset Formula $\bm{\beta}_{(a)}(\bm{s})\ne\bm{0}$
\end_inset

.
 Without loss of generality, assume these are covariates 
\begin_inset Formula $1,\dots,p_{0}$
\end_inset

.
 The remaining 
\begin_inset Formula $p-p_{0}$
\end_inset

 covariates 
\begin_inset Formula $\bm{X}_{(b)}(\bm{s})$
\end_inset

 have true coefficients equal to zero, denoted 
\begin_inset Formula $\bm{\beta}_{(b)}(\bm{s})=\bm{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.3)
\end_layout

\end_inset


\begin_inset Formula $\bm{X}(\bm{s}_{1}),\dots,\bm{X}(\bm{s}_{n})$
\end_inset

 are independent random vectors that are independent of 
\begin_inset Formula $\varepsilon_{1},\dots,\varepsilon_{n}$
\end_inset

.
 Also 
\begin_inset Formula $\Psi(\bm{s})=E\left\{ \bm{X}(\bm{s})\bm{X}(\bm{s})^{T}|\bm{s}\right\} $
\end_inset

 and 
\begin_inset Formula $\Psi_{(a)}(\bm{s})=E\left\{ \bm{X}_{(a)}(\bm{s})\bm{X}_{(a)}(\bm{s})^{T}|\bm{s}\right\} $
\end_inset

 are positive-definite and differentiable at location 
\begin_inset Formula $\bm{s}$
\end_inset

, 
\begin_inset Formula $E\left|\bm{X}(\bm{s})\right|^{2q}<\infty$
\end_inset

, and 
\begin_inset Formula $E\left|\varepsilon(\bm{s})\right|^{2q}<\infty$
\end_inset

 for some 
\begin_inset Formula $q>2$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.4)
\end_layout

\end_inset

The coefficient functions 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\beta_{j}(\cdot)$
\end_inset

, 
\begin_inset Formula $j=1,\dots,p$
\end_inset

 have continuous second partial derivatives at 
\begin_inset Formula $\bm{s}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.6)
\end_layout

\end_inset

The function 
\begin_inset Formula $f(\bm{s})$
\end_inset

 is differentiable at 
\begin_inset Formula $\bm{s}$
\end_inset

 and 
\begin_inset Formula $f(\bm{s})>0$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.6)
\end_layout

\end_inset


\begin_inset Formula $E\left\{ \left|\bm{X}(\bm{s})\right|^{3}|\bm{s}\right\} $
\end_inset

 is continuous as location 
\begin_inset Formula $\bm{s}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.7)
\end_layout

\end_inset


\begin_inset Formula $E\left\{ Y(\bm{s})^{4}|\bm{X}(\bm{s}),\bm{s}\right\} $
\end_inset

 is bounded as location 
\begin_inset Formula $\bm{s}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.8)
\end_layout

\end_inset


\begin_inset Formula $h=O\left(n^{-1/6}\right)$
\end_inset


\end_layout

\begin_layout Standard
These conditions are generally equivalent to those of 
\begin_inset CommandInset citation
LatexCommand citet
key "Cai-Fan-Li-2000"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

.
 The covariates 
\begin_inset Formula $\bm{X}(\bm{s}_{1}),\dots,\bm{X}(\bm{s}_{n})$
\end_inset

 were assumed to be 
\begin_inset Formula $iid$
\end_inset

 in 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

, which is not required here.
 The existence of of 
\begin_inset Formula $\Psi(\cdot)$
\end_inset

 is necessary for the existence of the limiting distribution of 
\begin_inset Formula $\hat{\bm{\beta}}(\bm{s})$
\end_inset

; its differentiability and that of 
\begin_inset Formula $f(\cdot)$
\end_inset

 are necessary in order to approximate those functions by Taylor expansion.
 The condition on moments of 
\begin_inset Formula $Y$
\end_inset

 is used when bounding the remainder term from Taylor expansion.
 
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "theorem:normality"

\end_inset

 
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Asymptotic normality
\end_layout

\end_inset

 Under (A.1)-(A.8), if 
\begin_inset Formula $h^{-1}n^{-1/2}a_{n}\xrightarrow{p}0$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

, then
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{gather*}
\left\{ f(\bm{s})h^{2}n\right\} ^{1/2}\left[\hat{\bm{\beta}}_{(a)}(\bm{s})-\bm{\beta}_{(a)}(\bm{s})-2^{-1}\kappa_{0}^{-1}\kappa_{2}h^{2}\left\{ \nabla_{uu}^{2}\bm{\beta}_{(a)}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{(a)}(\bm{s})\right\} \right]\\
\xrightarrow{d}N\left(0,\kappa_{0}^{-2}\nu_{0}\sigma^{2}\Psi_{(a)}(\bm{s})^{-1}\right)
\end{gather*}

\end_inset


\end_layout

\begin_layout Theorem
where 
\begin_inset Formula $\left\{ \nabla_{uu}^{2}\bm{\beta}_{(a)}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{(a)}(\bm{s})\right\} =\left(\nabla_{uu}^{2}\bm{\beta}_{1}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{1}(\bm{s}),\dots,\nabla_{uu}^{2}\bm{\beta}_{p_{o}}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{p_{o}}(\bm{s})\right)^{T}$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "theorem:selection"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Selection consistency
\end_layout

\end_inset

 Under (A.1)-(A.8), if 
\begin_inset Formula $h^{-1}n^{-1/2}a_{n}\xrightarrow{p}0$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

, then 
\begin_inset Formula 
\[
P\left\{ \|\hat{\bm{\zeta}}_{(j)}(\bm{s})\|=\bm{0}\right\} \to0\text{ if }j\le p_{0},\text{ and }P\left\{ \|\hat{\bm{\zeta}}_{(j)}(\bm{s})\|=\bm{0}\right\} \to1\text{ if }j>p_{0}.
\]

\end_inset


\end_layout

\begin_layout Standard
Together, Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset

 and Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:selection"

\end_inset

 indicate that the LAGR estimates have the same asymptotic distribution
 as a local regression model where the true nonzero coefficients are known
 in advance 
\begin_inset CommandInset citation
LatexCommand citep
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

, and that the LAGR estimates of true zero coefficients tend to zero with
 probability one.
 Thus, selection and estimation by LAGR has the oracle property.
 To establish the oracle properties of LAGR, we assumed that 
\begin_inset Formula $h^{-1}n^{-1/2}a_{n}\xrightarrow{p}0$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

.
 Therefore, 
\begin_inset Formula $h^{-1}n^{-1/2}\lambda_{n}\to0$
\end_inset

 for 
\begin_inset Formula $j\le p_{0}$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}\lambda_{n}\|\bm{\zeta}_{(j)}(\bm{s})\|^{-\gamma}\to\infty$
\end_inset

 for 
\begin_inset Formula $j>p_{0}$
\end_inset

.
 We require that 
\begin_inset Formula $\lambda_{n}$
\end_inset

 satisfy both assumptions.
 Suppose 
\begin_inset Formula $\lambda_{n}=n^{\alpha}$
\end_inset

.
 Since 
\begin_inset Formula $h=O\left(n^{-1/6}\right)$
\end_inset

 and 
\begin_inset Formula $\|\tilde{\bm{\zeta}}_{(p)}(\bm{s})\|=O\left(h^{-1}n^{-1/2}\right)$
\end_inset

, it follows that 
\begin_inset Formula $h^{-1}n^{-1/2}\lambda_{n}=O\left(n^{-1/3+\alpha}\right)$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}\lambda_{n}\|\tilde{\bm{\zeta}}_{(p)}(\bm{s})\|^{-\gamma}=O\left(n^{-2/3+\alpha+\gamma/3}\right)$
\end_inset

.
 Thus, 
\begin_inset Formula $\left(2-\gamma\right)/3<\alpha<1/3$
\end_inset

, which can only be satisfied for 
\begin_inset Formula $\gamma>1$
\end_inset

.
\end_layout

\begin_layout Subsection
Tuning Parameter Selection
\end_layout

\begin_layout Standard
In practical application, it is necessary to select the LAGR tuning parameter
 
\begin_inset Formula $\lambda_{n}$
\end_inset

 for each local model.
 A popular approach in other Lasso-type problems is to select the tuning
 parameter that maximizes a criterion that approximates the expected log-likelih
ood of a new, independent data set drawn from the same distribution.
 This is the framework of Mallows' Cp, Stein's unbiased risk estimate (SURE)
 and Akaike's information criterion (AIC) 
\begin_inset CommandInset citation
LatexCommand citep
key "Mallows-1973,Stein-1981,Akaike-1973"

\end_inset

.
\end_layout

\begin_layout Standard
These criteria use a so-called covariance penalty to estimate the bias due
 to using the same data set to select a model and to estimate its parameters
 
\begin_inset CommandInset citation
LatexCommand citep
key "Efron:2004a"

\end_inset

.
 We adopt the approximate degrees of freedom for the adaptive group Lasso
 from 
\begin_inset CommandInset citation
LatexCommand citet
key "Yuan-Lin-2006"

\end_inset

 and minimize the AICc to select the tuning parameter 
\begin_inset Formula $\lambda_{n}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "Hurvich-1998"

\end_inset

.
 That is, let
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\hat{df}(\lambda_{n};\bm{s})= & \sum_{j=1}^{p}I\left(\|\hat{\bm{\zeta}}(\lambda_{n};\bm{s})\|>0\right)+\sum_{j=1}^{p}\|\hat{\bm{\zeta}}(\lambda_{n};\bm{s})\|\|\tilde{\bm{\zeta}}(\bm{s})\|^{-1}(p_{j}-1)\\
\text{AIC}_{c}(\lambda_{n};\bm{s})= & \sum_{i=1}^{n}K_{h}(\|\bm{s}-\bm{s}_{i}\|)\sigma^{-2}\left\{ y_{i}-\bm{z}_{i}^{T}\hat{\bm{\zeta}}(\lambda_{n};\bm{s})\right\} ^{2}+2\hat{df}(\lambda_{n};\bm{s})\\
 & +2\hat{df}(\lambda_{n};\bm{s})\left\{ \hat{df}(\lambda_{n};\bm{s})+1\right\} \left\{ \sum_{i=1}^{n}K_{h}(\|\bm{s}-\bm{s}_{i}\|)-\hat{df}(\lambda_{n};\bm{s})-1\right\} ^{-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $I\left(\cdot\right)$
\end_inset

 is the indicator function and the local coefficient estimate is written
 
\begin_inset Formula $\hat{\bm{\zeta}}(\lambda_{n};\bm{s})$
\end_inset

 to emphasize that it depends on the tuning parameter.
\end_layout

\begin_layout Section
Simulation Study
\begin_inset CommandInset label
LatexCommand label
name "sec:simulations"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<simulation-imports, message=FALSE, warning=FALSE, echo=FALSE>>=
\end_layout

\begin_layout Plain Layout

	#Imports:
\end_layout

\begin_layout Plain Layout

	require(xtable)
\end_layout

\begin_layout Plain Layout

	require(dplyr)
\end_layout

\begin_layout Plain Layout

	require(brooks)
\end_layout

\begin_layout Plain Layout

	load("~/scratch/gwr-sim-output.RData")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Simulation Setup
\end_layout

\begin_layout Standard
A simulation study was conducted to assess the performance of the method
 described in Sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:vcr"

\end_inset

--
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:lagr-gaussian"

\end_inset

.
 Data were simulated on the domain 
\begin_inset Formula $[0,1]^{2}$
\end_inset

, which was divided into a 
\begin_inset Formula $30\times30$
\end_inset

 grid.
 Each of 
\begin_inset Formula $p=5$
\end_inset

 covariates 
\begin_inset Formula $X_{1},\dots,X_{5}$
\end_inset

 was simulated by a Gaussian random field with mean zero and exponential
 covariance function 
\begin_inset Formula $\text{Cov}\left(X_{ij},X_{i'j}\right)=\sigma_{x}^{2}\exp\left(-\tau_{x}^{-1}\delta_{ii'}\right)$
\end_inset

 where 
\begin_inset Formula $\sigma_{x}^{2}=1$
\end_inset

 is the variance, 
\begin_inset Formula $\tau_{x}=0.1$
\end_inset

 is the range parameter, and 
\begin_inset Formula $\delta_{ii'}=\|\bm{s}_{i}-\bm{s}_{i'}\|_{2}$
\end_inset

 is the Euclidean distance between locations 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

 and 
\begin_inset Formula $\bm{s}_{i'}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Correlation was induced between the covariates by multiplying the design
 matrix 
\begin_inset Formula $\bm{X}$
\end_inset

 by 
\begin_inset Formula $\bm{R}$
\end_inset

, where 
\begin_inset Formula $\bm{R}$
\end_inset

 is the Cholesky decomposition of the covariance matrix 
\begin_inset Formula $\bm{\Sigma}=\bm{R}'\bm{R}$
\end_inset

.
 The covariance matrix 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 is a 
\begin_inset Formula $5\times5$
\end_inset

 matrix that has ones on the diagonal and 
\begin_inset Formula $\rho$
\end_inset

 for all off-diagonal entries, where 
\begin_inset Formula $\rho$
\end_inset

 is the between-covariate correlation.
 
\end_layout

\begin_layout Standard
The simulated response was 
\begin_inset Formula $y_{i}=\bm{x}_{i}^{T}\bm{\beta}(\bm{s}_{i})+\varepsilon_{i}$
\end_inset

 for 
\begin_inset Formula $i=1,\dots,n$
\end_inset

 where 
\begin_inset Formula $n=900$
\end_inset

 and the 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

's were iid Gaussian with mean zero and variance 
\begin_inset Formula $\sigma_{\varepsilon}^{2}$
\end_inset

.
 The simulated data included the response 
\begin_inset Formula $y$
\end_inset

 and five covariates 
\begin_inset Formula $x_{1},\dots,x_{5}$
\end_inset

.
 The true data-generating model uses only 
\begin_inset Formula $x_{1}$
\end_inset

.
 The variables 
\begin_inset Formula $x_{2},\dots,x_{5}$
\end_inset

 are included to assess performance in model selection.
 
\end_layout

\begin_layout Standard
Three different functions were used for the coefficient surface 
\begin_inset Formula $\beta_{1}(\bm{s})$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:simulation-coefficient-functions"

\end_inset

).
 The first is the 
\begin_inset Quotes eld
\end_inset

step
\begin_inset Quotes erd
\end_inset

 function 
\begin_inset Formula $\mathbf{\beta_{step}(\bm{s})=}1$
\end_inset

 if 
\begin_inset Formula $s_{x}>0.6$
\end_inset

, 
\begin_inset Formula $5s_{x}-2$
\end_inset

 if 
\begin_inset Formula $0.4<s_{x}\le0.6$
\end_inset

, and 
\begin_inset Formula $0$
\end_inset

 otherwise.
 The second is the gradient function, 
\begin_inset Formula $\beta_{gradient}(\bm{s})=s_{x}$
\end_inset

, and the third is the parabola 
\begin_inset Formula $\beta_{parabola}(\bm{s})=1-2\left\{ (s_{x}-0.5)^{2}+(s_{y}-0.5)^{2}\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename ../../figures/simulation/step.pdf
	width 33text%

\end_inset


\begin_inset Graphics
	filename ../../figures/simulation/gradient.pdf
	width 33text%

\end_inset


\begin_inset Graphics
	filename ../../figures/simulation/parabola.pdf
	width 33text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The step function (left), gradient function (middle), and parabola function
 (right) that were used as the coefficient function 
\begin_inset Formula $\beta_{1}(\bm{s})$
\end_inset

 in the VCR model 
\begin_inset Formula $y_{i}=x_{1}(\bm{s}_{i})\beta_{1}(\bm{s}_{i})+\varepsilon_{i}$
\end_inset

 when generating the data for the simulation study.
\begin_inset CommandInset label
LatexCommand label
name "fig:simulation-coefficient-functions"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In total, three parameters were varied to produce 18 settings, each of which
 was simulated 100 times.
 There were the three functional forms for the coefficient surface 
\begin_inset Formula $\beta_{1}(\bm{s})$
\end_inset

; data was simulated both with low (
\begin_inset Formula $\rho=0$
\end_inset

), medium (
\begin_inset Formula $\rho=0.5$
\end_inset

), and high (
\begin_inset Formula $\rho=0.9$
\end_inset

) correlation between the covariates; and simulations were made with low
 (
\begin_inset Formula $\sigma_{\varepsilon}=0.5$
\end_inset

) and high (
\begin_inset Formula $\sigma_{\varepsilon}=1$
\end_inset

) variance for the random error term.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<prefix, echo=FALSE, results='asis', message=FALSE>>=
\end_layout

\begin_layout Plain Layout

	sim.prefix <- list()
\end_layout

\begin_layout Plain Layout

	sim.prefix$pos <- sapply(0:17, function(x) return(x), simplify=FALSE)
\end_layout

\begin_layout Plain Layout

	sim.prefix$command <- c("
\backslash

\backslash
multirow{6}{*}{step} & 
\backslash

\backslash
multirow{2}{*}{0} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ",
\end_layout

\begin_layout Plain Layout

		" & 
\backslash

\backslash
multirow{2}{*}{0.5} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ",
\end_layout

\begin_layout Plain Layout

		" & 
\backslash

\backslash
multirow{2}{*}{0.9} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ",
\end_layout

\begin_layout Plain Layout

		"
\backslash

\backslash
hline 
\backslash

\backslash
multirow{6}{*}{gradient} & 
\backslash

\backslash
multirow{2}{*}{0} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ",
\end_layout

\begin_layout Plain Layout

		" & 
\backslash

\backslash
multirow{2}{*}{0.5} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ",
\end_layout

\begin_layout Plain Layout

		" & 
\backslash

\backslash
multirow{2}{*}{0.9} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ",
\end_layout

\begin_layout Plain Layout

		"
\backslash

\backslash
hline 
\backslash

\backslash
multirow{6}{*}{parabola} & 
\backslash

\backslash
multirow{2}{*}{0} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ",
\end_layout

\begin_layout Plain Layout

		" & 
\backslash

\backslash
multirow{2}{*}{0.5} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ",
\end_layout

\begin_layout Plain Layout

		" & 
\backslash

\backslash
multirow{2}{*}{0.9} & 0.5 & ",
\end_layout

\begin_layout Plain Layout

		" &  & 1.0 & ")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The results are presented in terms of the mean integrated squared error
 (MISE) of the coefficient surface estimates 
\begin_inset Formula $\hat{\beta}_{1}(\bm{s}),\dots,\hat{\beta}_{5}(\bm{s})$
\end_inset

, the MISE of the fitted response 
\begin_inset Formula $\hat{y}(\bm{s})$
\end_inset

, and the frequency with which the coefficient surface estimates 
\begin_inset Formula $\hat{\beta}_{2}(\bm{s}),\dots,\hat{\beta}_{5}(\bm{s})$
\end_inset

 estimated by LAGR were zero.
 The performance of LAGR was compared to that of a VCR model without variable
 selection, and to a VCR model with oracle selection.
 Oracle selection means that exactly the correct set of covariates was used
 to fit each local model.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}
\end_layout

\begin_layout Plain Layout

	
\backslash
centering
\end_layout

\begin_layout Plain Layout

	
\backslash
begin{tabular}{|ccc|ccc|cc|}
\end_layout

\begin_layout Plain Layout

		
\backslash
hline
\end_layout

\begin_layout Plain Layout

		
\backslash
multicolumn{3}{|c|}{
\backslash
begin{tabular}[c]{@{}c@{}}Simulation
\backslash

\backslash
settings
\backslash
end{tabular}} & 
\backslash
multicolumn{3}{|c|}{
\backslash
begin{tabular}[c]{@{}c@{}}MISE
\backslash

\backslash
$
\backslash
hat{
\backslash
beta}_1$
\backslash
end{tabular}} & 
\backslash
multicolumn{2}{|c|}{
\backslash
begin{tabular}[c]{@{}c@{}}MISE
\backslash

\backslash
$
\backslash
hat{
\backslash
beta}_2, 
\backslash
dots, 
\backslash
hat{
\backslash
beta}_5$
\backslash
end{tabular}} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

		$
\backslash
beta_{1}(
\backslash
bm{s})$ & $
\backslash
rho$ & $
\backslash
sigma_{
\backslash
varepsilon}$ & LAGR & VCR & Oracle & LAGR & VCR 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

		
\backslash
hline 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<MISE-X1, echo=FALSE, results='asis', message=FALSE, warning=FALSE>>=
\end_layout

\begin_layout Plain Layout

#Calculate the MISE for the first coefficient:
\end_layout

\begin_layout Plain Layout

mise = matrix(0, nrow=0, ncol=3)
\end_layout

\begin_layout Plain Layout

for (setting in 1:18) {
\end_layout

\begin_layout Plain Layout

    #Get the proper coefficient surface
\end_layout

\begin_layout Plain Layout

    if (setting < 7) {B1 = step}
\end_layout

\begin_layout Plain Layout

    else if (setting < 13) {B1 = gradient}
\end_layout

\begin_layout Plain Layout

    else {B1 = parabola}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    #compute the MISE
\end_layout

\begin_layout Plain Layout

    row = sapply(c('lagr', 'gwr', 'oracle'),
\end_layout

\begin_layout Plain Layout

                  function(selection.method) {
\end_layout

\begin_layout Plain Layout

                     mean(sweep(output[[setting]][[selection.method]][['X1']],
 1, as.vector(B1))**2)                  			}
\end_layout

\begin_layout Plain Layout

	)
\end_layout

\begin_layout Plain Layout

    mise = rbind(mise, row)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#Which entries should be bolded (for having the lowest MISE)?
\end_layout

\begin_layout Plain Layout

bold = matrix(FALSE, nrow=nrow(mise), ncol=ncol(mise))
\end_layout

\begin_layout Plain Layout

for (i in 1:nrow(mise)){
\end_layout

\begin_layout Plain Layout

    bold[i,which.min(mise[i,])] = TRUE
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#Which entries should be italicised (for having the second-lowest MISE)?
\end_layout

\begin_layout Plain Layout

ital = matrix(FALSE, nrow=nrow(mise), ncol=ncol(mise))
\end_layout

\begin_layout Plain Layout

for (i in 1:nrow(mise)){
\end_layout

\begin_layout Plain Layout

    ital[i,order(mise[i,])[2]] = TRUE
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#Calculate the MISE for the second through fifth coefficients:
\end_layout

\begin_layout Plain Layout

mise.x2.x5 = matrix(0, nrow=0, ncol=2)
\end_layout

\begin_layout Plain Layout

for (setting in 1:18) {
\end_layout

\begin_layout Plain Layout

    #Get the proper coefficient surface
\end_layout

\begin_layout Plain Layout

    if (setting < 7) {B1 = step}
\end_layout

\begin_layout Plain Layout

    else if (setting < 13) {B1 = gradient}
\end_layout

\begin_layout Plain Layout

    else {B1 = parabola}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    #compute the MISE
\end_layout

\begin_layout Plain Layout

    row = sapply(c('lagr', 'gwr'),
\end_layout

\begin_layout Plain Layout

		function(selection.method) {
\end_layout

\begin_layout Plain Layout

			mean(sapply(c('X2','X3','X4','X5'), function(vv) {
\end_layout

\begin_layout Plain Layout

				output[[setting]][[selection.method]][[vv]]**2
\end_layout

\begin_layout Plain Layout

			}))
\end_layout

\begin_layout Plain Layout

		}
\end_layout

\begin_layout Plain Layout

	)
\end_layout

\begin_layout Plain Layout

    mise.x2.x5 = rbind(mise.x2.x5, row)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#Which entries should be bolded (for having the lowest MISE)?
\end_layout

\begin_layout Plain Layout

bold.x2.x5 = matrix(FALSE, nrow=nrow(mise.x2.x5), ncol=ncol(mise.x2.x5))
\end_layout

\begin_layout Plain Layout

for (i in 1:nrow(mise.x2.x5)){
\end_layout

\begin_layout Plain Layout

    bold.x2.x5[i,which.min(mise.x2.x5[i,])] = TRUE
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#Put names on the table
\end_layout

\begin_layout Plain Layout

rownames(mise) = NULL
\end_layout

\begin_layout Plain Layout

colnames(mise) = c("LAGR", "VCR", "Oracle")
\end_layout

\begin_layout Plain Layout

colnames(mise.x2.x5) = c("LAGR", "VCR")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#Generate the table, caption it, and print it with emphasis.
\end_layout

\begin_layout Plain Layout

mise.table = xtable(cbind(mise, mise.x2.x5))
\end_layout

\begin_layout Plain Layout

caption(mise.table) = "The MISE for the estimates of $
\backslash

\backslash
beta_1(
\backslash

\backslash
bm{s})$ in each simulation setting, under variable selection via LAGR, no
 variable selection, and oracular variable selection.
 Highlighting indicates the 
\backslash

\backslash
textbf{lowest} and 
\backslash

\backslash
emph{next-lowest} MISE."
\end_layout

\begin_layout Plain Layout

label(mise.table) = "tab:x1-mise"
\end_layout

\begin_layout Plain Layout

xtable.printbold(mise.table,
\end_layout

\begin_layout Plain Layout

	table.placement=NULL,
\end_layout

\begin_layout Plain Layout

	which.bold=cbind(bold, bold.x2.x5),
\end_layout

\begin_layout Plain Layout

	which.ital=cbind(ital, matrix(FALSE, nrow=18, ncol=2)),
\end_layout

\begin_layout Plain Layout

	hline.after=NULL,
\end_layout

\begin_layout Plain Layout

	only.contents=TRUE,
\end_layout

\begin_layout Plain Layout

	include.rownames=FALSE,
\end_layout

\begin_layout Plain Layout

	include.colnames=FALSE,
\end_layout

\begin_layout Plain Layout

	add.to.row=sim.prefix)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

	
\backslash
hline
\end_layout

\begin_layout Plain Layout

	
\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout

	
\backslash
caption{For each setting in the simulation study, the mean integrated squared
 error (MISE) of the coefficient estimates.
 First, the MISE of $
\backslash
hat{
\backslash
beta}_1$ from estimation by local adaptive grouped regularization (LAGR),
 by locally linear regression without selection (VCR), and by locally linear
 regression with oracular selection (Oracle).
 Here, highlighting indicates the 
\backslash
textbf{smallest} and 
\backslash
emph{next-smallest} MISE for $
\backslash
hat{
\backslash
beta}_1$.
 Also, the MISE of $
\backslash
hat{
\backslash
beta}_2,
\backslash
dots,
\backslash
hat{
\backslash
beta}_5$ from estimation by LAGR and VCR.
 Here, highlighting indicates the 
\backslash
textbf{smallest} MISE.}
\end_layout

\begin_layout Plain Layout

	
\backslash
label{tab:mise}
\end_layout

\begin_layout Plain Layout


\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Simulation Results
\end_layout

\begin_layout Standard
The MISE of the estimates of 
\begin_inset Formula $\beta_{1}(\bm{s})$
\end_inset

 are in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:misex"

\end_inset

.
 Recall that 
\begin_inset Formula $\beta_{2}(\bm{s}),\dots,\beta_{5}(\bm{s})$
\end_inset

 are exactly zero across the entire domain.
 Oracle selection will estimate these coefficients perfectly, so we focus
 on the comparison between estimation by LAGR and by the VCR model with
 no selection.
 These results show that for every simulation setting, LAGR estimation is
 more accurate than the standard VCR model.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<PZERO, echo=FALSE, results='asis', message=FALSE>>=
\end_layout

\begin_layout Plain Layout

load("~/scratch/gwr-sim-output.RData")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

zz = vector()
\end_layout

\begin_layout Plain Layout

for (i in 1:18) {
\end_layout

\begin_layout Plain Layout

    zz = c(zz, mean(sapply(pzero, function(x) x[i])))
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

zz = matrix(zz)
\end_layout

\begin_layout Plain Layout

colnames(zz) = c("Frequency of exact zero")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
From Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:misey"

\end_inset

 we see that LAGR has good ability to identify zero-coefficient covariates.
 The frequency with which 
\begin_inset Formula $\beta_{2}(\bm{s}),\dots,\beta_{5}(\bm{s})$
\end_inset

 were dropped from the LAGR models ranged from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(min(zz),2)}
\end_layout

\end_inset

 to 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(max(zz),2)}
\end_layout

\end_inset

.
 The MISE of the fitted 
\begin_inset Formula $\hat{y}(\bm{s})$
\end_inset

 is listed in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:misey"

\end_inset

, where the highlighting is based on which methods estimate an error variance
 that is closest to the known truth for the simulation.
 The results are all very similar to each other, indicating that no method
 was consistently better than the others in this simulation at fitting the
 model output.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}
\end_layout

\begin_layout Plain Layout

	
\backslash
centering
\end_layout

\begin_layout Plain Layout

	
\backslash
begin{tabular}{|ccc|c|ccc|}
\end_layout

\begin_layout Plain Layout

		
\backslash
hline
\end_layout

\begin_layout Plain Layout

		
\backslash
multicolumn{3}{|c|}{
\backslash
begin{tabular}[c]{@{}c@{}}Simulation
\backslash

\backslash
settings
\backslash
end{tabular}} &  
\backslash
multicolumn{1}{|c|}{
\backslash
begin{tabular}[c]{@{}c@{}}Zero
\backslash

\backslash
frequency
\backslash
end{tabular}} &  
\backslash
multicolumn{3}{|c|}{
\backslash
begin{tabular}[c]{@{}c@{}}MISE
\backslash

\backslash
$
\backslash
hat{y}$
\backslash
end{tabular}} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

		$
\backslash
beta_{1}(
\backslash
bm{s})$ & $
\backslash
rho$ & $
\backslash
sigma_{
\backslash
varepsilon}$ & $
\backslash
hat{
\backslash
beta}_2,
\backslash
dots,
\backslash
hat{
\backslash
beta}_5$ & LAGR & VCR & Oracle 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

		
\backslash
hline 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<MISEY, echo=FALSE, results='asis', message=FALSE>>=
\end_layout

\begin_layout Plain Layout

	std = matrix(rbind(c(0.25, 0.25, 0.25), c(1, 1, 1)), nrow=18, ncol=3)
\end_layout

\begin_layout Plain Layout

	bold.misey = matrix(FALSE, nrow=nrow(mise), ncol=ncol(mise))
\end_layout

\begin_layout Plain Layout

	for (i in 1:nrow(mise)){
\end_layout

\begin_layout Plain Layout

		bold.misey[i,which.min(abs(misey[i,] - std[i,]))] = TRUE
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	#Which entries should be italicised (for having the second-lowest MISE)?
\end_layout

\begin_layout Plain Layout

	ital.misey = matrix(FALSE, nrow=nrow(mise), ncol=ncol(mise))
\end_layout

\begin_layout Plain Layout

	for (i in 1:nrow(mise)){
\end_layout

\begin_layout Plain Layout

		ital.misey[i,order(abs(misey[i,] - std[i,]))[2]] = TRUE
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	#Put names on the table
\end_layout

\begin_layout Plain Layout

	rownames(misey) = NULL
\end_layout

\begin_layout Plain Layout

	colnames(misey) = c("LAGR", "VCR", "Oracle")
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	#Generate the table, caption it, and print it with emphasis.
\end_layout

\begin_layout Plain Layout

	misey.table = xtable(cbind(zz, misey))
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	xtable.printbold(misey.table,
\end_layout

\begin_layout Plain Layout

		table.placement=NULL,
\end_layout

\begin_layout Plain Layout

		which.bold=cbind(rep(FALSE, 18), bold.misey),
\end_layout

\begin_layout Plain Layout

		which.ital=cbind(rep(FALSE, 18), ital.misey),
\end_layout

\begin_layout Plain Layout

		hline.after=NULL,
\end_layout

\begin_layout Plain Layout

		only.contents=TRUE,
\end_layout

\begin_layout Plain Layout

		include.rownames=FALSE,
\end_layout

\begin_layout Plain Layout

		include.colnames=FALSE,
\end_layout

\begin_layout Plain Layout

		add.to.row=sim.prefix
\end_layout

\begin_layout Plain Layout

	)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

	
\backslash
hline
\end_layout

\begin_layout Plain Layout

	
\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout

	
\backslash
caption{For each setting of the simulation study, the frequency of exact
 zeroes in the estimates of $
\backslash
hat{
\backslash
beta}_2,
\backslash
dots,
\backslash
hat{
\backslash
beta}_5$ as estimated by local adaptive grouped regularizaton (LAGR).
 Also, the mean integrated squared error (MISE) for the fitted output of
 each simulation setting, under variable selection via LAGR, locally linear
 regression without selection (VCR), and by locally linear regression with
 oracular selection (Oracle).
 Highlighting indicates the 
\backslash
textbf{closest} and 
\backslash
emph{next-closest} to the actual error variance $
\backslash
sigma_
\backslash
varepsilon^2$ for that simulation setting.}
\end_layout

\begin_layout Plain Layout

	
\backslash
label{tab:misey}
\end_layout

\begin_layout Plain Layout


\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The proposed LAGR method was accurate in selection and estimation, with
 estimation accuracy for 
\begin_inset Formula $\beta_{1}(\bm{s})$
\end_inset

 about equal to that of the VCR model with no selection, and with consistently
 better accuracy for estimating 
\begin_inset Formula $\beta_{2}(\bm{s}),\dots,\beta_{5}(\bm{s})$
\end_inset

.
\end_layout

\begin_layout Standard
There was minimal difference in the performance of the proposed LAGR method
 between low (
\begin_inset Formula $\sigma_{\varepsilon}=0.5$
\end_inset

) and high (
\begin_inset Formula $\sigma_{\varepsilon}=1$
\end_inset

) error variance, and between no (
\begin_inset Formula $\rho=0$
\end_inset

) and moderate (
\begin_inset Formula $\rho=0.5$
\end_inset

) correlation among the covariates.
 But the selection and estimation accuracy did decline when there was high
 (
\begin_inset Formula $\rho=0.9$
\end_inset

) correlation among the covariates.
\end_layout

\begin_layout Section
Data Example
\begin_inset CommandInset label
LatexCommand label
name "sec:example"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<boston-coef-import, echo=FALSE, results='asis', message=FALSE>>=
\end_layout

\begin_layout Plain Layout

require(xtable)
\end_layout

\begin_layout Plain Layout

require(brooks)
\end_layout

\begin_layout Plain Layout

load("~/git/gwr/scratch/boston-preset-bw.RData")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

vars = c('CRIM', 'RM', 'RAD', 'TAX', 'LSTAT')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

boston.coef.summary = matrix(NA, nrow=0, ncol=3)
\end_layout

\begin_layout Plain Layout

for (v in vars) {
\end_layout

\begin_layout Plain Layout

	row = vector()
\end_layout

\begin_layout Plain Layout

	colname.coef = paste("coef", v, sep="")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	#Add the table's elements to this row:
\end_layout

\begin_layout Plain Layout

	row = c(row, mean(boston.tracts@data[,colname.coef]))
\end_layout

\begin_layout Plain Layout

	row = c(row, sd(boston.tracts@data[,colname.coef]))
\end_layout

\begin_layout Plain Layout

	row = c(row, mean(boston.tracts@data[,colname.coef]==0))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	#Add this row to the table:
\end_layout

\begin_layout Plain Layout

	boston.coef.summary = rbind(boston.coef.summary, row)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

rownames(boston.coef.summary) = vars
\end_layout

\begin_layout Plain Layout

colnames(boston.coef.summary) = c('Mean', 'SD', 'Prop.
 zero')
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The proposed LAGR estimation method was used to estimate the coefficients
 in a VCR model of the effect of some covariates on the price of homes in
 Boston based on data from the 1970 U.S.
 census
\begin_inset CommandInset citation
LatexCommand citep
key "Harrison-Rubinfeld-1978,Gilley-Pace-1996,Pace-Gilley-1997"

\end_inset

.
 The data are the median price of homes sold in 506 census tracts (MEDV),
 along with the potential covariates CRIM (the per-capita crime rate in
 the tract), RM (the mean number of rooms for houses sold in the tract),
 RAD (an index of how accessible the tract is from Boston's radial roads),
 TAX (the property tax per $10,000 of property value), and LSTAT (the percentage
 of the tract's residents who are considered 
\begin_inset Quotes eld
\end_inset

lower status
\begin_inset Quotes erd
\end_inset

).
 The bandwidth parameter was set to 0.2 for a nearest neighbors-type bandwidth,
 meaning that the sum of kernel weights for each local model was 20% of
 the total number of observations.
 The kernel used was the Epanechnikov kernel.
\end_layout

\begin_layout Standard
A summary of the local coefficients is in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:boston-coefs-lagr"

\end_inset

.
 It indicates that RM is the only covariate with a positive mean of the
 local coefficients.
 The coefficient of the CRIM variable was estimated to be exactly zero at
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{100*round(boston.coef.summary['CRIM','Prop.
 zero'],2)}
\end_layout

\end_inset

% of the locations.
 The percentage for the RAD variable was 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{100*round(boston.coef.summary['RAD','Prop.
 zero'],2)}
\end_layout

\end_inset

%.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{figure}
\end_layout

\begin_layout Plain Layout

<<boston-plots, echo=FALSE, results='asis', message=FALSE>>=
\end_layout

\begin_layout Plain Layout

require(brooks)
\end_layout

\begin_layout Plain Layout

load("~/git/gwr/scratch/boston-preset-bw.RData")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

bmap = list()
\end_layout

\begin_layout Plain Layout

for (v in c('CRIM', 'RM', 'RAD', 'TAX', 'LSTAT')) {
\end_layout

\begin_layout Plain Layout

    bmap[[v]] = ggplot(boston.map) +
\end_layout

\begin_layout Plain Layout

        aes(x=PolyCoordsY, y=PolyCoordsX, group=Poly_Name) +
\end_layout

\begin_layout Plain Layout

        aes_string(fill=paste('coef', v, sep='')) +
\end_layout

\begin_layout Plain Layout

        geom_polygon() +
\end_layout

\begin_layout Plain Layout

        scale_fill_gradient2(low='orange', mid='white', high="purple", midpoint=
0) +
\end_layout

\begin_layout Plain Layout

        xlab("longitude") +
\end_layout

\begin_layout Plain Layout

        ylab("latitude") + 
\end_layout

\begin_layout Plain Layout

		theme_bw()
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

multiplot(plotlist=bmap, cols=2)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
caption{Coefficients for the Boston house price data as estimated by local
 adaptive grouped regularization.
\backslash
label{fig:boston-lagr-coefs}}
\end_layout

\begin_layout Plain Layout


\backslash
end{figure}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Estimates of the regression coefficients are plotted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:boston-lagr-coefs"

\end_inset

.
 One interesting result is that the TAX variable was nowhere found to be
 an important predictor of the median house price.
 Another is that the coefficients of CRIM and LSTAT are everywhere negative
 or zero (meaning that a greater crime rate or proportion of lower-status
 individuals is associated with a lower median house price where the effect
 is discernable) and that of RM is positive (meaning that a greater average
 number of rooms per house is associated with a greater median house price).
 The coefficient of RAD is positive in some areas and negative in others.
 This indicates that there are parts of Boston where access to radial roads
 is associated with a greater median house price and parts where it is associate
d with a lesser median house price.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<boston-coef-table, echo=FALSE, results='asis', message=FALSE>>=
\end_layout

\begin_layout Plain Layout

require(xtable)
\end_layout

\begin_layout Plain Layout

require(brooks)
\end_layout

\begin_layout Plain Layout

load("~/git/gwr/scratch/boston-preset-bw.RData")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

vars = c('CRIM', 'RM', 'RAD', 'TAX', 'LSTAT')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

boston.coef.summary = matrix(NA, nrow=0, ncol=3)
\end_layout

\begin_layout Plain Layout

for (v in vars) {
\end_layout

\begin_layout Plain Layout

	row = vector()
\end_layout

\begin_layout Plain Layout

	colname.coef = paste("coef", v, sep="")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	#Add the table's elements to this row:
\end_layout

\begin_layout Plain Layout

	row = c(row, mean(boston.tracts@data[,colname.coef]))
\end_layout

\begin_layout Plain Layout

	row = c(row, sd(boston.tracts@data[,colname.coef]))
\end_layout

\begin_layout Plain Layout

	row = c(row, mean(boston.tracts@data[,colname.coef]==0))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	#Add this row to the table:
\end_layout

\begin_layout Plain Layout

	boston.coef.summary = rbind(boston.coef.summary, row)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

rownames(boston.coef.summary) = vars
\end_layout

\begin_layout Plain Layout

colnames(boston.coef.summary) = c('Mean', 'SD', 'Prop.
 zero')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#Generate the table, caption it, and print it with emphasis.
\end_layout

\begin_layout Plain Layout

boston.coef.table = xtable(boston.coef.summary, align="|c|ccc|")
\end_layout

\begin_layout Plain Layout

caption(boston.coef.table) = "The mean, standard deviation, and proportion
 of zeros among the local coefficients in a model for the median house price
 in census tracts in Boston, with coefficients selected and fitted by LAGR."
\end_layout

\begin_layout Plain Layout

label(boston.coef.table) = "tab:boston-coefs-lagr"
\end_layout

\begin_layout Plain Layout

print(boston.coef.table, table.placement=NULL)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In their example using the same data, 
\begin_inset CommandInset citation
LatexCommand cite
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

 estimated that the coefficients of RAD annd LSTAT should be constant, at
 
\begin_inset Formula $0.36$
\end_inset

 and 
\begin_inset Formula $-0.45$
\end_inset

, respectively.
 That conclusion differs from our result, which says that the mean local
 coefficient of RAD is actually negative (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(boston.coef.summary['RAD','Mean'],2)}
\end_layout

\end_inset

), while our mean fitted local coefficient for LSTAT was more negative than
 the estimate of 
\begin_inset CommandInset citation
LatexCommand cite
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

.
\end_layout

\begin_layout Section
Extension to Generalized Linear Regression
\begin_inset CommandInset label
LatexCommand label
name "sec:lagr-gllm"

\end_inset


\end_layout

\begin_layout Subsection
Model
\end_layout

\begin_layout Standard
Generalized linear models (GLMs) extend the linear regression model to a
 response variable following any distribution in a single-parameter exponential
 family 
\begin_inset CommandInset citation
LatexCommand citep
key "McCullagh-Nelder-1989"

\end_inset

.
 As was the case for the local linear regression model, local generalized
 GLM coefficients are smooth functions of location.
 If the response variable 
\begin_inset Formula $y$
\end_inset

 is from an exponential-family distribution then its density is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f\left(y(\bm{s})|\bm{x}(\bm{s}),\theta(\bm{s}0\right)=c\left(y(\bm{s}0\right)\times\exp\left[\theta(\bm{s})y(\bm{s})-b\left(\theta(\bm{s})\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\phi$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 are parameters, 
\begin_inset Formula $E\left\{ y(\bm{s})|\bm{x}(\bm{s})\right\} =\mu(\bm{s})=b'\left(\theta(\bm{s})\right)$
\end_inset

, 
\begin_inset Formula $\theta(\bm{s})=(g\circ b')^{-1}\left(\eta(\bm{s})\right)$
\end_inset

, 
\begin_inset Formula $\eta(\bm{s})=\bm{x}(\bm{s})^{T}\bm{\beta}(\bm{s})=g\left(\mu(\bm{s})\right)$
\end_inset

, and 
\begin_inset Formula $\text{\text{Var}}\left\{ y(\bm{s})|\bm{x}(\bm{s})\right\} =b''\left(\theta(\bm{s})\right)$
\end_inset

.
 The function 
\begin_inset Formula $g(\cdot)$
\end_inset

 is called the link function.
 If its inverse 
\begin_inset Formula $g^{-1}(\cdot)=b'(\cdot)$
\end_inset

, then the composition 
\begin_inset Formula $(g\circ b')(\cdot)$
\end_inset

 is the identity function.
 This particular 
\begin_inset Formula $g$
\end_inset

 is called the canonical link.
 
\end_layout

\begin_layout Subsection
Coefficient Estimation via Local Quasi-likelihood
\end_layout

\begin_layout Standard
Assuming the canonical link, all that is required is to specify the mean-varianc
e relationship via the variance function, 
\begin_inset Formula $V\left(\mu(\bm{s})\right)$
\end_inset

.
 Then the local coefficients can be estimated by maximizing the local quasi-like
lihood 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\mathcal{\ell}^{*}\left(\bm{\zeta}(\bm{s})\right) & =\sum_{i=1}^{n}K_{h}\left(\|\bm{s}-\bm{s}_{i}\|\right)Q\left(g^{-1}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s})\right),Y_{i}\right),
\end{align}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\bm{Z}\left(\bm{s}\right)$
\end_inset

 and 
\begin_inset Formula $\bm{\zeta}\left(\bm{s}\right)$
\end_inset

 are defined in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:augmented-covariates"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:augmented-coefficients"

\end_inset

.
 The local quasi-likelihood generalizes the local log-likelihood that was
 used to estimate coefficients in the local linear model case.
 The quasi-likelihood is convex, and is defined in terms of its derivative,
 the quasi-score function 
\begin_inset Formula $\left(\partial/\partial\mu\right)Q(\mu,y)=(y-\mu)\{V(\mu)\}^{-1}$
\end_inset

.
 The local quasi-likelihood is maximized by setting the local quasi-score
 equation to zero:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
(\partial/\partial\bm{\zeta})\mathcal{\ell}^{*}\left(\hat{\bm{\zeta}}(\bm{s})\right) & =\sum_{i=1}^{n}K_{h}\left(\|\bm{s}-\bm{s}_{i}\|\right)\left(y_{i}-\hat{\mu}(\bm{s}_{i};\bm{s})\right)\left\{ V\left(\hat{\mu}(\bm{s}_{i};\bm{s})\right)\right\} ^{-1}\bm{z}_{i}=\bm{0}_{3p},
\end{align}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\hat{\mu}(\bm{s}_{i};\bm{s})=g^{-1}\left(\bm{z}_{i}^{T}\hat{\bm{\zeta}}(\bm{s})\right)$
\end_inset

 is the mean at location 
\begin_inset Formula $\bm{s}_{i}$
\end_inset

 estimated using the coefficients 
\begin_inset Formula $\hat{\bm{\zeta}}(\bm{s})$
\end_inset

 fitted at location 
\begin_inset Formula $\bm{s}$
\end_inset

.
 The asymptotic distribution of the local coefficients in a varying-coefficient
 GLM with a one-dimensional effect-modifying parameter are given in 
\begin_inset CommandInset citation
LatexCommand cite
key "Cai-Fan-Li-2000"

\end_inset

.
 For coefficients that vary in two dimensions, it follows from Lemmas 
\begin_inset CommandInset ref
LatexCommand ref
reference "lemma:omega"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "lemma:delta"

\end_inset

 that the distribution of the estimated local coefficients is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left\{ nh^{2}f(\bm{s})\right\} ^{1/2}\left[\tilde{\bm{\beta}}(\bm{s})-\bm{\beta}(\bm{s})-(1/2)\kappa_{0}^{-1}\kappa_{2}h^{2}\left\{ \nabla_{uu}^{2}\bm{\beta}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}(\bm{s})\right\} \right]\xrightarrow{{D}}N\left(\bm{0},\kappa_{0}^{-2}\nu_{0}\Gamma^{-1}(\bm{s})\right).
\]

\end_inset


\end_layout

\begin_layout Subsection
LAGR Penalized Local Likelihood
\end_layout

\begin_layout Standard
As in the case of linear models, the LAGR for GLMs is a grouped 
\begin_inset Formula $\ell_{1}$
\end_inset

 regularization method.
 Now, though, we use a penalized local quasi-likelihood:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\mathcal{J}\left(\bm{\zeta}(\bm{s})\right) & =\mathcal{\ell}^{*}\left(\bm{\zeta}(\bm{s})\right)+\mathcal{P}\left(\bm{\zeta}(\bm{s})\right)\label{eq:adaptive-lasso-GLLM}\\
 & =\sum_{i=1}^{n}K_{h}\left(\|\bm{s}-\bm{s}_{i}\|\right)Q\left(g^{-1}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s})\right),Y_{i}\right)+\sum_{j=1}^{p}\phi_{j}(\bm{s})\|\bm{\zeta}_{(j)}(\bm{s})\|.
\end{align}

\end_inset


\end_layout

\begin_layout Standard
As in the case of Gaussian data, let 
\begin_inset Formula $\phi_{j}(\bm{s})=\lambda_{n}\|\tilde{\bm{\zeta}}_{(j)}(\bm{s})\|^{-\gamma}$
\end_inset

, where 
\begin_inset Formula $\lambda_{n}>0$
\end_inset

 is a the local tuning parameter applied to all coefficients at location
 
\begin_inset Formula $\bm{s}$
\end_inset

 and 
\begin_inset Formula $\tilde{\bm{\zeta}}_{(j)}(\bm{s})$
\end_inset

 is the vector of unpenalized local coefficients.
\end_layout

\begin_layout Subsection
Oracle properties of LAGR in the GLM setting
\end_layout

\begin_layout Standard
The following are additional to the definitions and assumptions of Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:oracle-properties"

\end_inset

:
\end_layout

\begin_layout Standard
Define 
\begin_inset Formula $\rho(\bm{s},\bm{z})=\left[g_{1}\left(\mu(\bm{s},\bm{z})\right)\right]^{2}Var\left\{ Y(\bm{s})|\bm{X}(\bm{s}),\bm{s}\right\} $
\end_inset

, where 
\begin_inset Formula $g_{1}(\cdot)=g'_{0}(\cdot)/g'(\cdot)$
\end_inset

, and 
\begin_inset Formula $g_{0}(\cdot)$
\end_inset

 is the canonical link function.
 So when the canonical link is used, 
\begin_inset Formula $\rho(\bm{s},\bm{z})=V\left(\mu(\bm{s},\bm{z})\right)$
\end_inset

.
 Let 
\begin_inset Formula $\Gamma\left(\bm{s}\right)=E\left\{ \rho\left(\bm{s},\bm{X}(\bm{s})\right)\bm{X}(\bm{s})\bm{X}(\bm{s})^{T}|\bm{s},\bm{Z}(\bm{s})=\bm{z}\right\} $
\end_inset

 and 
\begin_inset Formula $\Gamma_{(a)}\left(\bm{s}\right)=E\left\{ \rho\left(\bm{s},\bm{X}_{(a)}(\bm{s})\right)\bm{X}_{(a)}(\bm{s})\bm{X}_{(a)}(\bm{s})^{T}|\bm{s},\bm{Z}(\bm{s})=\bm{z}\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
Assume the following conditions.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.9)
\end_layout

\end_inset

The functions 
\begin_inset Formula $g'''\left(\bm{s}\right)$
\end_inset

, 
\begin_inset Formula $\nabla\Gamma\left(\bm{s}\right)$
\end_inset

, 
\begin_inset Formula $\nabla\Gamma_{(a)}\left(\bm{s}\right)$
\end_inset

, 
\begin_inset Formula $V\left(\mu\left(\bm{s},\bm{z}\right)\right)$
\end_inset

, and 
\begin_inset Formula $V'\left(\mu\left(\bm{s},\bm{z}\right)\right)$
\end_inset

 are continuous at 
\begin_inset Formula $\bm{s}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
(A.10)
\end_layout

\end_inset

The function 
\begin_inset Formula $\left(\partial^{2}/\partial\mu^{2}\right)Q\left(g^{-1}\left(\mu\right),y\right)<0$
\end_inset

 for 
\begin_inset Formula $\mu\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 in the range of the response.
\end_layout

\begin_layout Standard
These additonal conditions are necessary for Taylor expansion of the local
 quasi-likelihood.
 Condition (A.10) assures that the local quasi-likelihood is convex and has
 a unique minimizer.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "theorem:normality-glm"

\end_inset

 
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Asymptotic normality
\end_layout

\end_inset

 Under (A.1)-(A.10), if 
\begin_inset Formula $h^{-1}n^{-1/2}a_{n}\xrightarrow{p}0$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

, then 
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{gather*}
\left\{ nh^{2}f(\bm{s})\right\} ^{1/2}\left[\hat{\bm{\beta}}_{(a)}(\bm{s})-\bm{\beta}_{(a)}(\bm{s})-\left(2\kappa_{0}\right)^{-1}\kappa_{2}h^{2}\left\{ \nabla_{uu}^{2}\bm{\beta}_{\left(a\right)}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}_{\left(a\right)}(\bm{s})\right\} \right]\\
\xrightarrow{d}N\left(0,\kappa_{0}^{-2}\nu_{0}\Gamma_{(a)}(\bm{s})^{-1}\right)
\end{gather*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "theorem:selection-glm"

\end_inset

 
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Selection consistency
\end_layout

\end_inset

 Under (A.1)-(A.10), if 
\begin_inset Formula $h^{-1}n^{-1/2}a_{n}\xrightarrow{p}0$
\end_inset

 and 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

 then 
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\[
P\left\{ \|\hat{\bm{\zeta}}_{(j)}(\bm{s})\|=\bm{0}\right\} \to0\text{ if }j\le p_{0},\text{ and }P\left\{ \|\hat{\bm{\zeta}}_{(j)}(\bm{s})\|=\bm{0}\right\} \to1\text{ if }j>p_{0}.
\]

\end_inset

 
\end_layout

\begin_layout Standard
By Theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "theorem:normality-glm"

\end_inset

, the LAGR estimates acheive the same asymptotic distribution as if the
 nonzero coefficients were known in advance.
 The difference between the Gaussian and GLM cases is that 
\begin_inset Formula $\sigma^{2}\Psi_{(a)}(\bm{s})^{-1}$
\end_inset

 in the variance term of Theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "theorem:normality"

\end_inset

 has been replaced by 
\begin_inset Formula $\Gamma_{(a)}(\bm{s})^{-1}$
\end_inset

 in Theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "theorem:normality-glm"

\end_inset

 because the variance of the response in the GLM case depends on the expectation
 of the response.
\end_layout

\begin_layout Section*
\start_of_appendix
Appendix: Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sec:gaussian-normality-proof"

\end_inset

 
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $H_{n}(\bm{u})=\mathcal{J}\left(\bm{\zeta}(\bm{s})+h^{-1}n^{-1/2}\bm{u}\right)-\mathcal{J}\left(\bm{\zeta}(\bm{s})\right)$
\end_inset

 and 
\begin_inset Formula $\alpha_{n}=h^{-1}n^{-1/2}$
\end_inset

.
 Then, we have 
\begin_inset Formula 
\begin{align*}
H_{n}(\bm{u})= & (1/2)\left[\bm{Y}-\bm{Z}(\bm{s})\left\{ \bm{\zeta}(\bm{s})+\alpha_{n}\bm{u}\right\} \right]^{T}\bm{W}\!(\bm{s})\left[\bm{Y}-\bm{Z}(\bm{s})\left\{ \bm{\zeta}(\bm{s})+\alpha_{n}\bm{u}\right\} \right]\\
 & +\sum_{j=1}^{p}\phi_{j}(\bm{s})\|\bm{\zeta}_{j}(\bm{s})+\alpha_{n}\bm{u}_{j}\|\\
 & -(1/2)\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} ^{T}\bm{W}\!(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} -\sum_{j=1}^{p}\phi_{j}(\bm{s})\|\bm{\zeta}_{j}(\bm{s})\|\\
= & \left(1/2\right)\alpha_{n}^{2}\bm{u}^{T}\left\{ \bm{Z}(\bm{s})^{T}\bm{W}\!(\bm{s})\bm{Z}(\bm{s})\right\} \bm{u}\\
 & -\alpha_{n}\bm{u}^{T}\left[\bm{Z}(\bm{s})^{T}\bm{W}\!(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} \right]\\
 & +\sum_{j=1}^{p}n^{-1/2}\phi_{j}(\bm{s})n^{1/2}\left\{ \|\bm{\zeta}_{(j)}(\bm{s})+\alpha_{n}\bm{u}_{(j)}\|-\|\bm{\zeta}_{(j)}(\bm{s})\|\right\} 
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
The limiting behavior of the last term differs between the cases 
\begin_inset Formula $j\le p_{0}$
\end_inset

 and 
\begin_inset Formula $j>p_{0}$
\end_inset

.
\end_layout

\begin_layout Proof

\emph on
Case 
\begin_inset Formula $j\le p_{0}$
\end_inset


\emph default
: If 
\begin_inset Formula $j\le p_{0}$
\end_inset

, then 
\begin_inset Formula $n^{-1/2}\phi_{j}(\bm{s})\to n^{-1/2}\lambda_{n}\|\bm{\zeta}_{(j)}(\bm{s})\|^{-\gamma}$
\end_inset

 and 
\begin_inset Formula $|n^{1/2}\left\{ \|\bm{\zeta}_{(j)}(\bm{s})+\alpha_{n}\bm{u}_{(j)}\|-\|\bm{\zeta}_{(j)}(\bm{s})\|\right\} |\le h^{-1}\|\bm{u}_{(j)}\|$
\end_inset

 .
 Thus, 
\begin_inset Formula 
\[
\phi_{j}(\bm{s})\left(\|\bm{\zeta}_{(j)}(\bm{s})+\alpha_{n}\bm{u}_{(j)}\|-\|\bm{\zeta}_{(j)}(\bm{s})\|\right)\le\alpha_{n}\phi_{j}(\bm{s})\|\bm{u}_{(j)}\|\le\alpha_{n}a_{n}\|\bm{u}_{(j)}\|\to0
\]

\end_inset


\end_layout

\begin_layout Proof

\emph on
Case 
\begin_inset Formula $j>p_{0}$
\end_inset


\emph default
: If 
\begin_inset Formula $j>p_{0}$
\end_inset

, then 
\begin_inset Formula $\phi_{j}(\bm{s})\left(\|\bm{\zeta}_{(j)}(\bm{s})+\alpha_{n}\bm{u}_{(j)}\|-\|\bm{\zeta}_{(j)}(\bm{s})\|\right)=\phi_{j}(\bm{s})\alpha_{n}\|\bm{u}_{(j)}\|$
\end_inset

.
 Since 
\begin_inset Formula $h=O(n^{-1/6})$
\end_inset

, if 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

, then 
\begin_inset Formula $\alpha_{n}b_{n}\xrightarrow{p}\infty$
\end_inset

.
 Thus, if 
\begin_inset Formula $\|\bm{u}_{(j)}\|\ne0$
\end_inset

, then 
\begin_inset Formula 
\[
\alpha_{n}\phi_{j}(\bm{s})\|\bm{u}_{(j)}\|\ge\alpha_{n}b_{n}\|\bm{u}_{(j)}\|\to\infty.
\]

\end_inset

On the other hand, if 
\begin_inset Formula $\|\bm{u}_{(j)}\|=0$
\end_inset

, then 
\begin_inset Formula $\alpha_{n}\phi_{j}(\bm{s})\|\bm{u}_{(j)}\|=0$
\end_inset

.
\end_layout

\begin_layout Proof
Thus, the limit of 
\begin_inset Formula $H_{n}(\bm{u})$
\end_inset

 is the same as the limit of 
\begin_inset Formula $H_{n}^{*}(\bm{u})$
\end_inset

 where 
\begin_inset Formula $H_{n}^{*}(\bm{u})=\infty$
\end_inset

 if 
\begin_inset Formula $\|\bm{u}_{(j)}\|\ne0$
\end_inset

 for some 
\begin_inset Formula $j>p_{0}$
\end_inset

, and 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
H_{n}^{*}(\bm{u})=(1/2)\alpha_{n}^{2}\bm{u}^{T}\left\{ \bm{Z}(\bm{s})^{T}\bm{W}\!(\bm{s})\bm{Z}(\bm{s})\right\} \bm{u}-\alpha_{n}\bm{u}^{T}\left[\bm{Z}(\bm{s})^{T}\bm{W}\!(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} \right]
\]

\end_inset


\end_layout

\begin_layout Proof
otherwise.
 It follows that 
\begin_inset Formula $H_{n}^{*}(\bm{u})$
\end_inset

 is convex and its unique minimizer is 
\begin_inset Formula 
\[
\hat{\bm{u}}_{n}=\left\{ n^{-1}\bm{Z}(\bm{s})^{T}\bm{W}\!(\bm{s})\bm{Z}(\bm{s})\right\} ^{-1}\left[hn^{1/2}\bm{Z}(\bm{s})^{T}\bm{W}\!(\bm{s})\left\{ \bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})\right\} \right].
\]

\end_inset

By epiconvergence, the minimizer of the limiting function is the limit of
 the minimizers 
\begin_inset Formula $\hat{\bm{u}}_{n}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "Geyer-1994,Knight-Fu-2000"

\end_inset

.
 Since, by Lemma 2 of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\hat{\bm{u}}_{n}-\left(2\alpha_{n}f(\bm{s})^{1/2}\kappa_{0}\right)^{-1}\kappa_{2}h^{2}\left\{ \nabla_{uu}^{2}\bm{\zeta}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}(\bm{s})\right\} \xrightarrow{d}N\left(0,\alpha_{n}^{-2}f(\bm{s})^{-1}\kappa_{0}^{-2}\nu_{0}\sigma^{2}\Psi(\bm{s})^{-1}\right)
\]

\end_inset

the result of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset

 follows.
\end_layout

\begin_layout Section*
Appendix: Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:selection"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sec:gaussian-selection-proof"

\end_inset


\end_layout

\begin_layout Proof
We showed in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset

 that 
\begin_inset Formula $\hat{\bm{\zeta}}_{(j)}(\bm{s})\xrightarrow{p}\bm{\zeta}_{(j)}(\bm{s})+\left(2\kappa_{0}\right)^{-1}\kappa_{2}h^{2}\nabla_{uu}^{2}\left\{ \bm{\zeta}_{(j)}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{(j)}(\bm{s})\right\} $
\end_inset

, so to complete the proof of selection consistency, it only remains to
 show that 
\begin_inset Formula $P\left\{ \hat{\bm{\zeta}}_{(j)}(\bm{s})=\bm{0}\right\} \to1$
\end_inset

 if 
\begin_inset Formula $j>p_{0}$
\end_inset

.
\end_layout

\begin_layout Proof
The proof is by contradiction.
 Without loss of generality we consider only the case 
\begin_inset Formula $j=p$
\end_inset

.
\end_layout

\begin_layout Proof
Assume 
\begin_inset Formula $\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\ne0$
\end_inset

.
 Then 
\begin_inset Formula $\mathcal{J}\left(\bm{\zeta}(\bm{s})\right)$
\end_inset

 is differentiable w.r.t.
 
\begin_inset Formula $\bm{\zeta}_{(p)}(\bm{s})$
\end_inset

 and is minimized where 
\begin_inset Formula 
\begin{align*}
\bm{0}= & \bm{Z}_{(p)}^{T}(\bm{s})\bm{W}\!(\bm{s})\left\{ \bm{Y}-\bm{Z}_{(\mhyphen p)}\left(\bm{s}\right)\hat{\bm{\zeta}}_{(\mhyphen p)}(\bm{s})-\bm{Z}_{(p)}\left(\bm{s}\right)\hat{\bm{\zeta}}_{(p)}(\bm{s})\right\} -\phi_{(p)}(\bm{s})\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}\\
= & \bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!(\bm{s})\left[\bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})-\left(2\kappa_{0}\right)^{-1}h^{2}\kappa_{2}\left\{ \nabla_{uu}^{2}\bm{\zeta}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}(\bm{s})\right\} \right]\\
 & +\bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!(\bm{s})\bm{Z}_{(\mhyphen p)}(\bm{s})\left[\bm{\zeta}_{(\mhyphen p)}(\bm{s})+\left(2\kappa_{0}\right)^{-1}h^{2}\kappa_{2}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{(\mhyphen p)}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{(\mhyphen p)}(\bm{s})\right\} -\hat{\bm{\zeta}}_{(\mhyphen p)}(\bm{s})\right]\\
 & +\bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!(\bm{s})\bm{Z}_{(p)}(\bm{s})\left[\bm{\zeta}_{(p)}(\bm{s})+\left(2\kappa_{0}\right)^{-1}h^{2}\kappa_{2}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{(p)}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{(p)}(\bm{s})\right\} -\hat{\bm{\zeta}}_{(p)}(\bm{s})\right]\\
 & -\phi_{p}(\bm{s})\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Thus,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align}
\left(n^{-1}h^{2}\right)^{1/2} & \phi_{p}(\bm{s})\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}=\nonumber \\
 & \bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!(\bm{s})\left(n^{-1}h^{2}\right)^{1/2}\left[\bm{Y}-\bm{Z}(\bm{s})\bm{\zeta}(\bm{s})-\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}(\bm{s})\right\} \right]\nonumber \\
 & +\left\{ n^{-1}\bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!(\bm{s})\bm{Z}_{(\mhyphen p)}(\bm{s})\right\} \left(nh^{2}\right)^{1/2}\left[\bm{\zeta}_{(\mhyphen p)}(\bm{s})+\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{(\mhyphen p)}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{(\mhyphen p)}(\bm{s})\right\} -\hat{\bm{\zeta}}_{(\mhyphen p)}(\bm{s})\right]\nonumber \\
 & +\left\{ n^{-1}\bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!(\bm{s})\bm{Z}_{(p)}(\bm{s})\right\} \left(nh^{2}\right)^{1/2}\left[\bm{\zeta}_{(p)}(\bm{s})+\frac{h^{2}\kappa_{2}}{2\kappa_{0}}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{(p)}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{(p)}(\bm{s})\right\} -\hat{\bm{\zeta}}_{(p)}(\bm{s})\right]\label{eq:selection}
\end{align}

\end_inset


\end_layout

\begin_layout Proof
From Lemma 2 of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

, 
\begin_inset Formula 
\[
O_{p}\left(n^{-1}\bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!\left(\bm{s}\right)\bm{Z}_{(\mhyphen p)}(\bm{s})\right)=O_{p}\left(n^{-1}\bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!(\bm{s})\bm{Z}_{(p)}(\bm{s})\right)=O_{p}\left(1\right).
\]

\end_inset


\end_layout

\begin_layout Proof
From Theorem 3 of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sun-Yan-Zhang-Lu-2014"

\end_inset

, we have that 
\begin_inset Formula 
\[
\left(nh^{2}\right)^{1/2}\left[\hat{\bm{\zeta}}_{(\mhyphen p)}(\bm{s})-\bm{\zeta}_{(\mhyphen p)}(\bm{s})-\left(2\kappa_{0}\right)^{-1}h^{2}\kappa_{2}\left\{ \nabla_{uu}^{2}\zeta_{(\mhyphen p)}(\bm{s})+\nabla_{vv}^{2}\zeta_{(\mhyphen p)}(\bm{s})\right\} \right]=O_{p}\left(1\right)
\]

\end_inset

 and 
\begin_inset Formula 
\[
\left(nh^{2}\right)^{1/2}\left[\hat{\bm{\zeta}}_{(p)}(\bm{s})-\bm{\zeta}_{(p)}(\bm{s})-\left(2\kappa_{0}\right)^{-1}h^{2}\kappa_{2}\left\{ \nabla_{uu}^{2}\bm{\zeta}_{(p)}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{(p)}(\bm{s})\right\} \right]=O_{p}\left(1\right).
\]

\end_inset


\end_layout

\begin_layout Proof
We showed in the proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality"

\end_inset

 that
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\left(nh^{2}\right)^{1/2}\bm{Z}_{(p)}(\bm{s})^{T}\bm{W}\!(\bm{s})\left[\bm{Y}-\bm{Z}\!(\bm{s})\bm{\zeta}(\bm{s})-\left(2\kappa_{0}\right)^{-1}h^{2}\kappa_{2}\left\{ \nabla_{uu}^{2}\bm{\zeta}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}(\bm{s})\right\} \right]=O_{p}\left(1\right).
\]

\end_inset


\end_layout

\begin_layout Proof
The right hand side of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:selection"

\end_inset

) is 
\begin_inset Formula $O_{p}(1)$
\end_inset

, so for 
\begin_inset Formula $\hat{\bm{\zeta}}_{(p)}(\bm{s})$
\end_inset

 to be a solution, we must have that 
\begin_inset Formula $hn^{-1/2}\phi_{p}(\bm{s})\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}=O_{p}\left(1\right)$
\end_inset

.
\end_layout

\begin_layout Proof
But since by assumption 
\begin_inset Formula $\hat{\bm{\zeta}}_{(p)}(\bm{s})\ne\bm{0}$
\end_inset

, there must be some 
\begin_inset Formula $k\in\{1,2,3\}$
\end_inset

 such that 
\begin_inset Formula $|\hat{\zeta}_{(p)_{k}}(\bm{s})|=\max\{|\hat{\zeta}_{(p)_{m}}(\bm{s})|:1\le m\le3\}$
\end_inset

.
 And for this 
\begin_inset Formula $k$
\end_inset

, we have that 
\begin_inset Formula $|\hat{\zeta}_{(p)_{k}}(\bm{s})|\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}\ge3^{-1/2}>0$
\end_inset

.
\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $hn^{-1/2}b_{n}\to\infty$
\end_inset

, we have that 
\begin_inset Formula $hn^{-1/2}\phi_{p}(\bm{s})\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}\ge hb_{n}\left(3n\right)^{-1/2}\to\infty$
\end_inset

 and therefore the left hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:selection"

\end_inset

 dominates the sum to the right side.
 Thus, for large enough 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $\hat{\bm{\zeta}}_{(p)}(\bm{s})\ne\bm{0}$
\end_inset

 cannot maximize 
\begin_inset Formula $\mathcal{J}\left(\cdot\right)$
\end_inset

, and therefore 
\begin_inset Formula $P\left\{ \hat{\bm{\zeta}}_{(b)}(\bm{s})=\bm{0}\right\} \to1$
\end_inset

.
 
\end_layout

\begin_layout Section*
Appendix: Lemmas
\end_layout

\begin_layout Standard
The next proofs require the following lemmas.
 First, let 
\begin_inset Formula $\bm{z}\in\mathbb{R}^{3p}$
\end_inset

.
 Define the 
\begin_inset Formula $q$
\end_inset

-functions to be the derivatives of the quasi-likelihood: 
\begin_inset Formula $q_{j}(t,y)=\left(\partial/\partial t\right)^{j}Q\left(g^{-1}(t),y\right)$
\end_inset

.
 Then 
\begin_inset Formula $q_{1}\left(\eta\left(\bm{s},\bm{z}\right),\mu\left(\bm{s},\bm{z}\right)\right)=\bm{0}$
\end_inset

, and 
\begin_inset Formula $q_{2}\left(\eta\left(\bm{s},\bm{z}\right),\mu\left(\bm{s},\bm{z}\right)\right)=-\rho\left(\bm{s},\bm{z}\right)$
\end_inset

.
 Let 
\begin_inset Formula $\tilde{\bm{\beta}}_{i}''=\left[\left(\bm{s}_{i}-\bm{s}\right)^{T}\left\{ \nabla^{2}\beta_{1}(\bm{s})\right\} \left(\bm{s}_{i}-\bm{s}\right),\dots,\left(\bm{s}_{i}-\bm{s}\right)^{T}\left\{ \nabla^{2}\beta_{p}(\bm{s})\right\} \left(\bm{s}_{i}-\bm{s}\right)\right]^{T}$
\end_inset

 be the 
\begin_inset Formula $p$
\end_inset

-vector of quadratic forms of location interactions on the second derivatives
 of the coefficient functions.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lemma:omega"

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset Formula 
\begin{multline*}
E\left[\sum_{i=1}^{n}q_{1}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)\bm{Z}_{i}K_{h}\left(\|\bm{s}-\bm{s}_{i}\|\right)\right]=\\
\left(\begin{array}{c}
2^{-1}n^{1/2}h^{3}f(\bm{s})\kappa_{2}\Gamma(\bm{s})\left(\nabla_{uu}^{2}\bm{\beta}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}(\bm{s})\right)^{T}\\
\bm{0}_{2p}
\end{array}\right)+o_{p}\left(h^{2}\bm{1}_{3p}\right)
\end{multline*}

\end_inset


\end_layout

\begin_layout Lemma
and
\end_layout

\begin_layout Lemma
\begin_inset Formula 
\begin{align*}
Var\left[\sum_{i=1}^{n}q_{1}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)\bm{Z}_{i}K_{h}\left(\|\bm{s}-\bm{s}_{i}\|\right)\right]= & f(\bm{s}){\rm diag}\left\{ \nu_{0},\nu_{2},\nu_{2}\right\} \otimes\Gamma(\bm{s})+o\left(1\right)\\
= & \Lambda+o\left(1\right)
\end{align*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lemma:delta"

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset Formula 
\begin{align*}
E\left[\sum_{i=1}^{n}q_{2}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)\bm{Z}_{i}\bm{Z}_{i}^{T}K_{h}\left(\|\bm{s}-\bm{s}_{i}\|\right)\right]= & -f(\bm{s}){\rm diag}\left\{ \kappa_{0},\kappa_{2},\kappa_{2}\right\} \otimes\Gamma(\bm{s})+o\left(1\right)\\
= & -\Delta+o\left(1\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Lemma
and
\end_layout

\begin_layout Lemma
\begin_inset Formula 
\[
Var\left\{ \left(\sum_{i=1}^{n}q_{2}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)\bm{Z}_{i}\bm{Z}_{i}^{T}K_{h}\left(\|\bm{s}-\bm{s}_{i}\|\right)\right)_{ij}\right\} =O\left(n^{-1}h^{-2}\right)
\]

\end_inset


\end_layout

\begin_layout Section*
Appendix: Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality-glm"

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $H'_{n}(\bm{u})=\mathcal{J}^{*}\left(\bm{\zeta}(\bm{s})+\alpha_{n}\bm{u}\right)-\mathcal{J}^{*}\left(\bm{\zeta}(\bm{s})\right)$
\end_inset

 and 
\begin_inset Formula $\alpha_{n}=h^{-1}n^{-1/2}$
\end_inset

.
 Then, maximixing 
\begin_inset Formula $H'_{n}(\bm{u})$
\end_inset

 is equivalent to maximizing 
\begin_inset Formula $H_{n}(\bm{u})$
\end_inset

, where 
\begin_inset Formula 
\begin{align*}
H_{n}(\bm{u})= & n^{-1}\sum_{i=1}^{n}Q\left(g^{-1}\left(\bm{Z}_{i}^{T}\left\{ \bm{\zeta}(\bm{s})+\alpha_{n}\bm{u}\right\} \right),Y_{i}\right)K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right)\\
 & -n^{-1}\sum_{i=1}^{n}Q\left(g^{-1}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s})\right),Y_{i}\right)K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right)\\
 & +n^{-1}\sum_{j=1}^{p}\phi_{j}\left(\bm{s}\right)\|\bm{\zeta}_{(j)}(\bm{s})+\alpha_{n}\bm{u}\|-\sum_{j=1}^{p}\phi_{j}\left(\bm{s}\right)\|\bm{\zeta}_{(j)}(\bm{s})\|
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Define
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
\Omega_{n}= & \alpha_{n}\sum_{i=1}^{n}q_{1}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)\bm{Z}_{i}K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right)\\
= & \alpha_{n}\sum_{i=1}^{n}\omega_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
and 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
\Delta_{n}= & \alpha_{n}^{2}\sum_{i=1}^{n}q_{2}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)\bm{Z}_{i}\bm{Z}_{i}^{T}K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right)\\
= & \alpha_{n}^{2}\sum_{i=1}^{n}\delta_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Then it follows from the Taylor expansion of 
\begin_inset Formula $\mathcal{J}^{*}\left(\bm{\zeta}(\bm{s})+\alpha_{n}\bm{u}\right)$
\end_inset

 around 
\begin_inset Formula $\bm{\zeta}(\bm{s})$
\end_inset

 that
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align}
H_{n}\left(\bm{u}\right)= & \Omega_{n}^{T}\bm{u}\nonumber \\
 & +(1/2)\bm{u}^{T}\Delta_{n}\bm{u}\nonumber \\
 & +\left(\alpha_{n}^{3}/6\right)\sum_{i=1}^{n}q_{3}\left(\bm{Z}_{i}^{T}\tilde{\bm{\zeta}}_{i},Y_{i}\right)\left[\bm{Z}_{i}^{T}\bm{u}\right]^{3}K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right)\nonumber \\
 & +\sum_{j=1}^{p}\phi_{j}\left(\bm{s}\right)\left\{ \|\bm{\zeta}_{(j)}(\bm{s})+h^{-1}n^{-1/2}\bm{u}\|-\|\bm{\zeta}_{(j)}(\bm{s})\|\right\} .\label{eq:taylor-expanded-glm-criterion}
\end{align}

\end_inset


\end_layout

\begin_layout Proof
where 
\begin_inset Formula $\tilde{\bm{\zeta}_{i}}$
\end_inset

 lies between 
\begin_inset Formula $\bm{\zeta}(\bm{s})$
\end_inset

 and 
\begin_inset Formula $\bm{\zeta}(\bm{s})+\alpha_{n}\bm{u}$
\end_inset

.
 Since 
\begin_inset Formula $q_{3}\left(\bm{Z}_{i}^{T}\tilde{\bm{\zeta}}_{i},Y_{i}\right)$
\end_inset

 is linear in 
\begin_inset Formula $Y_{i}$
\end_inset

, 
\begin_inset Formula $K\left(\cdot\right)$
\end_inset

 is bounded, and, by condition (A.6),
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\left(\alpha_{n}^{3}/6\right)E\left|\sum_{i=1}^{n}q_{3}\left(\bm{Z}_{i}^{T}\tilde{\bm{\zeta}}_{i},Y_{i}\right)\left[\bm{Z}_{i}^{T}\bm{u}\right]^{3}K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right)\right|=O\left(\alpha_{n}\right),
\]

\end_inset


\end_layout

\begin_layout Proof
the third term in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:taylor-expanded-glm-criterion"

\end_inset

 is 
\begin_inset Formula $O_{p}\left(\alpha_{n}\right)$
\end_inset

.
 The limiting behavior of the last term of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:taylor-expanded-glm-criterion"

\end_inset

 differs between the cases 
\begin_inset Formula $j\le p_{0}$
\end_inset

 and 
\begin_inset Formula $j>p_{0}$
\end_inset

.
\end_layout

\begin_layout Proof

\emph on
Case 
\begin_inset Formula $j\le p_{0}$
\end_inset

:
\emph default
 If 
\begin_inset Formula $j\le p_{0}$
\end_inset

, then 
\begin_inset Formula $n^{-1/2}\phi_{j}(\bm{s})\to n^{-1/2}\lambda_{n}\|\bm{\zeta}_{(j)}(\bm{s})\|^{-\gamma}$
\end_inset

 and 
\begin_inset Formula $|\sqrt{n}\left\{ \|\bm{\zeta}_{(j)}(\bm{s})+\alpha_{n}\bm{u}_{(j)}\|-\|\bm{\zeta}_{(j)}(\bm{s})\|\right\} |\le h^{-1}\|\bm{u}_{(j)}\|$
\end_inset

 .
 Thus, 
\begin_inset Formula 
\[
\lim\limits _{n\to\infty}\phi_{j}(\bm{s})\left(\|\bm{\zeta}_{(j)}(\bm{s})+\alpha_{n}\bm{u}_{(j)}\|-\|\bm{\zeta}_{(j)}(\bm{s})\|\right)\le\alpha_{n}\phi_{j}(\bm{s})\|\bm{u}_{(j)}\|\le\alpha_{n}a_{n}\|\bm{u}_{(j)}\|\to0
\]

\end_inset


\end_layout

\begin_layout Proof

\emph on
Case 
\begin_inset Formula $j>p_{0}$
\end_inset

:
\emph default
 If 
\begin_inset Formula $j>p_{0}$
\end_inset

, then 
\begin_inset Formula $\phi_{j}(\bm{s})\left(\|\bm{\zeta}_{(j)}(\bm{s})+\alpha_{n}\bm{u}_{(j)}\|-\|\bm{\zeta}_{(j)}(\bm{s})\|\right)=\phi_{j}(\bm{s})\alpha_{n}\|\bm{u}_{(j)}\|$
\end_inset

.
 Since 
\begin_inset Formula $h=O(n^{-1/6})$
\end_inset

, if 
\begin_inset Formula $hn^{-1/2}b_{n}\xrightarrow{p}\infty$
\end_inset

, then 
\begin_inset Formula $\alpha_{n}b_{n}\xrightarrow{p}\infty$
\end_inset

.
 Now, if 
\begin_inset Formula $\|\bm{u}_{(j)}\|\ne0$
\end_inset

, then 
\begin_inset Formula 
\[
\alpha_{n}\phi_{j}(\bm{s})\|\bm{u}_{(j)}\|\ge\alpha_{n}b_{n}\|\bm{u}_{(j)}\|\to\infty.
\]

\end_inset

On the other hand, if 
\begin_inset Formula $\|\bm{u}_{(j)}\|=0$
\end_inset

, then 
\begin_inset Formula $\alpha_{n}\phi_{j}(\bm{s})\|\bm{u}_{(j)}\|=0$
\end_inset

.
\end_layout

\begin_layout Proof
By Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lemma:delta"

\end_inset

, 
\begin_inset Formula $\Delta_{n}=\Delta+O_{p}\left(\alpha_{n}\right)$
\end_inset

, so the limit of 
\begin_inset Formula $H_{n}(\bm{u})$
\end_inset

 is the same as the limit of 
\begin_inset Formula $H_{n}^{*}(\bm{u})$
\end_inset

 where
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
H_{n}^{*}(\bm{u})=\Omega_{n}^{T}\bm{u}+(1/2)\bm{u}^{T}\Delta\bm{u}+o_{p}\left(1\right)
\]

\end_inset

 
\end_layout

\begin_layout Proof
if 
\begin_inset Formula $\|\bm{u}_{j}\|=0\;\forall j>p_{0}$
\end_inset

, and 
\begin_inset Formula $H_{n}^{*}(\bm{u})=\infty$
\end_inset

 otherwise.
 It follows that 
\begin_inset Formula $H_{n}^{*}(\bm{u})$
\end_inset

 is convex and its unique minimizer is
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
\hat{\bm{u}}_{n}= & \Delta^{-1}\Omega_{n}+o_{p}\left(1\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
by the quadratic approximation lemma 
\begin_inset CommandInset citation
LatexCommand citep
key "Fan-Gijbels-1996"

\end_inset

.
 And by epiconvergence, the minimizer of the limiting function is the limit
 of the minimizers 
\begin_inset Formula $\hat{\bm{u}}_{n}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "Geyer-1994,Knight-Fu-2000"

\end_inset

.
\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $\Delta$
\end_inset

 is a constant, the normality of 
\begin_inset Formula $\hat{\bm{u}}_{n}$
\end_inset

 follows from the normality of 
\begin_inset Formula $\Omega_{n}$
\end_inset

, which is establised via the Cram
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
'{e}
\end_layout

\end_inset

r-Wold device.
 Let 
\begin_inset Formula $\bm{d}\in\mathbb{R}^{3p}$
\end_inset

 be a unit vector, and let
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\xi_{i}=q_{1}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)\bm{d}^{T}\bm{Z}_{i}K\left(h^{-1}\|\bm{s}_{i}-\bm{s}\|\right).
\]

\end_inset


\end_layout

\begin_layout Proof
Then 
\begin_inset Formula $\bm{d}^{T}\Omega_{n}=\alpha_{n}\sum_{i=1}^{n}\xi_{i}$
\end_inset

.
 We establish the normality of 
\begin_inset Formula $\bm{d}^{T}\Omega_{n}$
\end_inset

 by checking the Lyapunov condition of the sequence 
\begin_inset Formula $\left\{ \bm{d}^{T}Var\left(\Omega_{n}\right)\bm{d}\right\} ^{-1/2}\left\{ \bm{d}^{T}\Omega_{n}-\bm{d}^{T}E\Omega_{n}\right\} $
\end_inset

.
 By boundedness of 
\begin_inset Formula $K\left(\cdot\right)$
\end_inset

, linearity of 
\begin_inset Formula $q_{1}\left(\bm{Z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)$
\end_inset

 in 
\begin_inset Formula $Y_{i}$
\end_inset

, and assumptions (A.6), (A.7), and (A.9), we have that
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{equation}
n\alpha_{n}^{3}E\left(\left|\xi_{1}\right|^{3}\right)=O\left(\alpha_{n}\right)\to0.\label{eq:lyapunov-bound}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
We observe that 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lyapunov-bound"

\end_inset

 implies that 
\begin_inset Formula $n\alpha_{n}^{3}\left|E\left(\xi_{1}\right)\right|^{3}\to0$
\end_inset

, and since 
\begin_inset Formula $E\left(\left|\xi_{1}-E\xi_{1}\right|^{3}\right)<E\left\{ \left(\left|\xi_{1}\right|+\left|E\xi_{1}\right|\right)^{3}\right\} \to0$
\end_inset

, the Lyapunov condition is satisfied.
 Thus, 
\begin_inset Formula $\Omega_{n}$
\end_inset

 asymptotically follows a Gaussian distribution and the result follows from
 the quadratic approximation lemma.
\end_layout

\begin_layout Section*
Appendix: Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:selection-glm"

\end_inset


\end_layout

\begin_layout Proof
We showed in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem:normality-glm"

\end_inset

 that 
\begin_inset Formula $\hat{\bm{\zeta}}_{(j)}(\bm{s})\xrightarrow{p}\bm{\zeta}_{(j)}(\bm{s})+\left(2\kappa_{0}\right)^{-1}\kappa_{2}h^{2}\{\nabla_{uu}^{2}\bm{\zeta}_{(j)}(\bm{s})+\nabla_{vv}^{2}\bm{\zeta}_{(j)}(\bm{s})\}$
\end_inset

, so to complete the proof of selection consistency, it only remains to
 show that 
\begin_inset Formula $P\left\{ \hat{\bm{\zeta}}_{(j)}(\bm{s})=\bm{0}\right\} \to1$
\end_inset

 if 
\begin_inset Formula $j>p_{0}$
\end_inset

.
\end_layout

\begin_layout Proof
The proof is by contradiction.
 Without loss of generality we consider only the case 
\begin_inset Formula $j=p$
\end_inset

.
\end_layout

\begin_layout Proof
Assume 
\begin_inset Formula $\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\ne0$
\end_inset

.
 Then 
\begin_inset Formula $\mathcal{J}\left(\bm{\zeta}(\bm{s})\right)$
\end_inset

 is differentiable w.r.t.
 
\begin_inset Formula $\bm{\zeta}_{(p)}(\bm{s})$
\end_inset

 and is minimized where 
\begin_inset Formula 
\begin{align}
\phi_{p}(\bm{s})\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}= & \sum_{i=1}^{n}q_{1}\!\left(\bm{Z}_{i}^{T}\hat{\bm{\zeta}}(\bm{s}),Y_{i}\right)\bm{Z}_{i(p)}K\left(h^{-1}\|\bm{s}_{i}-\bm{s}\|\right)\label{eq:glm-selection}
\end{align}

\end_inset


\end_layout

\begin_layout Proof
From Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lemma:omega"

\end_inset

, the right hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:glm-selection"

\end_inset

 is 
\begin_inset Formula $O_{p}\left(1\right)$
\end_inset

, so for 
\begin_inset Formula $\hat{\bm{\zeta}}_{(p)}(\bm{s})$
\end_inset

 to be a solution, we must have that 
\begin_inset Formula $hn^{-1/2}\phi_{p}(\bm{s})\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}=O_{p}\left(1\right)$
\end_inset

.
\end_layout

\begin_layout Proof
But since by assumption 
\begin_inset Formula $\hat{\bm{\zeta}}_{(p)}(\bm{s})\ne\bm{0}$
\end_inset

, there must be some 
\begin_inset Formula $k\in\{1,2,3\}$
\end_inset

 such that 
\begin_inset Formula $|\hat{\zeta}_{(p)_{k}}(\bm{s})|=\max\{|\hat{\zeta}_{(p)_{m}}(\bm{s})|:1\le m\le3\}$
\end_inset

.
 And for this 
\begin_inset Formula $k$
\end_inset

, we have that 
\begin_inset Formula $|\hat{\zeta}_{(p)_{k}}(\bm{s})|\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}\ge3^{-1/2}>0$
\end_inset

.
\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $hn^{-1/2}b_{n}\to\infty$
\end_inset

, we have that 
\begin_inset Formula $hn^{-1/2}\phi_{p}(\bm{s})\hat{\bm{\zeta}}_{(p)}(\bm{s})\|\hat{\bm{\zeta}}_{(p)}(\bm{s})\|^{-1}\ge hb_{n}(3n)^{-1/2}\to\infty$
\end_inset

 and therefore the left hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:glm-selection"

\end_inset

 dominates the sum to the right side.
 Thus, for large enough 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $\hat{\bm{\zeta}}_{(p)}(\bm{s})\ne\bm{0}$
\end_inset

 cannot maximize 
\begin_inset Formula $\mathcal{J}\left(\cdot\right)$
\end_inset

, and therefore 
\begin_inset Formula $P\left\{ \hat{\bm{\zeta}}_{(b)}(\bm{s})=\bm{0}\right\} \to1$
\end_inset

.
 
\end_layout

\begin_layout Section*
Appendix: Proof of Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lemma:omega"

\end_inset


\end_layout

\begin_layout Proof

\series bold
Expectation
\series default
: For 
\begin_inset Formula $j=1,\dots,p$
\end_inset

, by a Taylor expansion of 
\begin_inset Formula $\beta_{j}(\bm{s}_{i})$
\end_inset

 around 
\begin_inset Formula $\bm{s}$
\end_inset

,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\beta_{j}(\bm{s}_{i})=\beta_{j}(\bm{s})+\nabla\beta_{j}(\bm{s})(\bm{s}_{i}-\bm{s})+(\bm{s}_{i}-\bm{s})^{T}\left\{ \nabla^{2}\beta_{j}(\bm{s})\right\} (\bm{s}_{i}-\bm{s})+o\left(h^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
and thus, for 
\begin_inset Formula $\bm{x}\in\mathbb{R}^{p}$
\end_inset

, 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\bm{x}_{i}^{T}\bm{\beta}\!\left(\bm{s}_{i}\right)=\sum_{j=1}^{p}x_{ij}\left[\beta_{j}(\bm{s})+\nabla\beta_{j}(\bm{s})^{T}(\bm{s}_{i}-\bm{s})+\tilde{\beta}''_{ij}\right]+o\left(h^{2}\right).
\]

\end_inset


\end_layout

\begin_layout Proof
Letting 
\begin_inset Formula $\bm{z}_{i}^{T}=\left\{ \left(1,s_{i,1}-s_{1},s_{i,2}-s_{2}\right)\otimes\bm{x}_{i}^{T}\right\} $
\end_inset

 and 
\begin_inset Formula $\bm{\zeta}(\bm{s})=\left(\bm{\beta}(\bm{s})^{T},\nabla_{u}\bm{\beta}(\bm{s})^{T},\nabla_{v}\bm{\beta}(\bm{s})^{T}\right)^{T}$
\end_inset

, we have that 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
\bm{x}_{i}^{T}\bm{\beta}(\bm{s}_{i})-\bm{z}_{i}^{T}\bm{\zeta}(\bm{s})= & \bm{x}_{i}^{T}\tilde{\bm{\beta}}''_{i}+o\left(h^{2}\right)\\
= & O_{p}\left(h^{2}\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
By a Taylor expansion around 
\begin_inset Formula $\bm{x}^{T}\bm{\beta}(\bm{s}_{i})$
\end_inset

, then, 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
q_{1}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}),\mu(\bm{s}_{i},\bm{z}_{i})\right)= & q_{1}\left(\bm{x}_{i}^{T}\bm{\beta}(\bm{s}_{i}),\mu(\bm{s}_{i},\bm{z})\right)\\
 & -q_{2}\left(\bm{x}_{i}^{T}\bm{\beta}(\bm{s}_{i}),\mu(\bm{s}_{i},\bm{z})\right)\bm{x}_{i}^{T}\tilde{\bm{\beta}}''_{i}\\
 & +o\left(h^{2}\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
And by (D.A.2)(a) and (D.A.2)(b), we have that
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
q_{1}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}),\mu(\bm{s}_{i},\bm{z}_{i})\right)=\rho(\bm{s}_{i},\bm{z}_{i})\bm{x}_{i}^{T}\tilde{\bm{\beta}}''_{i}+o\left(h^{2}\right).
\]

\end_inset


\end_layout

\begin_layout Proof
Now the expectation of 
\begin_inset Formula $\Omega_{n}$
\end_inset

 is 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
nE\left(\omega_{i}|\bm{Z}_{i}=\bm{z}_{i},\bm{s}_{i}\right)= & \left(1/2\right)\alpha_{n}\bm{z}_{i}q_{1}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}),\mu(\bm{s}_{i},\bm{z}_{i})\right)K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right)\\
= & \left(1/2\right)\alpha_{n}h^{2}\bm{z}_{i}\left\{ h^{-2}\rho(\bm{s}_{i},\bm{z}_{i})\bm{x}_{i}^{T}\tilde{\bm{\beta}}''_{i}+o\left(\bm{1}_{3p}\right)\right\} K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
To facilitate a change of variables, we observe that 
\begin_inset Formula $h^{-2}\tilde{\beta}''_{j}=\left(\frac{\bm{s}_{i}-\bm{s}}{h}\right)^{T}\left\{ \nabla^{2}\beta_{j}(\bm{s})\right\} \left(\frac{\bm{s}_{i}-\bm{s}}{h}\right)$
\end_inset

.
 Thus,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
E\left(\omega_{i}|\bm{s}_{i}\right)= & \left(1/2\right)\alpha_{n}h^{2}\left[\left(\begin{array}{c}
1\\
h^{-1}(s_{i,1}-s_{1})\\
h^{-1}(s_{i,2}-s_{2})
\end{array}\right)\otimes\left\{ \Gamma(\bm{s}_{i})h^{-2}\tilde{\bm{\beta}}''_{i}\right\} +o\left(\bm{1}_{3p}\right)\right]K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
And, using the symmetry of the kernel function,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
E\left(\omega_{i}\right)= & (1/2)\alpha_{n}h^{4}f(\bm{s})\left(\begin{array}{c}
\kappa_{2}\\
h\kappa_{3}\\
h\kappa_{3}
\end{array}\right)\otimes\left[\Gamma(\bm{s})\left\{ \nabla_{uu}^{2}\bm{\beta}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}(\bm{s})\right\} \right]+o\left(h^{2}\bm{1}_{3p}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
where 
\begin_inset Formula $\left\{ \nabla_{uu}^{2}\bm{\beta}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}(\bm{s})\right\} =\left(\nabla_{uu}^{2}\beta_{1}(\bm{s})+\nabla_{vv}^{2}\beta_{1}(\bm{s}),\dots,\nabla_{uu}^{2}\beta_{p}(\bm{s})+\nabla_{vv}^{2}\beta_{p}(\bm{s})\right)^{T}$
\end_inset

.
\end_layout

\begin_layout Proof
Thus,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
E\left(\Omega_{n}\right)= & \left(\begin{array}{c}
\alpha_{n}^{-1}2^{-1}h^{2}\kappa_{2}f(\bm{s})\Gamma(\bm{s})\left(\nabla_{uu}^{2}\bm{\beta}(\bm{s})+\nabla_{vv}^{2}\bm{\beta}(\bm{s})\right)^{T}\\
\bm{0}_{2p}
\end{array}\right)+o_{p}\left(h^{2}\bm{1}_{3p}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Proof

\series bold
Variance
\series default
: By the previous result, 
\begin_inset Formula $E\left(\Omega_{n}\right)=O\left(h^{2}\right)$
\end_inset

.
 Thus, 
\begin_inset Formula $var\left(\Omega_{n}\right)\to E\left(\Omega_{n}^{2}\right)$
\end_inset

, and since the observations are independent, 
\begin_inset Formula $E\left(\Omega_{n}^{2}\right)=\sum_{i=1}^{n}E\left(\omega_{i}^{2}\right)$
\end_inset

.
 And, by Taylor expansion around 
\begin_inset Formula $\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i})$
\end_inset

, 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
q_{1}^{2}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}),Y_{i}\right)= & q_{1}^{2}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i}),Y_{i}\right)\\
 & -q_{1}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i}),Y_{i}\right)q_{2}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i}),Y_{i}\right)\bm{x}_{i}^{T}\tilde{\bm{\beta}}''_{i}\\
 & +o\left(h^{2}\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $q_{1}\left(\cdot,\cdot\right)$
\end_inset

 is the quasi-score function, it follows that 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
E\left(\omega_{i}^{2}|\bm{Z}_{i}=\bm{z}_{i},\bm{s}_{i}\right)= & \alpha_{n}^{2}\rho(\bm{s}_{i},\bm{z}_{i})\bm{z}_{i}\bm{z}_{i}^{T}K\left(h^{-1}\|\bm{s}-\bm{s}_{i}\|\right)+o\left(h^{2}\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
By the symmetry of the kernel function,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
E\left(\omega_{i}^{2}\right)=n^{-1}f(\bm{s}){\rm diag}\left\{ \nu_{0},\nu_{2},\nu_{2}\right\} \otimes\Gamma(\bm{s})+o\left(1\right).
\]

\end_inset


\end_layout

\begin_layout Proof
Thus, 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
Var\left(\Omega_{n}\right)=f(\bm{s}){\rm diag}\left\{ \nu_{0},\nu_{2},\nu_{2}\right\} \otimes\Gamma(\bm{s})+o\left(1\right).
\]

\end_inset


\end_layout

\begin_layout Section*
Appendix: Proof of Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lemma:delta"

\end_inset


\end_layout

\begin_layout Proof

\series bold
Expectation
\series default
: The approach is similar to the proof of Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lemma:omega"

\end_inset

.
 By the Taylor expansion of 
\begin_inset Formula $q_{2}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}),\mu\left(\bm{s}_{i},\bm{z}_{i}\right)\right)$
\end_inset

 around 
\begin_inset Formula $\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i})$
\end_inset

:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
q_{2}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}),\mu(\bm{s}_{i},\bm{z}_{i})\right)= & q_{2}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i}),\mu(\bm{s}_{i},\bm{z}_{i})\right)+q_{3}\left(\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i}),\mu(\bm{s}_{i},\bm{z}_{i})\right)\left\{ \bm{z}_{i}^{T}\bm{\zeta}(\bm{s})-\bm{z}_{i}^{T}\bm{\zeta}(\bm{s}_{i})\right\} \\
= & -\rho(\bm{s}_{i},\bm{z}_{i})+o\left(1\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
And by the same arguments as before
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
E\left(\delta_{i}|\bm{Z}_{i}=\bm{z}_{i},\bm{s}_{i}\right)= & -\alpha_{n}^{2}\rho(\bm{s}_{i},\bm{z}_{i})\bm{z}_{i}\bm{z}_{i}^{T}K\left(h^{-1}\|\bm{s}_{i}-\bm{s}\|\right)\\
E\left(\delta_{i}|\bm{s}_{i}\right)= & -\alpha_{n}^{2}\left(\begin{array}{c}
1\\
h^{-1}(s_{i,1}-s_{1})\\
h^{-1}(s_{i,2}-s_{2})
\end{array}\right)\left(\begin{array}{c}
1\\
h^{-1}(s_{i,1}-s_{1})\\
h^{-1}(s_{i,2}-s_{2})
\end{array}\right)^{T}\otimes\Gamma(\bm{s}_{i})K\left(h^{-1}\|\bm{s}_{i}-\bm{s}\|\right)\\
E\left(\delta_{i}\right)= & -nf\left(\bm{s}\right){\rm diag}\left\{ \kappa_{0},\kappa_{2},\kappa_{2}\right\} \otimes\Gamma\left(\bm{s}\right)+o\left(n^{-1}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Thus, 
\begin_inset Formula 
\[
E\left(\Delta_{n}\right)=-f(\bm{s}){\rm diag}\left\{ \kappa_{0},\kappa_{2},\kappa_{2}\right\} \otimes\Gamma(\bm{s})+o\left(1\right)
\]

\end_inset


\end_layout

\begin_layout Proof

\series bold
Variance
\series default
: From the previous result, it follows that 
\begin_inset Formula $\left\{ E\left(\delta_{i}\right)\right\} ^{2}=O\left(n^{-2}\right)$
\end_inset

.
 By the definition of 
\begin_inset Formula $\delta_{i}$
\end_inset

,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{multline*}
E\left(\delta_{i}^{2}|\bm{Z}_{i}=\bm{z}_{i},\bm{s}_{i}\right)=\\
\alpha_{n}^{4}\bm{z}_{i}^{T}\bm{z}_{i}q_{2}^{2}(\bm{s}_{i},\bm{z}_{i})\left(\begin{array}{c}
1\\
h^{-1}(s_{i,1}-s_{1})\\
h^{-1}(s_{i,2}-s_{2})
\end{array}\right)\left(\begin{array}{c}
1\\
h^{-1}(s_{i,1}-s_{1})\\
h^{-1}(s_{i,2}-s_{2})
\end{array}\right)^{T}\bm{z}_{i}\bm{z}_{i}^{T}K^{2}\left(h^{-1}\|\bm{s}_{i}-\bm{s}\|\right)+o\left(1\right)
\end{multline*}

\end_inset


\end_layout

\begin_layout Proof
And it follows that 
\begin_inset Formula $E\left(\delta_{i}^{2}\right)=O\left(n^{-1}\alpha_{n}^{2}\right)$
\end_inset

, and 
\begin_inset Formula $Var\left(\Delta_{n}\right)=O\left(\alpha_{n}^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/Users/wesley/git/gwr/references/gwr"
options "chicago"

\end_inset


\end_layout

\end_body
\end_document
